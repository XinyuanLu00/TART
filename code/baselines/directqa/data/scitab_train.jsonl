{"id": "068fc4e5-d40c-4788-a5a7-bad858cbd7c7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.\nTable:\n|| Feature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1 ||\n|| +BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 |  0.93 | 0.94 | 0.92 |  0.93 | 0.91 | 0.91 |  0.91 ||\n|| +BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89 ||\n|| +Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88 ||\n|| +BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n\nQuestion: Is it true that The models using BoC outperform models using BoW as well as ASM features?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "4ab2d01c-b861-49be-9fd3-b87ffdc7ac20", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\nTable:\n||  | Difference Function | Seanad Abolition | Video Games | Pornography ||\n|| OD-parse | Absolute | 0.01 | -0.01 | 0.07 ||\n|| OD-parse | JS div. | 0.01 | -0.01 | -0.01 ||\n|| OD-parse | EMD | 0.07 | 0.01 | -0.01 ||\n|| OD | Absolute |  0.54 |  0.56 |  0.41 ||\n|| OD | JS div. | 0.07 | -0.01 | -0.02 ||\n|| OD | EMD | 0.26 | -0.01 | 0.01 ||\n|| OD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04 ||\n|| OD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02 ||\n|| OD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01 ||\n\nQuestion: Is it true that  OD significantly outperforms OD-parse: We observe that compared to OD-parse, OD is much more accurate?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "cc5ee69b-76a7-4812-b9f9-9bfbdb654101", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.\nTable:\n||  | MFT | UnsupEmb | Word2Tag ||\n|| POS | 91.95 | 87.06 | 95.55 ||\n|| SEM | 82.00 | 81.11 | 91.41 ||\n\nQuestion: Is it true that The UnsupEmb baseline performs rather poorly on both POS and SEM tagging?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "d3c5de39-44f7-4eff-93d4-8295045a1db1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 1: Classifier performance\nTable:\n|| Dataset | Class | Precision | Recall | F1 ||\n||  W. & H. | Racism | 0.73 | 0.79 | 0.76 ||\n||  | Sexism | 0.69 | 0.73 | 0.71 ||\n||  | Neither | 0.88 | 0.85 | 0.86 ||\n||  W. | Racism | 0.56 | 0.77 | 0.65 ||\n||  | Sexism | 0.62 | 0.73 | 0.67 ||\n||  | R. & S. | 0.56 | 0.62 | 0.59 ||\n||  | Neither | 0.95 | 0.92 | 0.94 ||\n||  D. et al. | Hate | 0.32 | 0.53 | 0.4 ||\n||  | Offensive | 0.96 | 0.88 | 0.92 ||\n||  | Neither | 0.81 | 0.95 | 0.87 ||\n||  G. et al. | Harass. | 0.41 | 0.19 | 0.26 ||\n||  | Non. | 0.75 | 0.9 | 0.82 ||\n||  F. et al. | Hate | 0.33 | 0.42 | 0.37 ||\n||  | Abusive | 0.87 | 0.88 | 0.88 ||\n||  | Spam | 0.5 | 0.7 | 0.58 ||\n||  | Neither | 0.88 | 0.77 | 0.82 ||\n\nQuestion: Is it true that In particular, we see that hate speech and harassment are particularly difficult to detect?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b4c1d97b-4782-4575-a319-1b40d0ece452", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\nTable:\n||  | Acc | Sim | PP | GM ||\n|| M0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0 ||\n|| M1: M0  +para | 0.819 | 0.734 | 26.3 | 14.2 ||\n|| M2: M0  +cyc | 0.813 | 0.770 | 36.4 | 18.8 ||\n|| M3: M0  +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5 ||\n|| M4: M0  +cyc+para | 0.798 | 0.783 | 39.7 | 19.2 ||\n|| M5: M0  +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3 ||\n|| M6: M0  +cyc+2d | 0.805 |  0.817 | 43.3 | 21.6 ||\n|| M7: M6+  para+lang | 0.818 | 0.805 |  29.0 |  22.8 ||\n\nQuestion: Is it true that  When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7ce0aea2-aa42-4cc8-99eb-6bdfe53cf6da", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\nTable:\n|| <bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold> ||\n|| LDC2015E86 | LDC2015E86 | LDC2015E86 ||\n|| Konstas et al. (2017) | 22.00 | - ||\n|| Song et al. (2018) | 23.28 | 30.10 ||\n|| Cao et al. (2019) | 23.50 | - ||\n|| Damonte et al.(2019) | 24.40 | 23.60 ||\n|| Guo et al. (2019) | <bold>25.70</bold> | - ||\n|| S2S | 22.55 ± 0.17 | 29.90 ± 0.31 ||\n|| G2S-GIN | 22.93 ± 0.20 | 29.72 ± 0.09 ||\n|| G2S-GAT | 23.42 ± 0.16 | 29.87 ± 0.14 ||\n|| G2S-GGNN | 24.32 ± 0.16 | <bold>30.53</bold> ± 0.30 ||\n|| LDC2017T10 | LDC2017T10 | LDC2017T10 ||\n|| Back et al. (2018) | 23.30 | - ||\n|| Song et al. (2018) | 24.86 | 31.56 ||\n|| Damonte et al.(2019) | 24.54 | 24.07 ||\n|| Cao et al. (2019) | 26.80 | - ||\n|| Guo et al. (2019) | 27.60 | - ||\n|| S2S | 22.73 ± 0.18 | 30.15 ± 0.14 ||\n|| G2S-GIN | 26.90 ± 0.19 | 32.62 ± 0.04 ||\n|| G2S-GAT | 26.72 ± 0.20 | 32.52 ± 0.02 ||\n|| G2S-GGNN | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 ||\n\nQuestion: Is it true that We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f447aac8-3df2-4446-82f9-89b20ad46901", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Localization of Fake News Detection via Multitask Transfer Learning Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. “Acc. Inc.” refers to the boost in performance contributed by the pretraining step. “% of Perf.” refers to the percentage of the total performance that the pretraining step contributes.\nTable:\n|| Finetuning | Pretrained? | Accuracy | Val. Loss | Acc. Inc. | % of Perf. ||\n|| Multitasking | No | 53.61% | 0.7217 | - | - ||\n||  | Yes | 96.28% | 0.2197 | +42.67% | 44.32% ||\n|| Standard | No | 51.02% | 0.7024 | - | - ||\n||  | Yes | 90.99% | 0.1826 | +39.97% | 43.93% ||\n\nQuestion: Is it true that In Table 5, it can be seen that generative pretraining via language modeling does not account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "286a8de8-bba3-4a30-8e62-b75d6d91ed7d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 3: Results on the dev set and on the test set. “perp” stands for perplexity, “acc” stands for accuracy (in percents), and “wer” stands for word-error-rate.\nTable:\n||  | dev perp ↓ | dev acc ↑ | dev wer ↓ | test perp ↓ | test acc ↑ | test wer ↓ ||\n|| Spanish-only-LM | 329.68 | 26.6 | 30.47 | 322.26 | 25.1 | 29.62 ||\n|| English-only-LM | 320.92 | 29.3 | 32.02 | 314.04 | 30.3 | 32.51 ||\n|| All:CS-last-LM | 76.64 | 47.8 | 14.56 | 76.97 | 49.2 | 14.13 ||\n|| All:Shuffled-LM | 68.00 | 51.8 | 13.64 | 68.72 | 51.4 | 13.89 ||\n|| CS-only-LM | 43.20 | 60.7 | 12.60 | 43.42 | 57.9 | 12.18 ||\n|| CS-only+vocab-LM | 45.61 | 61.0 | 12.56 | 45.79 | 58.8 | 12.49 ||\n|| Fine-Tuned-LM | 39.76 | 66.9 | 10.71 | 40.11 | 65.4 | 10.17 ||\n|| CS-only-disc | – | 72.0 | 6.35 | – | 70.5 | 6.70 ||\n|| Fine-Tuned-disc | – |  74.2 |  5.85 | – |  75.5 |  5.59 ||\n\nQuestion: Is it true that Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 70.5%, 5.1 points more than the accuracy of the FINE-TUNED-LM model?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "4169c808-fdbd-4bc2-a58c-0ad6872535b1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\nTable:\n||  Complaints   Feature |  Complaints   r |  Not Complaints   Feature |  Not Complaints   r ||\n||  Unigrams |  Unigrams |  Unigrams |  Unigrams ||\n|| not | .154 | [URL] | .150 ||\n|| my | .131 | ! | .082 ||\n|| working | .124 | he | .069 ||\n|| still | .123 | thank | .067 ||\n|| on | .119 | , | .064 ||\n|| can’t | .113 | love | .064 ||\n|| service | .112 | lol | .061 ||\n|| customer | .109 | you | .060 ||\n|| why | .108 | great | .058 ||\n|| website | .107 | win | .058 ||\n|| no | .104 | ’ | .058 ||\n|| ? | .098 | she | .054 ||\n|| fix | .093 | : | .053 ||\n|| won’t | .092 | that | .053 ||\n|| been | .090 | more | .052 ||\n|| issue | .089 | it | .052 ||\n|| days | .088 | would | .051 ||\n|| error | .087 | him | .047 ||\n|| is | .084 | life | .046 ||\n|| charged | .083 | good | .046 ||\n||  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) ||\n|| VBN | .141 | UH | .104 ||\n|| $ | .118 | NNP | .098 ||\n|| VBZ | .114 | PRP | .076 ||\n|| NN_VBZ | .114 | HT | .076 ||\n|| PRP$ | .107 | PRP_. | .076 ||\n|| PRP$_NN | .105 | PRP_RB | .067 ||\n|| VBG | .093 | NNP_NNP | .062 ||\n|| CD | .092 | VBP_PRP | .054 ||\n|| WRB_VBZ | .084 | JJ | .053 ||\n|| VBZ_VBN | .084 | DT_JJ | .051 ||\n\nQuestion: Is it true that Mentions of time are not specific of complaints (been, still, on, days, Temporal References cluster)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "2ac86f7a-9e75-41a0-a2c9-f36542cb12cf", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: What do Deep Networks Like to Read? Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.\nTable:\n||  | <bold>RNN</bold> | <bold>CNN</bold> | <bold>DAN</bold> ||\n|| Positive | +9.7 | +4.3 | +<bold>23.6</bold> ||\n|| Negative | +6.9 | +5.5 | +<bold>16.1</bold> ||\n|| Flipped to Positive | +20.2 | +24.9 | +27.4 ||\n|| Flipped to Negative | +31.5 | +28.6 | +19.3 ||\n\nQuestion: Is it true that This is especially true in the case of DAN where we see a large increase as the decoder repeatedly predicts words having high sentiment value?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0ee9a5d8-8b90-424c-9e68-f02437594591", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.\nTable:\n||  Model |  Type |  English-German #P |  English-German B |  English-German C |  English-Czech #P |  English-Czech B |  English-Czech C ||\n|| BoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | - ||\n|| CNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | - ||\n|| BiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | - ||\n|| PB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3 ||\n|| DCGCN (ours) | Single |   29.7M |  19.0 |  44.1 |   28.3M |  12.1 |  37.1 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9 ||\n|| DCGCN (ours) | Ensemble |   149M |  20.5 |  45.8 |   142M |  13.1 |  37.8 ||\n\nQuestion: Is it true that Our models DCGCN(single) and DCGCN(ensemble)consist of full GCN layers, removing the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "14c7a0f9-247b-4a40-bd54-5ad510a0a091", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 1: Image-caption ranking results for English (Multi30k)\nTable:\n||  | Image to Text R@1 | Image to Text R@5 | Image to Text R@10 | Image to Text Mr | Text to Image R@1 | Text to Image R@5 | Text to Image R@10 | Text to Image Mr | Alignment ||\n||  symmetric |  |  |  |  |  |  |  |  |  ||\n|| Parallel gella:17 | 31.7 | 62.4 | 74.1 | 3 | 24.7 | 53.9 | 65.7 | 5 | - ||\n|| UVS kiros:15 | 23.0 | 50.7 | 62.9 | 5 | 16.8 | 42.0 | 56.5 | 8 | - ||\n|| EmbeddingNet wang:18 | 40.7 | 69.7 | 79.2 | - | 29.2 | 59.6 | 71.7 | - | - ||\n|| sm-LSTM huang:17 | 42.5 | 71.9 | 81.5 | 2 | 30.2 | 60.4 | 72.3 | 3 | - ||\n|| VSE++ faghri:18 |  43.7 | 71.9 | 82.1 | 2 | 32.3 | 60.9 | 72.1 | 3 | - ||\n|| Mono | 41.4 | 74.2 | 84.2 | 2 | 32.1 | 63.0 | 73.9 | 3 | - ||\n|| FME | 39.2 | 71.1 | 82.1 | 2 | 29.7 | 62.5 | 74.1 | 3 | 76.81% ||\n|| AME | 43.5 |  77.2 |  85.3 |  2 |  34.0 |  64.2 |  75.4 |  3 | 66.91% ||\n||  asymmetric |  |  |  |  |  |  |  |  |  ||\n|| Pivot gella:17 | 33.8 | 62.8 | 75.2 | 3 | 26.2 | 56.4 | 68.4 | 4 | - ||\n|| Parallel gella:17 | 31.5 | 61.4 | 74.7 | 3 | 27.1 | 56.2 | 66.9 | 4 | - ||\n|| Mono | 47.7 | 77.1 | 86.9 | 2 | 35.8 | 66.6 | 76.8 | 3 | - ||\n|| FME | 44.9 | 76.9 | 86.4 | 2 | 34.2 | 66.1 | 77.1 | 3 | 76.81% ||\n|| AME |  50.5 |  79.7 |  88.4 |  1 |  38.0 |  68.5 |  78.4 |  2 | 73.10% ||\n\nQuestion: Is it true that AME performs better than FME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "4a50e39b-a5db-4dce-b474-46fda3d1159b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 2: Ratings of annotated NLDs by human judges.\nTable:\n|| # steps | Reachability | Derivability Step 1 | Derivability Step 2 | Derivability Step 3 ||\n|| 1 | 3.0 | 3.8 | - | - ||\n|| 2 | 2.8 | 3.8 | 3.7 | - ||\n|| 3 | 2.3 | 3.9 | 3.8 | 3.8 ||\n\nQuestion: Is it true that On the contrary, we found the quality of 3-step NLDs is relatively higher than the others?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e6163646-e624-431a-a99d-c4f2450a0183", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the “good” summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nTable:\n|| Metric |  ρ |  r | G-Pre | G-Rec ||\n|| ROUGE-1 | .290 | .304 | .392 | .428 ||\n|| ROUGE-2 | .259 | .278 | .408 | .444 ||\n|| ROUGE-L | .274 | .297 | .390 | .426 ||\n|| ROUGE-SU4 | .282 | .279 | .404 | .440 ||\n|| BLEU-1 | .256 | .281 | .409 | .448 ||\n|| BLEU-2 | .301 | .312 | .411 | .446 ||\n|| BLEU-3 | .317 | .312 | .409 | .444 ||\n|| BLEU-4 | .311 | .307 | .409 | .446 ||\n|| BLEU-5 | .308 | .303 | .420 | .459 ||\n|| METEOR | .305 | .285 | .409 | .444 ||\n|| InferSent-Cosine |  .329 |  .339 | .417 | .460 ||\n|| BERT-Cosine | .312 | .335 |  .440 |  .484 ||\n\nQuestion: Is it true that More importantly, their G-Pre and G-Rec scores are all above .50, which means that more than half of the good summaries identified by the metrics are actually good, and more than 50%?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "961db06c-7cce-438a-ad9b-89e45a05da2a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).\nTable:\n|| Train | Test |  System |  BLEU |  NIST |  METEOR |  ROUGE-L |  CIDEr |  Add |  Miss |  Wrong |  SER ||\n|| Original |  Cleaned | TGen− | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31 ||\n|| Original |  Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05 ||\n|| Original |  Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24 ||\n|| Original |  Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen− | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94 ||\n|| Cleaned missing |  Cleaned | TGen− | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52 ||\n|| Cleaned missing |  Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86 ||\n|| Cleaned missing |  Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen− | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88 ||\n\nQuestion: Is it true that the models more often fail to realise part of the MR, rather than hallucinating additional information?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0150f1a0-fe1d-4497-8489-a649003ab619", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set\nTable:\n|| Method | WER (%) | DCE ||\n|| No enhancement | 17.3 | 0.828 ||\n|| Wiener filter | 19.5 | 0.722 ||\n|| Minimizing DCE | 15.8 |  0.269 ||\n|| FSEGAN | 14.9 | 0.291 ||\n|| AAS (  wAC=1,  wAD=0) | 15.6 | 0.330 ||\n|| AAS (  wAC=1,  wAD=105) |  14.4 | 0.303 ||\n|| Clean speech | 5.7 | 0.0 ||\n\nQuestion: Is it true that The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "0fbadeff-af49-4236-b0b4-749c3e102f94", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).\nTable:\n||  |  Model |  dev mean |  dev best |  test mean |  test best |  α ||\n|| single | text | 86.54 | 86.80 | 86.47 | 86.96 | – ||\n|| single | raw | 35.00 | 37.33 | 35.78 | 37.70 | – ||\n|| single | innovations | 80.86 | 81.51 | 80.28 | 82.15 | – ||\n|| early | text + raw | 86.46 | 86.65 | 86.24 | 86.53 | – ||\n|| early | text + innovations | 86.53 | 86.77 | 86.54 | 87.00 | – ||\n|| early | text + raw + innovations | 86.35 | 86.69 | 86.55 | 86.44 | – ||\n|| late | text + raw | 86.71 | 87.05 | 86.35 | 86.71 | 0.2 ||\n|| late | text + innovations |  86.98 |  87.48 |  86.68 |  87.02 | 0.5 ||\n|| late | text + raw + innovations | 86.95 | 87.30 | 86.60 | 86.87 | 0.5 ||\n\nQuestion: Is it true that  We found that innovations are helpful in both early and late fusion frameworks, while late fusion performs better on average?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "c45cc229-e5f8-4a18-b769-42397cd1f57d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Zero-Shot Grounding of Objects from Natural Language Queries Table 3: Category-wise performance with the default split of Flickr30k Entities.\nTable:\n|| Method | Overall | people | clothing | bodyparts | animals | vehicles | instruments | scene | other ||\n|| QRC - VGG(det) | 60.21 | 75.08 | 55.9 | 20.27 | 73.36 | 68.95 | 45.68 | 65.27 | 38.8 ||\n|| CITE - VGG(det) | 61.89 |  75.95 | 58.50 | 30.78 |  77.03 |  79.25 | 48.15 | 58.78 | 43.24 ||\n|| ZSGNet - VGG (cls) | 60.12 | 72.52 | 60.57 | 38.51 | 63.61 | 64.47 | 49.59 | 64.66 | 41.09 ||\n|| ZSGNet - Res50 (cls) |  63.39 | 73.87 |  66.18 |  45.27 | 73.79 | 71.38 |  58.54 |  66.49 |  45.53 ||\n\nQuestion: Is it true that  As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\")?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8d3edac1-1144-4e49-ab95-1d0898c2acaf", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Method | STS12 | STS13 | STS14 | STS15 | STS16 ||\n|| CBOW | 43.5 |  50.0 |  57.7 |  63.2 | 61.0 ||\n|| CMOW | 39.2 | 31.9 | 38.7 | 49.7 | 52.2 ||\n|| Hybrid |  49.6 | 46.0 | 55.1 | 62.4 |  62.1 ||\n|| cmp. CBOW | +14.6% | -8% | -4.5% | -1.5% | +1.8% ||\n|| cmp. CMOW | +26.5% | +44.2% | +42.4 | +25.6% | +19.0% ||\n\nQuestion: Is it true that The hybrid model is able to repair this deficit, reducing the difference to 8%?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "6ff909e7-efc8-4f69-a140-ebb18d859825", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: The MeMAD Submission to the WMT18 Multimodal Translation Task Table 4: Adding automatic image captions (only the best one or all 5). The table shows BLEU scores in %. All results with Marian Amun.\nTable:\n|| en-fr | flickr16 | flickr17 | mscoco17 ||\n|| multi30k | 61.4 | 54.0 | 43.1 ||\n|| +autocap (dual attn.) | 60.9 | 52.9 | 43.3 ||\n|| +autocap 1 (concat) | 61.7 | 53.7 | 43.9 ||\n|| +autocap 1-5 (concat) |  62.2 |  54.4 |  44.1 ||\n|| en-de | flickr16 | flickr17 | mscoco17 ||\n|| multi30k | 38.9 | 32.0 | 27.7 ||\n|| +autocap (dual attn.) | 37.8 | 30.2 | 27.0 ||\n|| +autocap 1 (concat) | 39.7 |  32.2 |  28.8 ||\n|| +autocap 1-5 (concat) |  39.9 | 32.0 | 28.7 ||\n\nQuestion: Is it true that We can see that the dual attention model does not work at all and the scores slightly drop?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "e0ae18f4-9bba-4ce9-83da-01228c8b4f30", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.\nTable:\n||  Model |  Type |  English-German #P |  English-German B |  English-German C |  English-Czech #P |  English-Czech B |  English-Czech C ||\n|| BoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | - ||\n|| CNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | - ||\n|| BiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | - ||\n|| PB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3 ||\n|| DCGCN (ours) | Single |   29.7M |  19.0 |  44.1 |   28.3M |  12.1 |  37.1 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9 ||\n|| DCGCN (ours) | Ensemble |   149M |  20.5 |  45.8 |   142M |  13.1 |  37.8 ||\n\nQuestion: Is it true that Our single model is not comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e91d427d-29e5-46b3-a123-ba6111eb0525", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\nTable:\n||  ResNet-34 |  Eval set % |  #param ||\n|| Baseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M ||\n|| SA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M ||\n|| SA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M ||\n||  SA (S: 1,2,3 - B: 3) |  55.27 | } 0.107M ||\n\nQuestion: Is it true that We notice no significant improvements relative to the baseline showing that self-attention alone does not improve the VQA task?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "621a2ffa-a852-4a18-87d4-c5312befefd5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nTable:\n|| Dataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb ||\n||  Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 |  76.41 ||\n||  WinoCoref | AntePre | 68.37 | 74.32 | —– | 88.48 | 88.95 |  89.32 ||\n\nQuestion: Is it true that These results show no significant performance improvement by using Predicate Schemas knowledge on hard coreference problems?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "65f4f4b5-a857-4cff-a33a-12fbad54d0fd", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.\nTable:\n|| Schema | AntePre(Test) | AntePre(Train) ||\n|| Type 1 | 76.67 | 86.79 ||\n|| Type 2 | 79.55 | 88.86 ||\n|| Type 1 (Cat1) | 90.26 | 93.64 ||\n|| Type 2 (Cat2) | 83.38 | 92.49 ||\n\nQuestion: Is it true that Also, the performance drop between Cat1/Cat2 and full data indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition for further performance improvement?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "9f36b848-b4e8-449c-aa79-3a4ed3e10d71", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: On the difficulty of a distributional semantics of spoken language Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.\nTable:\n||  | Recall@10 (%) | Median rank | RSAimage ||\n|| VGS | 27 | 6 | 0.4 ||\n|| SegMatch |  10 |  37 |  0.5 ||\n|| Audio2vec-U | 5 | 105 | 0.0 ||\n|| Audio2vec-C | 2 | 647 | 0.0 ||\n|| Mean MFCC | 1 | 1,414 | 0.0 ||\n|| Chance | 0 | 3,955 | 0.0 ||\n\nQuestion: Is it true that Audio2vec works better than chance and mean MFCC on paraphrase retrieval, but does not correlate with the visual space?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3ccac9a1-709e-4f2a-a485-b6ecf90417cc", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\nTable:\n||  Method |  R-1 |  R-2 |  R-SU ||\n|| First-1 | 26.83 | 7.25 | 6.46 ||\n|| First-2 | 35.99 | 10.17 | 12.06 ||\n|| First-3 | 39.41 | 11.77 | 14.51 ||\n|| LexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20 ||\n|| TextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50 ||\n|| MMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91 ||\n|| PG-Original Lebanoff et al. ( 2018 ) | 41.85 | 12.91 | 16.46 ||\n|| PG-MMR Lebanoff et al. ( 2018 ) | 40.55 | 12.36 | 15.87 ||\n|| PG-BRNN Gehrmann et al. ( 2018 ) | 42.80 | 14.19 | 16.75 ||\n|| CopyTransformer Gehrmann et al. ( 2018 ) |  43.57 | 14.03 | 17.37 ||\n|| Hi-MAP (Our Model) | 43.47 |  14.89 |  17.41 ||\n\nQuestion: Is it true that Also, we notice a drop in performance between PG-original, and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f32c052e-7348-4b5c-86e9-8376b541c61d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326 ||\n|| P | EN | Ted Talks |  0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162 ||\n|| P | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 |  0.8052 | 0.4058 ||\n||  | PT | Ted Talks |  0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698 ||\n|| R | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 |  0.6490 | 0.0017 | 0.0003 ||\n|| R | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 |  0.6467 | 0.6332 | 0.0967 | 0.0003 ||\n|| R | PT | Europarl | 0.0002 | 0.1562 | 0.5157 |  0.7255 | 0.5932 | 0.0032 | 0.0001 ||\n||  | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 |  0.7000 | 0.5887 | 0.1390 | 0.0002 ||\n|| F | EN | Europarl | 0.0073 | 0.0162 | 0.0268 |  0.0293 |  0.0293 | 0.0033 | 0.0006 ||\n|| F | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 |  0.0520 | 0.0005 ||\n|| F | PT | Europarl | 0.0005 | 0.1733 | 0.4412 |  0.6240 | 0.5109 | 0.0064 | 0.0002 ||\n||  | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 |  0.6040 | 0.5149 | 0.2261 | 0.0004 ||\n\nQuestion: Is it true that TF and DF achieved almost the same values of precision, recall and f-measure using the English corpora, achieving the same value of precision (P=0.0150) and f-measure (F=0.0293) when using the Europarl corpus in English?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7e3de05e-1093-426a-9ad5-e6929654530d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.\nTable:\n||  GCN +RC (2) | B 16.8 | C 48.1 |  GCN +RC+LA (2) | B 18.3 | C 47.9 ||\n|| +RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1 ||\n|| +RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8 ||\n|| +RC (9) |  21.1 | 50.5 | +RC+LA (9) |  22.0 | 52.6 ||\n|| +RC (10) | 20.7 |  50.7 | +RC+LA (10) | 21.2 |  52.9 ||\n|| DCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7 ||\n|| DCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) |  25.5 |  55.4 ||\n\nQuestion: Is it true that The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "67b478a8-a83d-4700-8567-f8550bcce109", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 3: ARI and Silhouette coefficient scores.\nTable:\n|| Methods | Seanad Abolition ARI | Seanad Abolition   Sil | Video Games ARI | Video Games   Sil | Pornography ARI | Pornography   Sil ||\n|| TF-IDF | 0.23 | 0.02 | -0.01 | 0.01 | -0.02 | 0.01 ||\n|| WMD | 0.09 | 0.01 | 0.01 | 0.01 | -0.02 | 0.01 ||\n|| Sent2vec | -0.01 | -0.01 | 0.11 | 0.06 | 0.01 | 0.02 ||\n|| Doc2vec | -0.01 | -0.03 | -0.01 | 0.01 | 0.02 | -0.01 ||\n|| BERT | 0.03 | -0.04 | 0.08 | 0.05 | -0.01 | 0.03 ||\n|| OD-parse | 0.01 | -0.04 | -0.01 | 0.02 | 0.07 | 0.05 ||\n|| OD |  0.54 |  0.31 |  0.56 |  0.42 |  0.41 |  0.41 ||\n\nQuestion: Is it true that among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant  OD achieves high ARI and Sil scores,  From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec achieving ARI and Silhouette coefficient scores of close to zero on the \"Video Games\" and \"Pornography\" datasets (barely providing a performance improvement over random clustering, i.e., a zero ARI score)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8aac774b-9ded-41b0-8070-26614c5200f2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nTable:\n|| Model | Method | Training Data | Overall | Easy | Hard | p-value (%) ||\n|| goodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8 ||\n|| gordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5 ||\n|| sasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8∗ ||\n|| Word frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8 ||\n|| BERT-large-FT | LM, NSP | COPA | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗ ||\n|| RoBERTa-large-FT | LM | COPA | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗ ||\n\nQuestion: Is it true that The difference between accuracy on Easy and Hard is less pronounced for RoBERTa, but still suggests some reliance on superficial cues?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0d4a8de9-4bda-499e-a51c-870a1fed2a55", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\nTable:\n||  |  w/ System Retrieval   B-2 |  w/ System Retrieval   B-4 |  w/ System Retrieval   R-2 |  w/ System Retrieval   MTR |  w/ System Retrieval   #Word |  w/ System Retrieval   #Sent |  w/ Oracle Retrieval   B-2 |  w/ Oracle Retrieval   B-4 |  w/ Oracle Retrieval   R-2 |  w/ Oracle Retrieval   MTR |  w/ Oracle Retrieval   #Word |  w/ Oracle Retrieval   #Sent ||\n|| Human | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22 ||\n|| Retrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21 ||\n||  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  |  ||\n|| Seq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 ||\n|| Seq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14 ||\n||  w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12 ||\n|| H&W Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12 ||\n||  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  |  ||\n|| CANDELA | 12.02∗ |  2.99∗ |  14.93∗ |  16.92∗ | 119 | 22 | 15.80∗ |  5.00∗ |  23.75 |  20.18 | 116 | 22 ||\n||  w/o psg |  12.33∗ | 2.86∗ | 14.53∗ | 16.60∗ | 123 | 23 |  16.33∗ | 4.98∗ | 23.65 | 19.94 | 123 | 23 ||\n\nQuestion: Is it true that Under oracle setup, all models are notably improved due to the higher quality of reranked passages, but our model does not achieve statistically significantly better BLEU scores?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "d661490a-948e-4b22-ad8e-4d11b28c00cb", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.\nTable:\n|| System | MUC | BCUB | CEAFe | AVG ||\n|| ACE | ACE | ACE | ACE | ACE ||\n|| IlliCons |  78.17 | 81.64 |  78.45 |  79.42 ||\n|| KnowComb | 77.51 |  81.97 | 77.44 | 78.97 ||\n|| OntoNotes | OntoNotes | OntoNotes | OntoNotes | OntoNotes ||\n|| IlliCons | 84.10 |  78.30 |  68.74 |  77.05 ||\n|| KnowComb |  84.33 | 78.02 | 67.95 | 76.76 ||\n\nQuestion: Is it true that Our KnowComb system does not achieve the same level of performance as the state-of-art general coreference system we base it on?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "45d2802f-6f82-4ee9-96f9-462ec333cbc2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Distant Learning for Entity Linking with Automatic Noise Detection Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)\nTable:\n|| System | All LOC | All ORG | All PER | All MISC | In   E+ LOC | In   E+ ORG | In   E+ PER | In   E+ MISC ||\n|| Name matching | 96.26 | 89.48 | 57.38 | 96.60 | 92.32 | 76.87 | 47.40 | 76.29 ||\n|| MIL | 57.09 |  76.30 | 41.35 | 93.35 | 11.90 |  47.90 | 27.60 | 53.61 ||\n|| MIL-ND | 57.15 | 77.15 | 35.95 | 92.47 | 12.02 | 49.77 | 20.94 | 47.42 ||\n||  τMIL-ND |  55.15 | 76.56 |  34.03 |  92.15 |  11.14 | 51.18 |  20.59 |  40.00 ||\n|| Supervised learning | 55.58 | 61.32 | 24.98 | 89.96 | 8.80 | 14.95 | 7.40 | 29.90 ||\n\nQuestion: Is it true that  For LOC, it turns out that candidate selection is a bottleneck: when candidate selection was flawless, the models made only about 12% errors, down from about 57%?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8c6099dc-368a-44c2-8051-2af00a3c8bdd", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\nTable:\n||  | Difference Function | Seanad Abolition | Video Games | Pornography ||\n|| OD-parse | Absolute | 0.01 | -0.01 | 0.07 ||\n|| OD-parse | JS div. | 0.01 | -0.01 | -0.01 ||\n|| OD-parse | EMD | 0.07 | 0.01 | -0.01 ||\n|| OD | Absolute |  0.54 |  0.56 |  0.41 ||\n|| OD | JS div. | 0.07 | -0.01 | -0.02 ||\n|| OD | EMD | 0.26 | -0.01 | 0.01 ||\n|| OD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04 ||\n|| OD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02 ||\n|| OD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01 ||\n\nQuestion: Is it true that  Sentiment polarity shifters have a high impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" hurts the Opinion Representation phase, and thereby leads to incorrect computation of opinion distance?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "238007ba-b7a6-4b65-9173-b00fdafd9bf2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\nTable:\n||  | MSCOCO spice | MSCOCO cider | MSCOCO rouge  L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep↓ | Flickr30k spice | Flickr30k cider | Flickr30k rouge  L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep↓ ||\n|| softmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09 ||\n|| sparsemax |  18.9 |  0.990 |  53.5 |  31.5 |  25.3 | 3.69 |  13.7 |  0.444 |  44.3 |  20.7 |  19.3 | 5.84 ||\n|| TVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 |  3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 |  3.97 ||\n\nQuestion: Is it true that  Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax and significantly worse than softmax on MSCOCO and similar on Flickr30k?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "2f5f8672-0cea-486d-bf9b-88ee35183cc5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nTable:\n|| Dataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb ||\n||  Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 |  76.41 ||\n||  WinoCoref | AntePre | 68.37 | 74.32 | —– | 88.48 | 88.95 |  89.32 ||\n\nQuestion: Is it true that The results also show that it is better to compile knowledge into constraints when the knowledge quality is high than add them as features?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "715f5a99-6ca2-4117-b4df-a55d7bb5833e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Dim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n|| 400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7 ||\n|| 400 | CMOW/400 |  34.4 | 68.8 | 80.1 |  79.9 |  59.8 | 81.9 |  79.2 |  70.7 |  50.3 | 70.7 ||\n|| 400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 |  87.2 ||\n|| 400 | H-CMOW | 32.3 |  70.8 |  81.3 | 76.0 | 59.6 |  82.3 | 77.4 | 70.0 | 50.2 | 38.2 ||\n|| 784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 |  89.5 ||\n|| 784 | CMOW/784 |  35.1 |  70.8 |  82.0 | 80.2 |  61.8 | 82.8 |  79.7 | 74.2 |  50.7 | 72.9 ||\n|| 800 | Hybrid | 35.0 |  70.8 | 81.7 |  81.0 | 59.4 |  84.4 | 79.0 |  74.3 | 49.3 | 87.6 ||\n|| - | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1% ||\n|| - | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9% ||\n\nQuestion: Is it true that Consequently, with an 8% improvement on average, the hybrid model  Word Content are increased?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3e5908f5-1d9d-47a6-8e48-a0822530dbdb", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nTable:\n|| Model | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time ||\n|| Rocktäschel et al. ( 2016 ) | Rocktäschel et al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | - ||\n|| This | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 |  90.49 | 0.696 ||\n|| This | GRU | 6.41M |  85.71 | 0.245 |  86.05 | 0.419 |  90.29 | 0.529 | 90.10 | 0.695 ||\n|| This | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580 ||\n|| Work | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555 ||\n||  | LRN | 4.25M | 84.88 |  0.209 | 85.06 |  0.223 | 89.98 |  0.488 | 89.93 |  0.506 ||\n\nQuestion: Is it true that  Results with BERT show that contextual information is valuable for performance improvement?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "1563ec9d-c56c-4d98-b401-792e93c5a56d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nTable:\n|| Dataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb ||\n||  Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 |  76.41 ||\n||  WinoCoref | AntePre | 68.37 | 74.32 | —– | 88.48 | 88.95 |  89.32 ||\n\nQuestion: Is it true that It improves by over 20% over a state-of-art general coreference system on Winograd and also outperforms Rahman and Ng (2012) by a margin of 3.3%?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "9be65c57-384f-4389-a499-15fd4ac3ff16", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.\nTable:\n|| System | NC-v11 BLEU | NC-v11 TER↓ | NC-v11 Meteor | Full BLEU | Full TER↓ | Full Meteor ||\n|| OpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225 ||\n|| Transformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344 ||\n|| Seq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258 ||\n|| Dual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246 ||\n|| Duel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223 ||\n|| Dual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328 ||\n|| Dual2seq |  *19.2* |  0.6305 |  0.3840 |  *25.5* |  0.5480 |  0.4376 ||\n\nQuestion: Is it true that Dual2seq-LinAMR shows much worse performance than our Dual2seq model and significantly outperforms the Seq2seq baseline?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "5db7cece-882f-45e8-96c3-82a786c846c2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664 ||\n|| P | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 |  0.7311 | 0.5676 ||\n||  | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 |  0.6533 | 0.5656 ||\n|| R | EN | Europarl | 0.0396 | 0.3999 | 0.5499 |  0.6045 | 0.5887 | 0.0023 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 |  0.6077 | 0.2666 | 0.0019 ||\n|| R | PT | Europarl | 0.0111 | 0.3554 | 0.5795 |  0.6727 | 0.5184 | 0.0053 | 0.0012 ||\n||  | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 |  0.6877 | 0.5515 | 0.4706 | 0.0011 ||\n|| F | EN | Europarl | 0.0591 | 0.0671 | 0.0922 |  0.1015 | 0.1003 | 0.0044 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 |  0.1121 | 0.0037 ||\n|| F | PT | Europarl | 0.0217 | 0.3438 | 0.5513 |  0.6403 | 0.5555 | 0.0105 | 0.0024 ||\n||  | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 |  0.6475 | 0.5819 | 0.5471 | 0.0022 ||\n\nQuestion: Is it true that  Analyzing Table 3, we can observe that all values of precision using the Portuguese corpora have higher scores when compared with the English corpora?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "2f553672-527e-49c7-85e8-c13ecb888e56", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.\nTable:\n||  | WN-N P | WN-N R | WN-N F | WN-V P | WN-V R | WN-V F | VN P | VN R | VN F ||\n|| Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 ||\n|| type | .700 | .654 | .676 | .535 | .474 | .503 | .327 | .309 | .318 ||\n|| x+POS | .699 | .651 | .674 | .544 | .472 | .505 | .339 | .312 | .325 ||\n|| lemma | .706 | .660 | .682 | .576 | .520 | .547 | .384 | .360 | .371 ||\n|| x+POS | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold> ||\n|| Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep ||\n|| type | .712 | .661 | .686 | .545 | .457 | .497 | .324 | .296 | .310 ||\n|| x+POS | .715 | .659 | .686 | .560 | .464 | .508 | .349 | .320 | .334 ||\n|| lemma | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591 | .512 | .548 | .408 | .371 | .388 ||\n|| x+POS | .722 | .666 | .693 | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold> ||\n\nQuestion: Is it true that For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p ≤ .005) with no significant difference for WN-N and WN-V (p ≈ .05)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ba4e9b1e-818d-4a1e-97d9-9d7b71b2e18d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nTable:\n|| Model | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4 ||\n|| Shortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3 ||\n|| PRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9 ||\n|| PRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4 ||\n\nQuestion: Is it true that Supervising path attentions (the PRKGC+NS model) is not effective for improving the human interpretability of generated NLDs?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "fecfd170-1f8f-4f70-8b18-e211486982f2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1038 | 0.0170 | 0.0490 | 0.0641 | 0.0641 | 0.0613 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1282 | 0.0291 | 0.0410 | 0.0270 | 0.0270 | 0.1154 | 0.0661 ||\n|| P | PT | Europarl | 0.6185 | 0.3744 | 0.4144 | 0.4394 | 0.4394 |  0.7553 | 0.5676 ||\n||  | PT | Ted Talks | 0.6308 | 0.4124 | 0.4404 | 0.4515 | 0.4945 |  0.8609 | 0.5295 ||\n|| R | EN | Europarl |  0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0011 | 0.0008 | 0.0011 | 0.0008 | 0.0008 |  0.0030 | 0.0018 ||\n|| R | PT | Europarl | 0.0012 | 0.0008 | 0.0009 | 0.0010 | 0.0010 |  0.0016 | 0.0012 ||\n||  | PT | Ted Talks | 0.0003 | 0.0009 | 0.0009 | 0.0010 | 0.0010 |  0.0017 | 0.0011 ||\n|| F | EN | Europarl |  0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0022 | 0.0016 | 0.0022 | 0.0015 | 0.0015 |  0.0058 | 0.0036 ||\n|| F | PT | Europarl | 0.0024 | 0.0016 | 0.0018 | 0.0019 | 0.0019 |  0.0031 | 0.0023 ||\n||  | PT | Ted Talks | 0.0005 | 0.0018 | 0.0018 | 0.0020 | 0.0021 |  0.0034 | 0.0022 ||\n\nQuestion: Is it true that On the other hand, choosing the best hypernym worked very well for DocSub which obtained the best precision for the Portuguese corpora?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "52f9985b-e7c4-4516-a758-9eebf47cb971", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\nTable:\n||  ResNet-34 |  Eval set % |  #param ||\n|| Baseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M ||\n|| SA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M ||\n|| SA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M ||\n||  SA (S: 1,2,3 - B: 3) |  55.27 | } 0.107M ||\n\nQuestion: Is it true that  We showed that it is possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "2de15a72-9055-4a7e-897f-cf0c3c3aeb36", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nTable:\n|| Method | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success ||\n|| GP-MBCM | 2.99 | 19.04 | 44.29 | 28.9 ||\n|| ACER | 10.49 | 77.98 | 62.83 | 50.8 ||\n|| PPO | 9.83 | 83.34 | 69.09 | 59.1 ||\n|| ALDM | 12.47 | 81.20 | 62.60 | 61.2 ||\n|| GDPL-sess |  7.49 | 88.39 | 77.56 | 76.4 ||\n|| GDPL-discr | 7.86 | 93.21 | 80.43 | 80.5 ||\n|| GDPL | 7.64 |  94.97 |  83.90 |  86.5 ||\n||  Human |  7.37 |  66.89 |  95.29 |  75.0 ||\n\nQuestion: Is it true that  Though ALDM obtains a lower inform F1 and match rate than PPO, it gets a slight improvement  on task success  Ablation test is investigated in Table 3?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0f61a515-9f46-4502-a2db-a1ef9a6c2148", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.\nTable:\n|| System reference | BLEU↑ | TER↓ ||\n|| en-fr-rnn-rev | 33.3 | 50.2 ||\n|| en-fr-smt-rev | 36.5 | 47.1 ||\n|| en-fr-trans-rev |  36.8 |  46.8 ||\n|| en-es-rnn-rev | 37.8 | 45.0 ||\n|| en-es-smt-rev | 39.2 | 44.0 ||\n|| en-es-trans-rev |  40.4 |  42.7 ||\n\nQuestion: Is it true that we present BLEU and TER for the REV systems in Table 5,  While RNN models are the best ones according to the evaluation metrics,?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "9b0998ee-9b1d-472a-a9dc-19f4dc5c57a8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.\nTable:\n|| Feature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1 ||\n|| +BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 |  0.93 | 0.94 | 0.92 |  0.93 | 0.91 | 0.91 |  0.91 ||\n|| +BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89 ||\n|| +Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88 ||\n|| +BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n\nQuestion: Is it true that  Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) outperforms GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1)  the combination feature of BoC and sentence embeddings outperforms sentence embeddings alone, but do not exceed the upper boundary of BoC feature, in which again demonstrating the competitiveness of BoC feature?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "775b4ac6-9478-4306-b388-b0e8203c4ac0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Zero-Shot Grounding of Objects from Natural Language Queries Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600×600\nTable:\n|| Model | Accuracy on RefClef ||\n|| BM + Softmax | 48.54 ||\n|| BM + BCE | 55.20 ||\n|| BM + FL | 57.13 ||\n|| BM + FL + Img-Resize |  61.75 ||\n\nQuestion: Is it true that  Finally, image resizing gives another 4% increase?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3197bce9-5af1-44bb-ae78-af57c4346c14", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM Table 5: Confusion matrix for testing set predictions\nTable:\n||  LabelPrediction |  C |  D |  Q |  S ||\n||  Commenting | 760 | 0 | 12 | 6 ||\n||  Denying | 68 | 0 | 1 | 2 ||\n||  Querying | 69 | 0 | 36 | 1 ||\n||  Supporting | 67 | 0 | 1 | 26 ||\n\nQuestion: Is it true that Most denying instances get misclassified as querying (see Table 5),?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "919169e5-b0e4-4818-96d5-27895efad28b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\nTable:\n||  Emoji alias |  N |  emoji # |  emoji % |  no-emoji # |  no-emoji % |  Δ% ||\n|| mask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27 ||\n|| two_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59 ||\n|| heart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91 ||\n|| heart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75 ||\n|| rage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04 ||\n|| cry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07 ||\n|| sob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67 ||\n|| unamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00 ||\n|| weary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49 ||\n|| joy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05 ||\n|| sweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80 ||\n|| confused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60 ||\n\nQuestion: Is it true that  When removing sweat smile and confused accuracy decreased?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "65d85bed-5ff2-4954-92c9-f7e7cfd25951", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving Generalization by Incorporating Coverage in Natural Language Inference Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\nTable:\n||  | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK ||\n|| MQAN | 72.30 | 60.91 | 41.82 | 53.95 ||\n|| + coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold> ||\n|| ESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37 ||\n|| + coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold> ||\n\nQuestion: Is it true that The resulting cross-dataset improvements on the SNLI and Glockner datasets are larger than those on the SICK dataset?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ef13c2cf-6271-4c5c-a1ee-17e71aea7566", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Entity, Relation, and Event Extraction with Contextualized Span Representations Table 3: F1 scores on Relation.\nTable:\n||  | ACE05 | SciERC | WLPC ||\n|| BERT + LSTM | 60.6 | 40.3 | 65.1 ||\n|| +RelProp | 61.9 | 41.1 | 65.3 ||\n|| +CorefProp | 59.7 | 42.6 | - ||\n|| BERT FineTune |  62.1 | 44.3 | 65.4 ||\n|| +RelProp | 62.0 | 43.0 |  65.5 ||\n|| +CorefProp | 60.0 |  45.3 | - ||\n\nQuestion: Is it true that  Relation propagation (RelProp) improves relation extraction performance over both pretrained and fine-tuned BERT?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e9ed6a2c-59be-47fd-815e-3a08898c26c5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 2: Ablation study results.\nTable:\n||  Variation |  Accuracy (%) |  Δ% ||\n|| Submitted |  69.23 | - ||\n|| No emoji | 68.36 | - 0.87 ||\n|| No ELMo | 65.52 | - 3.71 ||\n|| Concat Pooling | 68.47 | - 0.76 ||\n|| LSTM hidden=4096 | 69.10 | - 0.13 ||\n|| LSTM hidden=1024 | 68.93 | - 0.30 ||\n|| LSTM hidden=512 | 68.43 | - 0.80 ||\n|| POS emb dim=100 | 68.99 | - 0.24 ||\n|| POS emb dim=75 | 68.61 | - 0.62 ||\n|| POS emb dim=50 | 69.33 | + 0.10 ||\n|| POS emb dim=25 | 69.21 | - 0.02 ||\n|| SGD optim lr=1 | 64.33 | - 4.90 ||\n|| SGD optim lr=0.1 | 66.11 | - 3.12 ||\n|| SGD optim lr=0.01 | 60.72 | - 8.51 ||\n|| SGD optim lr=0.001 | 30.49 | - 38.74 ||\n\nQuestion: Is it true that We performed an ablation study on a single model having obtained 69.23% accuracy on the validation set?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "2df5a453-0b55-4883-8f4a-a1486ebbb214", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Two Causal Principles for Improving Visual Dialog Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.\nTable:\n|| Model | baseline | QT | S   R0 | S   R1 | S   R2 | S   R3 | D ||\n|| LF  | 57.21 | 58.97 | 67.82 | 71.27 | 72.04 | 72.36 | 72.65 ||\n|| LF +P1 | 61.88 | 62.87 | 69.47 | 72.16 | 72.85 | 73.42 |  73.63 ||\n\nQuestion: Is it true that Overall, none of the implementations can improve the performances of base models?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "d697a74d-c867-4d09-950e-3d3f84fef4c5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result Lample et al. (2016).\nTable:\n|| Model | #Params | NER ||\n|| LSTM* | - | 90.94 ||\n|| LSTM | 245K |  89.61 ||\n|| GRU | 192K | 89.35 ||\n|| ATR | 87K | 88.46 ||\n|| SRU | 161K | 88.89 ||\n|| LRN | 129K | 88.56 ||\n\nQuestion: Is it true that As shown in Table 6, the performance of LRN matches that of ATR and SRU, though LSTM and GRU operate better (+1.05 and +0.79)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "dffdd441-9432-4656-830a-adf397ec3283", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.\nTable:\n||  Model |  Type |  English-German #P |  English-German B |  English-German C |  English-Czech #P |  English-Czech B |  English-Czech C ||\n|| BoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | - ||\n|| CNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | - ||\n|| BiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | - ||\n|| PB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3 ||\n|| DCGCN (ours) | Single |   29.7M |  19.0 |  44.1 |   28.3M |  12.1 |  37.1 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9 ||\n|| DCGCN (ours) | Ensemble |   149M |  20.5 |  45.8 |   142M |  13.1 |  37.8 ||\n\nQuestion: Is it true that Our models DCGCN(single) and DCGCN(ensemble) do not remove the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers, as evidenced by the results of BoW+GCN, CNN+GCN, and BiRNN+GCN?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a1f1fc3a-e648-4af9-84ce-3b59c2a584d5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Dim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n|| 400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7 ||\n|| 400 | CMOW/400 |  34.4 | 68.8 | 80.1 |  79.9 |  59.8 | 81.9 |  79.2 |  70.7 |  50.3 | 70.7 ||\n|| 400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 |  87.2 ||\n|| 400 | H-CMOW | 32.3 |  70.8 |  81.3 | 76.0 | 59.6 |  82.3 | 77.4 | 70.0 | 50.2 | 38.2 ||\n|| 784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 |  89.5 ||\n|| 784 | CMOW/784 |  35.1 |  70.8 |  82.0 | 80.2 |  61.8 | 82.8 |  79.7 | 74.2 |  50.7 | 72.9 ||\n|| 800 | Hybrid | 35.0 |  70.8 | 81.7 |  81.0 | 59.4 |  84.4 | 79.0 |  74.3 | 49.3 | 87.6 ||\n|| - | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1% ||\n|| - | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9% ||\n\nQuestion: Is it true that Consequently, with an 8% decrease on average, the hybrid model  Word Content are decreased?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "cd352a89-328c-4845-bf4c-d60c006603a7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Method | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R ||\n|| CBOW/784 | 90.0 |  79.2 |  74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 |  78.1 ||\n|| CMOW/784 | 87.5 | 73.4 | 70.6 |  87.3 | 69.6 |  88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2 ||\n|| Hybrid |  90.2 | 78.7 | 73.7 |  87.3 |  72.7 | 87.6 |  79.4 |  79.6 |  43.3 |  63.4 | 77.8 ||\n|| cmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4% ||\n|| cmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1% ||\n\nQuestion: Is it true that On 7 out of 11 supervised tasks, the joint model even improves upon the better model, and on SST2, SST5, and MRPC the difference is more than 1 point?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "362601b1-3bf5-47a7-a8ab-192050bbeea7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| Encoder Modules |  |  ||\n|| -Linear Combination | 23.7 | 53.2 ||\n|| -Global Node | 24.2 | 54.6 ||\n|| -Direction Aggregation | 24.6 | 54.6 ||\n|| -Graph Attention | 24.9 | 54.7 ||\n|| -Global Node&Linear Combination | 22.9 | 52.4 ||\n|| Decoder Modules |  |  ||\n|| -Coverage Mechanism | 23.8 | 53.0 ||\n\nQuestion: Is it true that This suggests that enriching input graphs with the global node and excluding the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "cb276238-d6d0-427f-9a25-e923b519df9b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nTable:\n|| Batch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear ||\n|| 1 | 46.7 | 27.3 | 7.6 ||\n|| 10 | 125.2 | 78.2 | 22.7 ||\n|| 25 | 129.7 | 83.1 | 45.4 ||\n\nQuestion: Is it true that For all batch sizes, the training throughput on the balanced dataset is the highest, while the throughput on the linear dataset is the lowest?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "d0317274-0a8c-4613-b192-9b9d0da2ca72", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.\nTable:\n|| System | NC-v11 BLEU | NC-v11 TER↓ | NC-v11 Meteor | Full BLEU | Full TER↓ | Full Meteor ||\n|| OpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225 ||\n|| Transformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344 ||\n|| Seq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258 ||\n|| Dual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246 ||\n|| Duel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223 ||\n|| Dual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328 ||\n|| Dual2seq |  *19.2* |  0.6305 |  0.3840 |  *25.5* |  0.5480 |  0.4376 ||\n\nQuestion: Is it true that  Dual2seq is signifi  cantly better than Seq2seq in both settings,  In particular, the improvement is much larger under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "a5635632-7ec5-4631-bf69-893fe583a701", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1038 | 0.0170 | 0.0490 | 0.0641 | 0.0641 | 0.0613 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1282 | 0.0291 | 0.0410 | 0.0270 | 0.0270 | 0.1154 | 0.0661 ||\n|| P | PT | Europarl | 0.6185 | 0.3744 | 0.4144 | 0.4394 | 0.4394 |  0.7553 | 0.5676 ||\n||  | PT | Ted Talks | 0.6308 | 0.4124 | 0.4404 | 0.4515 | 0.4945 |  0.8609 | 0.5295 ||\n|| R | EN | Europarl |  0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0011 | 0.0008 | 0.0011 | 0.0008 | 0.0008 |  0.0030 | 0.0018 ||\n|| R | PT | Europarl | 0.0012 | 0.0008 | 0.0009 | 0.0010 | 0.0010 |  0.0016 | 0.0012 ||\n||  | PT | Ted Talks | 0.0003 | 0.0009 | 0.0009 | 0.0010 | 0.0010 |  0.0017 | 0.0011 ||\n|| F | EN | Europarl |  0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0022 | 0.0016 | 0.0022 | 0.0015 | 0.0015 |  0.0058 | 0.0036 ||\n|| F | PT | Europarl | 0.0024 | 0.0016 | 0.0018 | 0.0019 | 0.0019 |  0.0031 | 0.0023 ||\n||  | PT | Ted Talks | 0.0005 | 0.0018 | 0.0018 | 0.0020 | 0.0021 |  0.0034 | 0.0022 ||\n\nQuestion: Is it true that  Analyzing Table 5 we observe that Patt achieves again the best precision values for the English corpora?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ed4eb2c0-b3d0-4980-bf4b-9425b6ef4eb9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nTable:\n|| Model | Training data | Overall | Easy | Hard ||\n|| BERT-large-FT | B-COPA | 74.5 (± 0.7) | 74.7 (± 0.4) |  74.4 (± 0.9) ||\n|| BERT-large-FT | B-COPA (50%) | 74.3 (± 2.2) | 76.8 (± 1.9) | 72.8 (± 3.1) ||\n|| BERT-large-FT | COPA |  76.5 (± 2.7) |  83.9 (± 4.4) | 71.9 (± 2.5) ||\n|| RoBERTa-large-FT | B-COPA |  89.0 (± 0.3) | 88.9 (± 2.1) |  89.0 (± 0.8) ||\n|| RoBERTa-large-FT | B-COPA (50%) | 86.1 (± 2.2) | 87.4 (± 1.1) | 85.4 (± 2.9) ||\n|| RoBERTa-large-FT | COPA | 87.7 (± 0.9) |  91.6 (± 1.1) | 85.3 (± 2.0) ||\n\nQuestion: Is it true that The smaller performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely less on superficial cues?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "43e1a4df-038c-4b24-9c68-e1455a5a77a7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.\nTable:\n|| Train | Test |  System |  BLEU |  NIST |  METEOR |  ROUGE-L |  CIDEr |  Add |  Miss |  Wrong |  SER ||\n|| Original |  Original | TGen− | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94 ||\n|| Original |  Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27 ||\n|| Original |  Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80 ||\n|| Original |  Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen− | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37 ||\n|| Cleaned missing |  Original | TGen− | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61 ||\n|| Cleaned missing |  Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53 ||\n|| Cleaned missing |  Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen− | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45 ||\n\nQuestion: Is it true that However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without.11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "9602fe65-f85f-4388-81ee-1fd46873e99b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 1: Image-caption ranking results for English (Multi30k)\nTable:\n||  | Image to Text R@1 | Image to Text R@5 | Image to Text R@10 | Image to Text Mr | Text to Image R@1 | Text to Image R@5 | Text to Image R@10 | Text to Image Mr | Alignment ||\n||  symmetric |  |  |  |  |  |  |  |  |  ||\n|| Parallel gella:17 | 31.7 | 62.4 | 74.1 | 3 | 24.7 | 53.9 | 65.7 | 5 | - ||\n|| UVS kiros:15 | 23.0 | 50.7 | 62.9 | 5 | 16.8 | 42.0 | 56.5 | 8 | - ||\n|| EmbeddingNet wang:18 | 40.7 | 69.7 | 79.2 | - | 29.2 | 59.6 | 71.7 | - | - ||\n|| sm-LSTM huang:17 | 42.5 | 71.9 | 81.5 | 2 | 30.2 | 60.4 | 72.3 | 3 | - ||\n|| VSE++ faghri:18 |  43.7 | 71.9 | 82.1 | 2 | 32.3 | 60.9 | 72.1 | 3 | - ||\n|| Mono | 41.4 | 74.2 | 84.2 | 2 | 32.1 | 63.0 | 73.9 | 3 | - ||\n|| FME | 39.2 | 71.1 | 82.1 | 2 | 29.7 | 62.5 | 74.1 | 3 | 76.81% ||\n|| AME | 43.5 |  77.2 |  85.3 |  2 |  34.0 |  64.2 |  75.4 |  3 | 66.91% ||\n||  asymmetric |  |  |  |  |  |  |  |  |  ||\n|| Pivot gella:17 | 33.8 | 62.8 | 75.2 | 3 | 26.2 | 56.4 | 68.4 | 4 | - ||\n|| Parallel gella:17 | 31.5 | 61.4 | 74.7 | 3 | 27.1 | 56.2 | 66.9 | 4 | - ||\n|| Mono | 47.7 | 77.1 | 86.9 | 2 | 35.8 | 66.6 | 76.8 | 3 | - ||\n|| FME | 44.9 | 76.9 | 86.4 | 2 | 34.2 | 66.1 | 77.1 | 3 | 76.81% ||\n|| AME |  50.5 |  79.7 |  88.4 |  1 |  38.0 |  68.5 |  78.4 |  2 | 73.10% ||\n\nQuestion: Is it true that FME performs better than AME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "cc03ab7e-6e0d-45fd-a054-8cf38d14a2a1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).\nTable:\n|| Model | #Params | Base | +Elmo ||\n|| rnet* | - | 71.1/79.5 | -/- ||\n|| LSTM | 2.67M |  70.46/78.98 | 75.17/82.79 ||\n|| GRU | 2.31M | 70.41/  79.15 | 75.81/83.12 ||\n|| ATR | 1.59M | 69.73/78.70 | 75.06/82.76 ||\n|| SRU | 2.44M | 69.27/78.41 | 74.56/82.50 ||\n|| LRN | 2.14M | 70.11/78.83 |  76.14/  83.83 ||\n\nQuestion: Is it true that After integrating Elmo for contextual modeling, the performance of LRN does not reach the best (76.1 EM and 83.83 F1), with GRU and LSTM outperforming it (+0.33EM, +0.71F1)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "737a3ba3-f0ff-476c-b995-bb8fc27b877e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nTable:\n|| Cue | App. | Prod. | Cov. ||\n|| in | 47 | 55.3 | 9.40 ||\n|| was | 55 | 61.8 | 11.0 ||\n|| to | 82 | 40.2 | 16.4 ||\n|| the | 85 | 38.8 | 17.0 ||\n|| a | 106 | 57.5 | 21.2 ||\n\nQuestion: Is it true that Its productivity of 57.5% expresses that it appears in in correct alternatives 7.5% more often than expected by random chance?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "36419694-fbe7-448e-ab77-5a057e25f499", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 4: Scores for different training objectives on the linguistic probing tasks.\nTable:\n|| Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n|| CMOW-C |  36.2 | 66.0 | 81.1 | 78.7 | 61.7 |  83.9 | 79.1 | 73.6 | 50.4 | 66.8 ||\n|| CMOW-R | 35.1 |  70.8 |  82.0 |  80.2 |  61.8 | 82.8 |  79.7 |  74.2 |  50.7 |  72.9 ||\n|| CBOW-C |  34.3 |  50.5 |  79.8 |  79.9 | 53.0 |  75.9 |  79.8 |  72.9 | 48.6 | 89.0 ||\n|| CBOW-R | 33.0 | 49.6 | 79.3 | 78.4 |  53.6 | 74.5 | 78.6 | 72.0 |  49.6 |  89.5 ||\n\nQuestion: Is it true that While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points lower scores on WordContent  and BigramShift?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "a859b35d-7e4b-4d4d-b124-9e84252c100b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Low-supervision urgency detection and transfer in short crisis messages TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal\nTable:\n|| System | Accuracy | Precision | Recall | F-Measure ||\n|| Local | 63.97% | 64.27% | 64.50% | 63.93% ||\n|| Manual | 64.25% |  70.84%∗∗ | 48.50% | 57.11% ||\n|| Wiki | 67.25% | 66.51% | 69.50% | 67.76% ||\n|| Local-Manual | 65.75% | 67.96% | 59.50% | 62.96% ||\n|| Wiki-Local | 67.40% | 65.54% | 68.50% | 66.80% ||\n|| Wiki-Manual | 67.75% | 70.38% | 63.00% | 65.79% ||\n||  Our Approach |  69.25%∗∗∗ | 68.76% |  70.50%∗∗ |  69.44%∗∗∗ ||\n\nQuestion: Is it true that The results illustrate the viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "d3085af2-d938-41fe-8453-0c632cca7716", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.\nTable:\n||  Model |  Type |  English-German #P |  English-German B |  English-German C |  English-Czech #P |  English-Czech B |  English-Czech C ||\n|| BoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | - ||\n|| CNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | - ||\n|| BiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | - ||\n|| PB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3 ||\n|| DCGCN (ours) | Single |   29.7M |  19.0 |  44.1 |   28.3M |  12.1 |  37.1 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9 ||\n|| DCGCN (ours) | Ensemble |   149M |  20.5 |  45.8 |   142M |  13.1 |  37.8 ||\n\nQuestion: Is it true that BoW+GCN, CNN+GCN and BiRNN+GCN refer to employing the following encoders with a GCN layer on top respectively: 1) a bag-of-words encoder, 2) a one-layer CNN, 3) a bidirectional RNN?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b9b60316-88c2-497b-a548-5474b6280198", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE IX: Accuracies (%) for Sentiment Classification Task\nTable:\n|| GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed ||\n|| 77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26 ||\n\nQuestion: Is it true that Pretrained Word2Sense embeddings outperform our method, however it has the advantage of training on a larger corpus?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "73f747e6-bd1f-459a-be5d-557593d0128c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.\nTable:\n|| Model | Encoder |  Reg. loss (Eq. ( 1 ))   ρ |  Reg. loss (Eq. ( 1 ))   r |  Reg. loss (Eq. ( 1 )) G-Pre |  Reg. loss (Eq. ( 1 )) G-Rec |  Pref. loss (Eq. ( 3 ))   ρ |  Pref. loss (Eq. ( 3 ))   r |  Pref. loss (Eq. ( 3 )) G-Pre |  Pref. loss (Eq. ( 3 )) G-Rec ||\n|| MLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524 ||\n|| MLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556 ||\n|| MLP | BERT |  .487 |  .526 |  .544 |  .597 |  .505 |  .531 |  .556 |  .608 ||\n|| SimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549 ||\n|| SimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551 ||\n|| SimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533 ||\n|| Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174 ||\n\nQuestion: Is it true that MLP with BERT as en(2018) coder has the best overall performance?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "a2ada531-d61d-4735-836c-cbb20e8c7d46", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.\nTable:\n||  GCN +RC (2) | B 16.8 | C 48.1 |  GCN +RC+LA (2) | B 18.3 | C 47.9 ||\n|| +RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1 ||\n|| +RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8 ||\n|| +RC (9) |  21.1 | 50.5 | +RC+LA (9) |  22.0 | 52.6 ||\n|| +RC (10) | 20.7 |  50.7 | +RC+LA (10) | 21.2 |  52.9 ||\n|| DCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7 ||\n|| DCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) |  25.5 |  55.4 ||\n\nQuestion: Is it true that When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on AMR15 development set?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ced2b584-1a28-45df-92fc-3613d7dbcf34", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 7: Comparisons of different DCGCN models under almost the same parameter budget.\nTable:\n||  Model | D | #P | B | C ||\n|| DCGCN(1) | 300 | 10.9M | 20.9 | 52.0 ||\n|| DCGCN(2) | 180 | 10.9M |  22.2 |  52.3 ||\n|| DCGCN(2) | 240 | 11.3M | 22.8 | 52.8 ||\n|| DCGCN(4) | 180 | 11.4M |  23.4 |  53.4 ||\n|| DCGCN(1) | 420 | 12.6M | 22.2 | 52.4 ||\n|| DCGCN(2) | 300 | 12.5M | 23.8 | 53.8 ||\n|| DCGCN(3) | 240 | 12.3M |  23.9 |  54.1 ||\n|| DCGCN(2) | 360 | 14.0M | 24.2 |  54.4 ||\n|| DCGCN(3) | 300 | 14.0M |  24.4 | 54.2 ||\n|| DCGCN(2) | 420 | 15.6M | 24.1 | 53.7 ||\n|| DCGCN(4) | 300 | 15.6M |  24.6 |  54.8 ||\n|| DCGCN(3) | 420 | 18.6M | 24.5 | 54.6 ||\n|| DCGCN(4) | 360 | 18.4M |  25.5 |  55.4 ||\n\nQuestion: Is it true that Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "2496440e-30cf-465a-8de6-814a13e9b1f5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 1: Results on belinkov2014exploring’s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Schütze (2015) on GloVe.\nTable:\n||  System |  Initialization |  Embedding |  Resources |  Test Acc. ||\n|| HPCD (full) | Syntactic-SG | Type | WordNet, VerbNet | 88.7 ||\n|| LSTM-PP | GloVe | Type | - | 84.3 ||\n|| LSTM-PP | GloVe-retro | Type | WordNet | 84.8 ||\n|| OntoLSTM-PP | GloVe-extended | Token | WordNet |  89.7 ||\n\nQuestion: Is it true that OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0e60f569-bde5-4ada-8fbc-6176240824bf", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 1: Multi-action evaluation results. The “w” and “w/o” column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nTable:\n|| Model & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/ ||\n|| Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines ||\n|| DAMD + greedy |  1.00 |  1.00 | 1.95 |  2.51 ||\n|| HDSA + fixed threshold |  1.00 |  1.00 | 2.07 |  2.40 ||\n|| 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation ||\n|| DAMD + beam search | 2.67 |  2.87 | 3.36 |  4.39 ||\n|| DAMD + diverse beam search | 2.68 |  2.88 | 3.41 |  4.50 ||\n|| DAMD + top-k sampling | 3.08 |  3.43 | 3.61 |  4.91 ||\n|| DAMD + top-p sampling | 3.08 |  3.40 | 3.79 |  5.20 ||\n|| HDSA + sampled threshold | 1.32 |  1.50 | 3.08 |  3.31 ||\n|| 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation ||\n|| DAMD + beam search | 3.06 |  3.39 | 4.06 |  5.29 ||\n|| DAMD + diverse beam search | 3.05 |  3.39 | 4.05 |  5.31 ||\n|| DAMD + top-k sampling | 3.59 |  4.12 | 4.21 |  5.77 ||\n|| DAMD + top-p sampling | 3.53 |  4.02 | 4.41 |  6.17 ||\n|| HDSA + sampled threshold | 1.54 |  1.83 | 3.42 |  3.92 ||\n\nQuestion: Is it true that  After applying our data augmentation, both the action and slot diversity are improved consistently,  HDSA has the better performance and benefits more from data augmentation comparing to our proposed domain-aware multi-decoder network?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a2e66d42-21d8-4914-9262-6b5dac5f738d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Low-supervision urgency detection and transfer in short crisis messages TABLE II: Details on datasets used for experiments.\nTable:\n|| Dataset | Unlabeled / Labeled Messages | Urgent / Non-urgent Messages | Unique Tokens | Avg. Tokens / Message | Time Range ||\n|| Nepal | 6,063/400 | 201/199 | 1,641 | 14 | 04/05/2015-05/06/2015 ||\n|| Macedonia | 0/205 | 92/113 | 129 | 18 | 09/18/2018-09/21/2018 ||\n|| Kerala | 92,046/400 | 125/275 | 19,393 | 15 | 08/17/2018-08/22/2018 ||\n\nQuestion: Is it true that Table II shows that Nepal is roughly balanced, while Kerala is imbalanced?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "2ff59490-1779-47f3-828f-e2c1600f71e9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nTable:\n|| Dataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb ||\n||  Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 |  76.41 ||\n||  WinoCoref | AntePre | 68.37 | 74.32 | —– | 88.48 | 88.95 |  89.32 ||\n\nQuestion: Is it true that On the WinoCoref dataset, KnowComb does not improve by 15%?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "fb607d1d-2d8e-4b05-ac94-895e601e5682", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VII: Precision scores for the Analogy Test\nTable:\n|| Methods | # dims | Analg. (sem) | Analg. (syn) | Total ||\n|| GloVe | 300 | 78.94 | 64.12 | 70.99 ||\n|| Word2Vec | 300 | 81.03 | 66.11 | 73.03 ||\n|| OIWE-IPG | 300 | 19.99 | 23.44 | 21.84 ||\n|| SOV | 3000 | 64.09 | 46.26 | 54.53 ||\n|| SPINE | 1000 | 17.07 | 8.68 | 12.57 ||\n|| Word2Sense | 2250 | 12.94 | 19.44 | 5.84 ||\n|| Proposed | 300 | 79.96 | 63.52 | 71.15 ||\n\nQuestion: Is it true that However, our proposed method has comparable performance with the original GloVe embeddings?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ab2c2f1d-1b29-4c35-81bd-e2ea6c8073c8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Two Causal Principles for Improving Visual Dialog Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.\nTable:\n|| Model | LF  | HCIAE  | CoAtt  | RvA  ||\n|| baseline | 57.21 | 56.98 | 56.46 | 56.74 ||\n|| +P1 | 61.88 | 60.12 | 60.27 | 61.02 ||\n|| +P2 | 72.65 | 71.50 | 71.41 | 71.44 ||\n|| +P1+P2 |  73.63 | 71.99 | 71.87 | 72.88 ||\n\nQuestion: Is it true that In general, our principle P2 can improve all the models in any ablative condition (i.e., P1, P2, P1+P2), while P1 does not always lead to an improvement?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ca69f7e8-9e0e-4870-859c-649e8f88eceb", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: What do Deep Networks Like to Read? Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.\nTable:\n||  | <bold>RNN</bold> | <bold>CNN</bold> | <bold>DAN</bold> ||\n|| Positive | +9.7 | +4.3 | +<bold>23.6</bold> ||\n|| Negative | +6.9 | +5.5 | +<bold>16.1</bold> ||\n|| Flipped to Positive | +20.2 | +24.9 | +27.4 ||\n|| Flipped to Negative | +31.5 | +28.6 | +19.3 ||\n\nQuestion: Is it true that By considering only adjectives, we obtain a measure of the positive and negative score for each sentence before and after fine-tuning?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f30d3d9e-a1e3-4e50-a778-882897039098", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Table 6: Performance of our baselines on the development set. Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.\nTable:\n||  | M | F | B | O ||\n|| Random | 43.6 | 39.3 |  0.90 | 41.5 ||\n|| Token Distance | 50.1 | 42.4 |  0.85 | 46.4 ||\n|| Topical Entity | 51.5 | 43.7 |  0.85 | 47.7 ||\n|| Syntactic Distance | 63.0 | 56.2 |  0.89 | 59.7 ||\n|| Parallelism |  67.1 |  63.1 |    0.94 |  65.2 ||\n|| Parallelism+URL |  71.1 |  66.9 |    0.94 |  69.0 ||\n|| Transformer-Single | 58.6 | 51.2 |  0.87 | 55.0 ||\n|| Transformer-Multi | 59.3 | 52.9 |  0.89 | 56.2 ||\n\nQuestion: Is it true that  TRANSFORMER-MULTI is stronger than TRANSFORMER-SINGLE  .2% overall improvement over TRANSFORMER-SINGLE for the goldtwo-mention task?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "5bf06f43-9f2d-4b94-a6da-8fc836362016", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.\nTable:\n|| Train | Test |  System |  BLEU |  NIST |  METEOR |  ROUGE-L |  CIDEr |  Add |  Miss |  Wrong |  SER ||\n|| Original |  Original | TGen− | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94 ||\n|| Original |  Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27 ||\n|| Original |  Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80 ||\n|| Original |  Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen− | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37 ||\n|| Cleaned missing |  Original | TGen− | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61 ||\n|| Cleaned missing |  Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53 ||\n|| Cleaned missing |  Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen− | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45 ||\n\nQuestion: Is it true that In other words,  However, the results in bottom halves  of Tables 2 and 3 do not support our hypothesis: we observe no significant effect on SER from cleaning the missed slots?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "6468a0b4-715d-495e-9627-1f859a1198cb", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).\nTable:\n|| Model | #Params | Base | +Elmo ||\n|| rnet* | - | 71.1/79.5 | -/- ||\n|| LSTM | 2.67M |  70.46/78.98 | 75.17/82.79 ||\n|| GRU | 2.31M | 70.41/  79.15 | 75.81/83.12 ||\n|| ATR | 1.59M | 69.73/78.70 | 75.06/82.76 ||\n|| SRU | 2.44M | 69.27/78.41 | 74.56/82.50 ||\n|| LRN | 2.14M | 70.11/78.83 |  76.14/  83.83 ||\n\nQuestion: Is it true that In this task, ATR and SRU outperform LRN in terms of both EM and F1 score?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a8cb7bdd-dbe4-4991-8ecd-99ee870ab569", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nTable:\n|| Model | Method | Training Data | Overall | Easy | Hard | p-value (%) ||\n|| goodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8 ||\n|| gordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5 ||\n|| sasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8∗ ||\n|| Word frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8 ||\n|| BERT-large-FT | LM, NSP | COPA | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗ ||\n|| RoBERTa-large-FT | LM | COPA | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗ ||\n\nQuestion: Is it true that We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets. As Table 4 shows, previous models perform significantly better on the Easy subset than on the Hard subset, with the exception of Sasaki et al?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "4e1bee90-35f0-4df8-ba48-b2a758e2d9d6", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.\nTable:\n||  k | Ar | Es | Fr | Ru | Zh | En ||\n|| POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy ||\n|| 0 | 88.0 | 87.9 | 87.9 | 87.8 | 87.7 | 87.4 ||\n|| 1 | 92.4 | 91.9 | 92.1 | 92.1 | 91.5 | 89.4 ||\n|| 2 | 91.9 | 91.8 | 91.8 | 91.8 | 91.3 | 88.3 ||\n|| 3 | 92.0 | 92.3 | 92.1 | 91.6 | 91.2 | 87.9 ||\n|| 4 | 92.1 | 92.4 | 92.5 | 92.0 | 90.5 | 86.9 ||\n|| SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy ||\n|| 0 | 81.9 | 81.9 | 81.8 | 81.8 | 81.8 | 81.2 ||\n|| 1 | 87.9 | 87.7 | 87.8 | 87.9 | 87.7 | 84.5 ||\n|| 2 | 87.4 | 87.5 | 87.4 | 87.3 | 87.2 | 83.2 ||\n|| 3 | 87.8 | 87.9 | 87.9 | 87.3 | 87.3 | 82.9 ||\n|| 4 | 88.3 | 88.6 | 88.4 | 88.1 | 87.7 | 82.1 ||\n|| BLEU | BLEU | BLEU | BLEU | BLEU | BLEU | BLEU ||\n||  | 32.7 | 49.1 | 38.5 | 34.2 | 32.1 | 96.6 ||\n\nQuestion: Is it true that  Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 only marginally boost the performance to around 87-88%,  which is not significantly higher than the UnsupEmb and MFT baselines?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "59e2114d-5576-4698-9ac1-bea4da38592d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. ↓ indicates the lower the better.\nTable:\n|| System | NC-v11 BLEU | NC-v11 TER↓ | NC-v11 Meteor | Full BLEU | Full TER↓ | Full Meteor ||\n|| OpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225 ||\n|| Transformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344 ||\n|| Seq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258 ||\n|| Dual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246 ||\n|| Duel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223 ||\n|| Dual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328 ||\n|| Dual2seq |  *19.2* |  0.6305 |  0.3840 |  *25.5* |  0.5480 |  0.4376 ||\n\nQuestion: Is it true that Dual2seq is not consistently better than the other systems under all three metrics,  as OpenNMT-tf and Transformer-tf both outperform Dual2seq in terms of BLEU and Meteor scores?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "dad6a4ed-cf24-42d0-9293-ac3ed0d9efcf", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set\nTable:\n|| Method | WER (%) | DCE ||\n|| No enhancement | 38.4 | 0.958 ||\n|| Wiener filter | 41.0 | 0.775 ||\n|| Minimizing DCE | 31.1 |  0.392 ||\n|| FSEGAN | 29.1 | 0.421 ||\n|| AAS (  wAC=1,  wAD=0) | 27.7 | 0.476 ||\n|| AAS (  wAC=1,  wAD=105) |  26.1 | 0.462 ||\n|| Clean speech | 9.3 | 0.0 ||\n\nQuestion: Is it true that The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "1b8acf49-27ee-44b2-a81b-2ea143aa00f0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1\nTable:\n|| Dataset | Class | ˆ  piblack | ˆ  piwhite |  t |  p | ˆ  piblackˆ  piwhite ||\n||  Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505 ||\n||  | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724 ||\n||  Waseem | Racism | 0.001 | 0.001 | 0.035 |  | 1.001 ||\n||  | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993 ||\n||  | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120 ||\n||  Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573 ||\n||  | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653 ||\n||  Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396 ||\n||  Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812 ||\n||  | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239 ||\n||  | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854 ||\n\nQuestion: Is it true that (2017), we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech compared to 2% of those in the white-aligned set?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "88f1cf27-946a-4442-98dc-02d34534f76e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\nTable:\n||  | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall ||\n|| softmax | ✓ |  | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97 ||\n|| sparsemax | ✓ |  | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94 ||\n|| soft-TVmax | ✓ |  | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11 ||\n|| sparse-TVmax | ✓ |  | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17 ||\n|| softmax |  | ✓ | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04 ||\n|| sparsemax |  | ✓ |  85.40 |  50.87 | 58.67 | 68.79 |  85.80 | 50.18 | 59.08 | 69.19 ||\n|| softmax | ✓ | ✓ | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17 ||\n|| sparse-TVmax | ✓ | ✓ | 85.35 | 50.52 |  59.15 |  68.96 | 85.72 |  50.66 |  59.22 |  69.28 ||\n\nQuestion: Is it true that Additionally, when using bounding box features, sparsemax outperforms softmax, showing that selecting only the bounding boxes of the relevant objects leads to a better answering capability?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b90516c0-5d54-4636-b318-cdc83d2fce12", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.\nTable:\n||  | MFT | UnsupEmb | Word2Tag ||\n|| POS | 91.95 | 87.06 | 95.55 ||\n|| SEM | 82.00 | 81.11 | 91.41 ||\n\nQuestion: Is it true that The UnsupEmb baseline performs comparably to the Word2Tag upper bound on both POS and SEM tagging?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "406069b0-825f-4f0b-b982-048d1e765fcf", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 4: Lexicon member coverage (%)\nTable:\n|| target | VN | WN-V | WN-N ||\n|| type | 81 | 66 | 47 ||\n|| x+POS | 54 | 39 | 43 ||\n|| lemma | 88 | 76 | 53 ||\n|| x+POS | 79 | 63 | 50 ||\n|| shared | 54 | 39 | 41 ||\n\nQuestion: Is it true that POS-disambiguation does not fragment the vocabulary and consistently increases the coverage with the effect being more pronounced for lemmatized targets?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "c9a67956-a20f-488b-bd85-062a6fc04a01", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1\nTable:\n|| Dataset | Class | ˆ  piblack | ˆ  piwhite |  t |  p | ˆ  piblackˆ  piwhite ||\n||  Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505 ||\n||  | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724 ||\n||  Waseem | Racism | 0.001 | 0.001 | 0.035 |  | 1.001 ||\n||  | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993 ||\n||  | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120 ||\n||  Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573 ||\n||  | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653 ||\n||  Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396 ||\n||  Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812 ||\n||  | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239 ||\n||  | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854 ||\n\nQuestion: Is it true that In most cases the racial disparities persist, and are generally larger in magnitude than the disparities for other classes?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "c67da597-da7b-4c75-99a6-5184dfb0c485", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Entity, Relation, and Event Extraction with Contextualized Span Representations Table 3: F1 scores on Relation.\nTable:\n||  | ACE05 | SciERC | WLPC ||\n|| BERT + LSTM | 60.6 | 40.3 | 65.1 ||\n|| +RelProp | 61.9 | 41.1 | 65.3 ||\n|| +CorefProp | 59.7 | 42.6 | - ||\n|| BERT FineTune |  62.1 | 44.3 | 65.4 ||\n|| +RelProp | 62.0 | 43.0 |  65.5 ||\n|| +CorefProp | 60.0 |  45.3 | - ||\n\nQuestion: Is it true that CorefProp also improves relation extraction on SciERC?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "fede62ad-8591-411a-974e-a263d0e6dd91", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nTable:\n|| Model | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time ||\n|| Rocktäschel et al. ( 2016 ) | Rocktäschel et al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | - ||\n|| This | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 |  90.49 | 0.696 ||\n|| This | GRU | 6.41M |  85.71 | 0.245 |  86.05 | 0.419 |  90.29 | 0.529 | 90.10 | 0.695 ||\n|| This | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580 ||\n|| Work | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555 ||\n||  | LRN | 4.25M | 84.88 |  0.209 | 85.06 |  0.223 | 89.98 |  0.488 | 89.93 |  0.506 ||\n\nQuestion: Is it true that LRN obtains an accuracy of 90.49 with BERT, the highest among all models?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "96bf0f6b-f429-4648-bab7-cf3759539016", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).\nTable:\n||  | dev CS | dev mono | test CS | test mono ||\n|| CS-only-LM | 45.20 | 65.87 | 43.20 | 62.80 ||\n|| Fine-Tuned-LM | 49.60 | 72.67 | 47.60 | 71.33 ||\n|| CS-only-disc |  75.60 | 70.40 | 70.80 | 70.53 ||\n|| Fine-Tuned-disc | 70.80 |  74.40 |  75.33 |  75.87 ||\n\nQuestion: Is it true that Table 5 breaks down the results of the different models according to two conditions: when the gold sentence is code-switched, and when the gold sentence is monolingual?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "03375542-1aaf-400c-9743-0e332dd4183b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nTable:\n|| Recall | 0.1 | 0.2 | 0.3 | AUC ||\n|| -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 ||\n|| -Capsule | 0.635 | 0.507 | 0.413 | 0.386 ||\n|| Our Model | 0.650 | 0.519 | 0.422 | 0.405 ||\n\nQuestion: Is it true that According to the table, the drop of precision demonstrates that the word-level attention is quite useful?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "966e1252-7bc6-49de-8cde-ddf9dd9771a0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 4: Scores for different training objectives on the linguistic probing tasks.\nTable:\n|| Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n|| CMOW-C |  36.2 | 66.0 | 81.1 | 78.7 | 61.7 |  83.9 | 79.1 | 73.6 | 50.4 | 66.8 ||\n|| CMOW-R | 35.1 |  70.8 |  82.0 |  80.2 |  61.8 | 82.8 |  79.7 |  74.2 |  50.7 |  72.9 ||\n|| CBOW-C |  34.3 |  50.5 |  79.8 |  79.9 | 53.0 |  75.9 |  79.8 |  72.9 | 48.6 | 89.0 ||\n|| CBOW-R | 33.0 | 49.6 | 79.3 | 78.4 |  53.6 | 74.5 | 78.6 | 72.0 |  49.6 |  89.5 ||\n\nQuestion: Is it true that While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points higher scores on WordContent and BigramShift?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "63fe7961-86a5-4ceb-9556-1f4592c15b2c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| Encoder Modules |  |  ||\n|| -Linear Combination | 23.7 | 53.2 ||\n|| -Global Node | 24.2 | 54.6 ||\n|| -Direction Aggregation | 24.6 | 54.6 ||\n|| -Graph Attention | 24.9 | 54.7 ||\n|| -Global Node&Linear Combination | 22.9 | 52.4 ||\n|| Decoder Modules |  |  ||\n|| -Coverage Mechanism | 23.8 | 53.0 ||\n\nQuestion: Is it true that After removing the graph attention module, our model gives 24.9 BLEU points?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "9df0b311-fa64-4aaa-b766-568444a3f1a9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.\nTable:\n||  m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1 ||\n|| 1 | 0.541 | 0.595 |  0.566 | 0.495 | 0.621 | 0.551 ||\n|| 2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555 ||\n|| 3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564 ||\n|| 4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 |  0.571 ||\n|| 5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567 ||\n\nQuestion: Is it true that We observe that for the NYT10 dataset, m = {1, 2, 3} gives good performance with m = 1 achieving the highest F1 score?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3ce2ce6f-939a-4a30-b4ec-06b484e1ae64", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nTable:\n|| Model | Method | Training Data | Overall | Easy | Hard | p-value (%) ||\n|| goodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8 ||\n|| gordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5 ||\n|| sasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8∗ ||\n|| Word frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8 ||\n|| BERT-large-FT | LM, NSP | COPA | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗ ||\n|| RoBERTa-large-FT | LM | COPA | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗ ||\n\nQuestion: Is it true that (2017).8 Overall both BERT (76.5%) and  RoBERTa (87.7%) considerably outperform the best previous model (71.4%)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "dfff7b52-8d27-4ac2-8e6a-fb62361453d3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.\nTable:\n||  Model |  T | #P | B | C ||\n|| Seq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5 ||\n|| DCGCN (ours) | S |  19.1M | 27.9 | 57.3 ||\n|| DCGCN (ours) | E | 92.5M |  30.4 |  59.6 ||\n\nQuestion: Is it true that In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8a6a5782-c6a0-428a-aadb-4813c0a9d2ae", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nTable:\n|| Model | Training data | Overall | Easy | Hard ||\n|| BERT-large-FT | B-COPA | 74.5 (± 0.7) | 74.7 (± 0.4) |  74.4 (± 0.9) ||\n|| BERT-large-FT | B-COPA (50%) | 74.3 (± 2.2) | 76.8 (± 1.9) | 72.8 (± 3.1) ||\n|| BERT-large-FT | COPA |  76.5 (± 2.7) |  83.9 (± 4.4) | 71.9 (± 2.5) ||\n|| RoBERTa-large-FT | B-COPA |  89.0 (± 0.3) | 88.9 (± 2.1) |  89.0 (± 0.8) ||\n|| RoBERTa-large-FT | B-COPA (50%) | 86.1 (± 2.2) | 87.4 (± 1.1) | 85.4 (± 2.9) ||\n|| RoBERTa-large-FT | COPA | 87.7 (± 0.9) |  91.6 (± 1.1) | 85.3 (± 2.0) ||\n\nQuestion: Is it true that The larger performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely more on superficial cues?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "37eee057-4b1f-43f6-9889-4aa72045c882", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Syntactic Dependency Representations in Neural Relation Classification Table 1: Effect of using the shortest dependency path on each relation type.\nTable:\n||  Relation |  best F1 (in 5-fold) without sdp |  best F1 (in 5-fold) with sdp |  Diff. ||\n|| USAGE | 60.34 | 80.24 | + 19.90 ||\n|| MODEL-FEATURE | 48.89 | 70.00 | + 21.11 ||\n|| PART_WHOLE | 29.51 | 70.27 | +40.76 ||\n|| TOPIC | 45.80 | 91.26 | +45.46 ||\n|| RESULT | 54.35 | 81.58 | +27.23 ||\n|| COMPARE | 20.00 | 61.82 | + 41.82 ||\n|| macro-averaged | 50.10 | 76.10 | +26.00 ||\n\nQuestion: Is it true that However, the sdp information does not have a clear positive impact on all the relation types (Table 1)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "cbc48b80-a560-4ead-aeb2-98b1dcb907eb", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\nTable:\n||  Model |  External | B ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | - | 22.0 ||\n|| GraphLSTM (Song et al.,  2018 ) | - | 23.3 ||\n|| GCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4 ||\n|| DCGCN(single) | - | 25.9 ||\n|| DCGCN(ensemble) | - |  28.2 ||\n|| TSP (Song et al.,  2016 ) | ALL | 22.4 ||\n|| PBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9 ||\n|| Tree2Str (Flanigan et al.,  2016 ) | ALL | 23.0 ||\n|| SNRG (Song et al.,  2017 ) | ALL | 25.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4 ||\n|| GraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2 ||\n|| DCGCN(single) | 0.1M | 29.0 ||\n|| DCGCN(single) | 0.2M |  31.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3 ||\n|| GraphLSTM (Song et al.,  2018 ) | 2M | 33.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8 ||\n|| DCGCN(single) | 0.3M | 33.2 ||\n|| DCGCN(ensemble) | 0.3M |  35.3 ||\n\nQuestion: Is it true that DCGCN model is able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a score of 33.6 by using 2M data and Seq2SeqK achieves a score of 33.8 by using 20M data?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "bd1eba72-ce56-4f45-a304-cb354ff75544", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set\nTable:\n|| Method | WER (%) | DCE ||\n|| No enhancement | 38.4 | 0.958 ||\n|| Wiener filter | 41.0 | 0.775 ||\n|| Minimizing DCE | 31.1 |  0.392 ||\n|| FSEGAN | 29.1 | 0.421 ||\n|| AAS (  wAC=1,  wAD=0) | 27.7 | 0.476 ||\n|| AAS (  wAC=1,  wAD=105) |  26.1 | 0.462 ||\n|| Clean speech | 9.3 | 0.0 ||\n\nQuestion: Is it true that acoustic supervision (27.7%) and multi-task learning (26.1%) show higher WER than minimizing DCE (31.1%) and FSEGAN (29.1%)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "2df13a17-6c33-4cd4-8cf3-7c85fc97cb01", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluation of Greek Word Embeddings Table 4: Word similarity.\nTable:\n|| Model | Pearson | p-value | Pairs (unknown) ||\n|| gr_def |  0.6042 | 3.1E-35 | 2.3% ||\n|| gr_neg10 | 0.5973 | 2.9E-34 | 2.3% ||\n|| cc.el.300 | 0.5311 | 1.7E-25 | 4.9% ||\n|| wiki.el | 0.5812 | 2.2E-31 | 4.5% ||\n|| gr_cbow_def | 0.5232 | 2.7E-25 | 2.3% ||\n|| gr_d300_nosub | 0.5889 | 3.8E-33 | 2.3% ||\n|| gr_w2v_sg_n5 | 0.5879 | 4.4E-33 | 2.3% ||\n\nQuestion: Is it true that According to Pearson correlation, gr def model had the highest correlation with human ratings of similarity?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f273252e-5941-436d-aaf0-23e946eaca18", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Entity, Relation, and Event Extraction with Contextualized Span Representations Table 7: In-domain pre-training: SciBERT vs. BERT\nTable:\n||  | SciERC Entity | SciERC Relation | GENIA Entity ||\n|| Best BERT | 69.8 | 41.9 | 78.4 ||\n|| Best SciBERT |  72.0 |  45.3 |  79.5 ||\n\nQuestion: Is it true that SciBERT significantly boosts performance for scientific datasets including SciERC and GENIA?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "02999797-f0ae-4c27-b7bd-8c4a44e60537", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.\nTable:\n|| Metric | Method of validation | Yelp | Lit. ||\n|| Acc | % of machine and human judgments that match | 94 | 84 ||\n|| Sim | Spearman’s   ρ b/w Sim and human ratings of semantic preservation | 0.79 | 0.75 ||\n|| PP | Spearman’s   ρ b/w negative PP and human ratings of fluency | 0.81 | 0.67 ||\n\nQuestion: Is it true that  We validate Sim and PP by computing sentence-level Spearman's ρ between the metric and human judgments  From Table 5, all validations show strong correlations on the Yelp dataset and reasonable correlations on Literature?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "07c4b243-00fb-4439-94ae-89bb5c1641f5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\nTable:\n||  Complaints   Feature |  Complaints   r |  Not Complaints   Feature |  Not Complaints   r ||\n||  Unigrams |  Unigrams |  Unigrams |  Unigrams ||\n|| not | .154 | [URL] | .150 ||\n|| my | .131 | ! | .082 ||\n|| working | .124 | he | .069 ||\n|| still | .123 | thank | .067 ||\n|| on | .119 | , | .064 ||\n|| can’t | .113 | love | .064 ||\n|| service | .112 | lol | .061 ||\n|| customer | .109 | you | .060 ||\n|| why | .108 | great | .058 ||\n|| website | .107 | win | .058 ||\n|| no | .104 | ’ | .058 ||\n|| ? | .098 | she | .054 ||\n|| fix | .093 | : | .053 ||\n|| won’t | .092 | that | .053 ||\n|| been | .090 | more | .052 ||\n|| issue | .089 | it | .052 ||\n|| days | .088 | would | .051 ||\n|| error | .087 | him | .047 ||\n|| is | .084 | life | .046 ||\n|| charged | .083 | good | .046 ||\n||  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) ||\n|| VBN | .141 | UH | .104 ||\n|| $ | .118 | NNP | .098 ||\n|| VBZ | .114 | PRP | .076 ||\n|| NN_VBZ | .114 | HT | .076 ||\n|| PRP$ | .107 | PRP_. | .076 ||\n|| PRP$_NN | .105 | PRP_RB | .067 ||\n|| VBG | .093 | NNP_NNP | .062 ||\n|| CD | .092 | VBP_PRP | .054 ||\n|| WRB_VBZ | .084 | JJ | .053 ||\n|| VBZ_VBN | .084 | DT_JJ | .051 ||\n\nQuestion: Is it true that  Negations are uncovered through unigrams (not, no, won't)  Several unigrams (error, issue, working, fix)  However, words regularly describing negative sentiment or emotions are not one of the most distinctive features for complaints?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "144aa87d-c757-4945-bf5f-39149c5ba574", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| -{4} dense block | 24.8 | 54.9 ||\n|| -{3, 4} dense blocks | 23.8 | 54.1 ||\n|| -{2, 3, 4} dense blocks | 23.2 | 53.1 ||\n\nQuestion: Is it true that These results indicate dense connections do play a significant role in our model?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "88a1e00a-fb2f-47ef-b350-3f22f3214735", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nTable:\n|| Model | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time ||\n|| Rocktäschel et al. ( 2016 ) | Rocktäschel et al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | - ||\n|| This | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 |  90.49 | 0.696 ||\n|| This | GRU | 6.41M |  85.71 | 0.245 |  86.05 | 0.419 |  90.29 | 0.529 | 90.10 | 0.695 ||\n|| This | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580 ||\n|| Work | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555 ||\n||  | LRN | 4.25M | 84.88 |  0.209 | 85.06 |  0.223 | 89.98 |  0.488 | 89.93 |  0.506 ||\n\nQuestion: Is it true that Results with BERT show that contextual information is not always valuable for performance improvement?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "bfe65751-16ee-43e3-9cc4-8301d4625a8e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\nTable:\n||  Method |  R-1 |  R-2 |  R-SU ||\n|| First-1 | 26.83 | 7.25 | 6.46 ||\n|| First-2 | 35.99 | 10.17 | 12.06 ||\n|| First-3 | 39.41 | 11.77 | 14.51 ||\n|| LexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20 ||\n|| TextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50 ||\n|| MMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91 ||\n|| PG-Original Lebanoff et al. ( 2018 ) | 41.85 | 12.91 | 16.46 ||\n|| PG-MMR Lebanoff et al. ( 2018 ) | 40.55 | 12.36 | 15.87 ||\n|| PG-BRNN Gehrmann et al. ( 2018 ) | 42.80 | 14.19 | 16.75 ||\n|| CopyTransformer Gehrmann et al. ( 2018 ) |  43.57 | 14.03 | 17.37 ||\n|| Hi-MAP (Our Model) | 43.47 |  14.89 |  17.41 ||\n\nQuestion: Is it true that The Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7020ecb6-4c9f-47c3-9983-05d987388d83", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluation of Greek Word Embeddings Table 3: Summary for 3CosAdd and top-1 nearest vectors.\nTable:\n|| Category Semantic | Category no oov words | gr_def 58.42% | gr_neg10 59.33% | cc.el.300   68.80% | wiki.el 27.20% | gr_cbow_def 31.76% | gr_d300_nosub 60.79% | gr_w2v_sg_n5 52.70% ||\n||  | with oov words | 52.97% | 55.33% |  64.34% | 25.73% | 28.80% | 55.11% | 47.82% ||\n|| Syntactic | no oov words | 65.73% | 61.02% |  69.35% | 40.90% | 64.02% | 53.69% | 52.60% ||\n||  | with oov words |  53.95% | 48.69% | 49.43% | 28.42% | 52.54% | 44.06% | 43.13% ||\n|| Overall | no oov words | 63.02% | 59.96% |  68.97% | 36.45% | 52.04% | 56.30% | 52.66% ||\n||  | with oov words | 53.60% | 51.00% |  54.60% | 27.50% | 44.30% | 47.90% | 44.80% ||\n\nQuestion: Is it true that Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model gr def had the best performance in both cases, even when we included the out-of-vocabulary (oov) terms?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "4c2dc82f-365f-43a9-b795-daccfa505e9c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nTable:\n|| Model | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time ||\n|| Rocktäschel et al. ( 2016 ) | Rocktäschel et al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | - ||\n|| This | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 |  90.49 | 0.696 ||\n|| This | GRU | 6.41M |  85.71 | 0.245 |  86.05 | 0.419 |  90.29 | 0.529 | 90.10 | 0.695 ||\n|| This | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580 ||\n|| Work | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555 ||\n||  | LRN | 4.25M | 84.88 |  0.209 | 85.06 |  0.223 | 89.98 |  0.488 | 89.93 |  0.506 ||\n\nQuestion: Is it true that LRN is not the fastest model, with ATR outperforming it by 8%∼27%?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e41db642-41b4-4b36-846a-3e019d6ab36a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nTable:\n|| Batch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear ||\n|| 1 | 46.7 | 27.3 | 7.6 ||\n|| 10 | 125.2 | 78.2 | 22.7 ||\n|| 25 | 129.7 | 83.1 | 45.4 ||\n\nQuestion: Is it true that  On the contrary, for the linear dataset, the recursive implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b2b4fcf5-0b84-4720-ae2c-90e27e36dcef", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. † denotes the previous best state-of-the-art model.\nTable:\n|| Model | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1 ||\n|| CNN zeng2014relation | 0.413 | 0.591 | 0.486 | 0.444 | 0.625 | 0.519 ||\n|| PCNN zeng2015distant | 0.380 |  0.642 | 0.477 | 0.446 | 0.679 | 0.538† ||\n|| EA huang2016attention | 0.443 | 0.638 | 0.523† | 0.419 | 0.677 | 0.517 ||\n|| BGWA jat2018attention | 0.364 | 0.632 | 0.462 | 0.417 |  0.692 | 0.521 ||\n|| BiLSTM-CNN | 0.490 | 0.507 | 0.498 | 0.473 | 0.606 | 0.531 ||\n|| Our model |  0.541 | 0.595 |  0.566* |  0.507 | 0.652 |  0.571* ||\n\nQuestion: Is it true that Our model outperforms the previous stateof-the-art models on both datasets in terms of F1 score?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "932c9362-088b-48c2-b6bc-63df4037f765", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\nTable:\n||  Complaints   Feature |  Complaints   r |  Not Complaints   Feature |  Not Complaints   r ||\n||  Unigrams |  Unigrams |  Unigrams |  Unigrams ||\n|| not | .154 | [URL] | .150 ||\n|| my | .131 | ! | .082 ||\n|| working | .124 | he | .069 ||\n|| still | .123 | thank | .067 ||\n|| on | .119 | , | .064 ||\n|| can’t | .113 | love | .064 ||\n|| service | .112 | lol | .061 ||\n|| customer | .109 | you | .060 ||\n|| why | .108 | great | .058 ||\n|| website | .107 | win | .058 ||\n|| no | .104 | ’ | .058 ||\n|| ? | .098 | she | .054 ||\n|| fix | .093 | : | .053 ||\n|| won’t | .092 | that | .053 ||\n|| been | .090 | more | .052 ||\n|| issue | .089 | it | .052 ||\n|| days | .088 | would | .051 ||\n|| error | .087 | him | .047 ||\n|| is | .084 | life | .046 ||\n|| charged | .083 | good | .046 ||\n||  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) ||\n|| VBN | .141 | UH | .104 ||\n|| $ | .118 | NNP | .098 ||\n|| VBZ | .114 | PRP | .076 ||\n|| NN_VBZ | .114 | HT | .076 ||\n|| PRP$ | .107 | PRP_. | .076 ||\n|| PRP$_NN | .105 | PRP_RB | .067 ||\n|| VBG | .093 | NNP_NNP | .062 ||\n|| CD | .092 | VBP_PRP | .054 ||\n|| WRB_VBZ | .084 | JJ | .053 ||\n|| VBZ_VBN | .084 | DT_JJ | .051 ||\n\nQuestion: Is it true that  On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the top most distinctive features for a tweet not being labeled as a complaint?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8246391e-06c9-4bb7-a7c8-749c2940f735", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM Table 5: Confusion matrix for testing set predictions\nTable:\n||  LabelPrediction |  C |  D |  Q |  S ||\n||  Commenting | 760 | 0 | 12 | 6 ||\n||  Denying | 68 | 0 | 1 | 2 ||\n||  Querying | 69 | 0 | 36 | 1 ||\n||  Supporting | 67 | 0 | 1 | 26 ||\n\nQuestion: Is it true that Most denying instances get misclassified as commenting (see Table 5),?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ee5d43ff-0f49-4fe4-8d83-d0e9571e46d3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nTable:\n|| Dataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb ||\n||  Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 |  76.41 ||\n||  WinoCoref | AntePre | 68.37 | 74.32 | —– | 88.48 | 88.95 |  89.32 ||\n\nQuestion: Is it true that The best performing system is KnowComb?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "e8fa28c5-0b0f-4d5e-a963-f0867a4b2f2a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 4: Precisions on the Wikidata dataset with different choice of d.\nTable:\n|| Recall | 0.1 | 0.2 | 0.3 | AUC | Time ||\n||  d=1 | 0.602 | 0.487 | 0.403 | 0.367 | 4h ||\n||  d=32 | 0.645 | 0.501 | 0.393 | 0.370 | - ||\n||  d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h ||\n||  d=8 | 0.650 | 0.519 | 0.422 | 0.405 | 8h ||\n\nQuestion: Is it true that As the table 4 depicts, the precision increases with the growth of d, but the training time also increases?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "736bdd5b-3280-45f8-ba57-7a23885bf208", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "83e691c8-43ce-46c6-bbc7-2b3d400d8c58", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nTable:\n|| Method | En→It best | En→It avg | En→It iters | En→De best | En→De avg | En→De iters | En→Fi best | En→Fi avg | En→Fi iters | En→Es best | En→Es avg | En→Es iters ||\n|| Artetxe et al., 2018b |  48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808 ||\n|| Noise-aware Alignment |  48.53 |  48.20 | 471 |  49.67 |  48.89 | 568 |  33.98 |  33.68 | 502 |  38.40 |  37.79 | 551 ||\n\nQuestion: Is it true that Our model does not improve the results in the translation tasks?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "d9f56bd3-ead2-44b1-8f61-415c79ef85fa", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 3: Number of tweets annotated as complaints across the nine domains.\nTable:\n||  Category |  Complaints |  Not Complaints ||\n|| Food & Beverage | 95 | 35 ||\n|| Apparel | 141 | 117 ||\n|| Retail | 124 | 75 ||\n|| Cars | 67 | 25 ||\n|| Services | 207 | 130 ||\n|| Software & Online Services | 189 | 103 ||\n|| Transport | 139 | 109 ||\n|| Electronics | 174 | 112 ||\n|| Other | 96 | 33 ||\n|| Total | 1232 | 739 ||\n\nQuestion: Is it true that In total, 1,232 tweets (62.4%) are complaints and 739 are not complaints (37.6%)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "5d51c21b-4d94-4b64-bb07-a899563e5b9a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set\nTable:\n|| Method | WER (%) | DCE ||\n|| No enhancement | 17.3 | 0.828 ||\n|| Wiener filter | 19.5 | 0.722 ||\n|| Minimizing DCE | 15.8 |  0.269 ||\n|| FSEGAN | 14.9 | 0.291 ||\n|| AAS (  wAC=1,  wAD=0) | 15.6 | 0.330 ||\n|| AAS (  wAC=1,  wAD=105) |  14.4 | 0.303 ||\n|| Clean speech | 5.7 | 0.0 ||\n\nQuestion: Is it true that The Wiener filtering method shows lower DCE, but higher WER than no enhancement?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7f0dcd5c-173e-4858-a85d-6f861623a0d4", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Assessing Gender Bias in Machine Translation – A Case Study with Google Translate Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table\nTable:\n|| Category | Female (%) | Male (%) | Neutral (%) ||\n|| Office and administrative support | 11.015 | 58.812 | 16.954 ||\n|| Architecture and engineering | 2.299 | 72.701 | 10.92 ||\n|| Farming, fishing, and forestry | 12.179 | 62.179 | 14.744 ||\n|| Management | 11.232 | 66.667 | 12.681 ||\n|| Community and social service | 20.238 | 62.5 | 10.119 ||\n|| Healthcare support | 25.0 | 43.75 | 17.188 ||\n|| Sales and related | 8.929 | 62.202 | 16.964 ||\n|| Installation, maintenance, and repair | 5.22 | 58.333 | 17.125 ||\n|| Transportation and material moving | 8.81 | 62.976 | 17.5 ||\n|| Legal | 11.905 | 72.619 | 10.714 ||\n|| Business and financial operations | 7.065 | 67.935 | 15.58 ||\n|| Life, physical, and social science | 5.882 | 73.284 | 10.049 ||\n|| Arts, design, entertainment, sports, and media | 10.36 | 67.342 | 11.486 ||\n|| Education, training, and library | 23.485 | 53.03 | 9.091 ||\n|| Building and grounds cleaning and maintenance | 12.5 | 68.333 | 11.667 ||\n|| Personal care and service | 18.939 | 49.747 | 18.434 ||\n|| Healthcare practitioners and technical | 22.674 | 51.744 | 15.116 ||\n|| Production | 14.331 | 51.199 | 18.245 ||\n|| Computer and mathematical | 4.167 | 66.146 | 14.062 ||\n|| Construction and extraction | 8.578 | 61.887 | 17.525 ||\n|| Protective service | 8.631 | 65.179 | 12.5 ||\n|| Food preparation and serving related | 21.078 | 58.333 | 17.647 ||\n|| Total | 11.76 | 58.93 | 15.939 ||\n\nQuestion: Is it true that Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics ?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "44c10fb7-e496-4663-8f4c-d71ccd7dd67d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\nTable:\n||  | Acc | Sim | PP | GM ||\n|| M0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0 ||\n|| M1: M0  +para | 0.819 | 0.734 | 26.3 | 14.2 ||\n|| M2: M0  +cyc | 0.813 | 0.770 | 36.4 | 18.8 ||\n|| M3: M0  +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5 ||\n|| M4: M0  +cyc+para | 0.798 | 0.783 | 39.7 | 19.2 ||\n|| M5: M0  +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3 ||\n|| M6: M0  +cyc+2d | 0.805 |  0.817 | 43.3 | 21.6 ||\n|| M7: M6+  para+lang | 0.818 | 0.805 |  29.0 |  22.8 ||\n\nQuestion: Is it true that Table 2 shows that the model with cyclic loss (M2) and the model with cyclic loss, paraphrase loss, and language model loss (M5) both have lower Sim than M0 on both datasets under similar Acc?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "7ee1b8c8-3a20-4d62-9344-ec5f5a542ba9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Variational Self-attention Model for Sentence Representation Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.\nTable:\n|| Model | Accuracy (%) agree | Accuracy (%) disagree | Accuracy (%) discuss | Accuracy (%) unrelated | Micro F1(%) ||\n|| Average of Word2vec Embedding | 12.43 | 01.30 | 43.32 | 74.24 | 45.53 ||\n|| CNN-based Sentence Embedding | 24.54 | 05.06 | 53.24 | 79.53 | 81.72 ||\n|| RNN-based Sentence Embedding | 24.42 | 05.42 | 69.05 | 65.34 | 78.70 ||\n|| Self-attention Sentence Embedding | 23.53 | 04.63 | 63.59 | 80.34 | 80.11 ||\n|| Our model | 28.53 | 10.43 | 65.43 | 82.43 |  83.54 ||\n\nQuestion: Is it true that As for the micro F1 evaluation metric, our model does not achieve the highest performance (83.54%) on the FNC-1 testing subset?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e55f868d-c3b3-42e4-91ae-3e87cab511f3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\nTable:\n|| # of Heads | Accuracy | Val. Loss | Effect ||\n|| 1 | 89.44% | 0.2811 | -6.84% ||\n|| 2 | 91.20% | 0.2692 | -5.08% ||\n|| 4 | 93.85% | 0.2481 | -2.43% ||\n|| 8 | 96.02% | 0.2257 | -0.26% ||\n|| 10 | 96.28% | 0.2197 |  ||\n|| 16 | 96.32% | 0.2190 | +0.04 ||\n\nQuestion: Is it true that This shows that more attention heads, thereby attending to multiple different contexts at once, is important to boosting performance to state-of-the-art results?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8ca9d3e5-cce5-41f9-ada1-afa9e75a6cf1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.\nTable:\n|| System | MUC | BCUB | CEAFe | AVG ||\n|| ACE | ACE | ACE | ACE | ACE ||\n|| IlliCons |  78.17 | 81.64 |  78.45 |  79.42 ||\n|| KnowComb | 77.51 |  81.97 | 77.44 | 78.97 ||\n|| OntoNotes | OntoNotes | OntoNotes | OntoNotes | OntoNotes ||\n|| IlliCons | 84.10 |  78.30 |  68.74 |  77.05 ||\n|| KnowComb |  84.33 | 78.02 | 67.95 | 76.76 ||\n\nQuestion: Is it true that Despite our system achieving the same level of performance compared to a state-of-art general coreference system, we still observe significant performance improvement on the ACE and OntoNotes datasets?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "5ea07570-7f39-4790-b562-22fd697fb6ef", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| Encoder Modules |  |  ||\n|| -Linear Combination | 23.7 | 53.2 ||\n|| -Global Node | 24.2 | 54.6 ||\n|| -Direction Aggregation | 24.6 | 54.6 ||\n|| -Graph Attention | 24.9 | 54.7 ||\n|| -Global Node&Linear Combination | 22.9 | 52.4 ||\n|| Decoder Modules |  |  ||\n|| -Coverage Mechanism | 23.8 | 53.0 ||\n\nQuestion: Is it true that Results also show the global node is more effective than the linear combination?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ebaa19a5-b7cf-4f3b-8b04-b4304e779ff8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\nTable:\n||  Training data |  Add |  Miss |  Wrong |  Disfl ||\n|| Original | 0 | 22 | 0 | 14 ||\n|| Cleaned added | 0 | 23 | 0 | 14 ||\n|| Cleaned missing | 0 | 1 | 0 | 2 ||\n|| Cleaned | 0 | 0 | 0 | 5 ||\n\nQuestion: Is it true that All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "36fa8fb5-8f19-4084-8cbb-4c3195e16bc3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.\nTable:\n|| Feature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1 ||\n|| +BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 |  0.93 | 0.94 | 0.92 |  0.93 | 0.91 | 0.91 |  0.91 ||\n|| +BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89 ||\n|| +Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88 ||\n|| +BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n\nQuestion: Is it true that The models using BoC do not outperform models using BoW as well as ASM features?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "0b7152d7-54db-4a1f-b5cb-18c3b4b17aa2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 7: Test accuracy on SNLI task with Base+LN setting and test perplexity on PTB task with Base setting.\nTable:\n|| Model | SNLI | PTB ||\n|| LRN |  85.06 |  61.26 ||\n|| gLRN | 84.72 | 92.49 ||\n|| eLRN | 83.56 | 169.81 ||\n\nQuestion: Is it true that Results in Table 7 show that although the accuracy on SNLI is acceptable, gLRN and eLRN perform significantly worse on the PTB task?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "15ca12b8-56ac-4053-8fe7-3b00a97b9a07", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: UKP TU-DA at GermEval 2017:Deep Learning for Aspect Based Sentiment Detection Table 5: Task B results\nTable:\n||  | Micro F1 ||\n|| Baseline | 0.709 ||\n|| W2V (<italic>d</italic>=50) | 0.736 ||\n|| W2V (<italic>d</italic>=500) | 0.753 ||\n|| S2V | 0.748 ||\n|| S2V + W2V (<italic>d</italic>=50) | 0.744 ||\n|| S2V + K + W2V(<italic>d</italic>=50) | 0.749 ||\n|| SIF (DE) | 0.759 ||\n|| SIF (DE-EN) | <bold>0.765</bold> ||\n\nQuestion: Is it true that For Task B, the baseline model outperformed all models trained on the stacked learner when using only plain averaged word embeddings?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "05f252e0-8409-4a35-8596-dc70f2f2b281", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\nTable:\n||  | Difference Function | Seanad Abolition | Video Games | Pornography ||\n|| OD-parse | Absolute | 0.01 | -0.01 | 0.07 ||\n|| OD-parse | JS div. | 0.01 | -0.01 | -0.01 ||\n|| OD-parse | EMD | 0.07 | 0.01 | -0.01 ||\n|| OD | Absolute |  0.54 |  0.56 |  0.41 ||\n|| OD | JS div. | 0.07 | -0.01 | -0.02 ||\n|| OD | EMD | 0.26 | -0.01 | 0.01 ||\n|| OD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04 ||\n|| OD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02 ||\n|| OD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01 ||\n\nQuestion: Is it true that This is evident from the significant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "6e0cec48-2365-4b12-a062-d9a7a42c0fa8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\nTable:\n||  Model | R | MUC P |  F1 | R | B3 P |  F1 | R | CEAF-  e P |  F1 | CoNLL   F1 ||\n||  Baselines |  |  |  |  |  |  |  |  |  |  ||\n|| Cluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5 ||\n|| CV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73 ||\n|| KCP Kenyon-Dean et al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69 ||\n|| Cluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6 ||\n||  Model Variants |  |  |  |  |  |  |  |  |  |  ||\n|| Disjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5 ||\n|| Joint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 |  79.5 ||\n\nQuestion: Is it true that Our joint model does not outperform all the base lines, with a gap of only 10.5 CoNLL F1 points from the last published results (KCP), and only surpassing our strong lemma baseline by 3 points?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a4aa23f5-33df-4432-aa1a-31d016705823", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.\nTable:\n||  Model |  Acc |  F1 |  AUC ||\n|| Most Frequent Class | 64.2 | 39.1 | 0.500 ||\n|| Logistic Regression |  |  |  ||\n|| Sentiment – MPQA | 64.2 | 39.1 | 0.499 ||\n|| Sentiment – NRC | 63.9 | 42.2 | 0.599 ||\n|| Sentiment – V&B | 68.9 | 60.0 | 0.696 ||\n|| Sentiment – VADER | 66.0 | 54.2 | 0.654 ||\n|| Sentiment – Stanford | 68.0 | 55.6 | 0.696 ||\n|| Complaint Specific (all) | 65.7 | 55.2 | 0.634 ||\n|| Request | 64.2 | 39.1 | 0.583 ||\n|| Intensifiers | 64.5 | 47.3 | 0.639 ||\n|| Downgraders | 65.4 | 49.8 | 0.615 ||\n|| Temporal References | 64.2 | 43.7 | 0.535 ||\n|| Pronoun Types | 64.1 | 39.1 | 0.545 ||\n|| POS Bigrams | 72.2 | 66.8 | 0.756 ||\n|| LIWC | 71.6 | 65.8 | 0.784 ||\n|| Word2Vec Clusters | 67.7 | 58.3 | 0.738 ||\n|| Bag-of-Words | 79.8 | 77.5 | 0.866 ||\n|| All Features |  80.5 |  78.0 |  0.873 ||\n|| Neural Networks |  |  |  ||\n|| MLP | 78.3 | 76.2 | 0.845 ||\n|| LSTM | 80.2 | 77.0 | 0.864 ||\n\nQuestion: Is it true that Syntactic part-ofspeech features alone obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section hold high predictive accuracy for the task?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "83f16698-7b03-47a0-8716-3ab3a76eb741", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nTable:\n|| Classifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore ||\n|| SVM-w/o neg. | 0.57 | 0.72 | 0.64 ||\n|| SVM-Punct. neg. | 0.58 | 0.70 | 0.63 ||\n|| SVM-our-neg. | 0.58 | 0.73 | 0.65 ||\n|| CNN | 0.63 | 0.83 | 0.72 ||\n|| CNN-LSTM | 0.71 | 0.72 | 0.72 ||\n|| CNN-LSTM-Our-neg-Ant |  0.78 |  0.77 |  0.78 ||\n||  | Negative Sentiment | Negative Sentiment | Negative Sentiment ||\n||  | Precision | Recall | Fscore ||\n|| SVM-w/o neg. | 0.78 | 0.86 | 0.82 ||\n|| SVM-Punct. neg. | 0.78 | 0.87 | 0.83 ||\n|| SVM-Our neg. | 0.80 | 0.87 | 0.83 ||\n|| CNN | 0.88 | 0.72 | 0.79 ||\n|| CNN-LSTM. | 0.83 | 0.83 | 0.83 ||\n|| CNN-LSTM-our-neg-Ant |  0.87 |  0.87 |  0.87 ||\n||  | Train |  | Test ||\n|| Positive tweets | 5121 |  | 1320 ||\n|| Negative tweets | 9094 |  | 2244 ||\n\nQuestion: Is it true that The proposed CNN-LSTMOur-neg-Ant improves upon the simple CNNLSTM-w/o neg?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ef59728f-b28b-4481-b3b7-3aac7d96ac9d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\nTable:\n|| <bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold> ||\n|| LDC2015E86 | LDC2015E86 | LDC2015E86 ||\n|| Konstas et al. (2017) | 22.00 | - ||\n|| Song et al. (2018) | 23.28 | 30.10 ||\n|| Cao et al. (2019) | 23.50 | - ||\n|| Damonte et al.(2019) | 24.40 | 23.60 ||\n|| Guo et al. (2019) | <bold>25.70</bold> | - ||\n|| S2S | 22.55 ± 0.17 | 29.90 ± 0.31 ||\n|| G2S-GIN | 22.93 ± 0.20 | 29.72 ± 0.09 ||\n|| G2S-GAT | 23.42 ± 0.16 | 29.87 ± 0.14 ||\n|| G2S-GGNN | 24.32 ± 0.16 | <bold>30.53</bold> ± 0.30 ||\n|| LDC2017T10 | LDC2017T10 | LDC2017T10 ||\n|| Back et al. (2018) | 23.30 | - ||\n|| Song et al. (2018) | 24.86 | 31.56 ||\n|| Damonte et al.(2019) | 24.54 | 24.07 ||\n|| Cao et al. (2019) | 26.80 | - ||\n|| Guo et al. (2019) | 27.60 | - ||\n|| S2S | 22.73 ± 0.18 | 30.15 ± 0.14 ||\n|| G2S-GIN | 26.90 ± 0.19 | 32.62 ± 0.04 ||\n|| G2S-GAT | 26.72 ± 0.20 | 32.52 ± 0.02 ||\n|| G2S-GGNN | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 ||\n\nQuestion: Is it true that In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is lower than Guo et al. (2019), a state-of-the-art model that does not employ external information?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "4c4e4ede-1cec-4827-b374-11f06872dac4", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nTable:\n|| Batch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear ||\n|| 1 | 46.7 | 27.3 | 7.6 ||\n|| 10 | 125.2 | 78.2 | 22.7 ||\n|| 25 | 129.7 | 83.1 | 45.4 ||\n\nQuestion: Is it true that  As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0206cb33-81d5-41f7-8a9b-65f54e67f75e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nTable:\n|| Model | Method | Training Data | Overall | Easy | Hard | p-value (%) ||\n|| goodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8 ||\n|| gordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5 ||\n|| sasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8∗ ||\n|| Word frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8 ||\n|| BERT-large-FT | LM, NSP | COPA | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗ ||\n|| RoBERTa-large-FT | LM | COPA | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗ ||\n\nQuestion: Is it true that However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is actually 4.8 percent (71.9% vs. 69.0%)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a24c8c0a-c398-4600-8503-17bec62989ed", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A context sensitive real-time Spell Checker with language adaptability TABLE II: Synthetic Data Performance results\nTable:\n||  Language |  # Test |  P@1 |  P@3 |  P@5 |  P@10 |  MRR ||\n||  Language |  Samples |  P@1 |  P@3 |  P@5 |  P@10 |  MRR ||\n|| Bengali | 140000 | 91.30 | 97.83 | 98.94 | 99.65 | 94.68 ||\n|| Czech | 94205 | 95.84 | 98.72 | 99.26 | 99.62 | 97.37 ||\n|| Danish | 140000 | 85.84 | 95.19 | 97.28 | 98.83 | 90.85 ||\n|| Dutch | 140000 | 86.83 | 95.01 | 97.04 | 98.68 | 91.32 ||\n|| English | 140000 | 97.08 | 99.39 | 99.67 | 99.86 | 98.27 ||\n|| Finnish | 140000 | 97.77 | 99.58 | 99.79 | 99.90 | 98.69 ||\n|| French | 140000 | 86.52 | 95.66 | 97.52 | 98.83 | 91.38 ||\n|| German | 140000 | 87.58 | 96.16 | 97.86 | 99.05 | 92.10 ||\n|| Greek | 30022 | 84.95 | 94.99 | 96.88 | 98.44 | 90.27 ||\n|| Hebrew | 132596 | 94.00 | 98.26 | 99.05 | 99.62 | 96.24 ||\n|| Hindi | 140000 | 82.19 | 93.71 | 96.28 | 98.30 | 88.40 ||\n|| Indonesian | 140000 | 95.01 | 98.98 | 99.50 | 99.84 | 97.04 ||\n|| Italian | 140000 | 89.93 | 97.31 | 98.54 | 99.38 | 93.76 ||\n|| Marathi | 140000 | 93.01 | 98.16 | 99.06 | 99.66 | 95.69 ||\n|| Polish | 140000 | 95.65 | 99.17 | 99.62 | 99.86 | 97.44 ||\n|| Portuguese | 140000 | 86.73 | 96.29 | 97.94 | 99.10 | 91.74 ||\n|| Romanian | 140000 | 95.52 | 98.79 | 99.32 | 99.68 | 97.22 ||\n|| Russian | 140000 | 94.85 | 98.74 | 99.33 | 99.71 | 96.86 ||\n|| Spanish | 140000 | 85.91 | 95.35 | 97.18 | 98.57 | 90.92 ||\n|| Swedish | 140000 | 88.86 | 96.40 | 98.00 | 99.14 | 92.87 ||\n|| Tamil | 140000 | 98.05 | 99.70 | 99.88 | 99.98 | 98.88 ||\n|| Telugu | 140000 | 97.11 | 99.68 | 99.92 | 99.99 | 98.38 ||\n|| Thai | 12403 | 98.73 | 99.71 | 99.78 | 99.85 | 99.22 ||\n|| Turkish | 140000 | 97.13 | 99.51 | 99.78 | 99.92 | 98.33 ||\n\nQuestion: Is it true that The system performs well on synthetic dataset with a minimum of 80% P@1 and 98% P@10?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "aded4603-ff26-4698-8185-8b0236c4f926", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.\nTable:\n|| <bold>Model</bold> | <bold>External</bold> | <bold>BLEU</bold> ||\n|| Konstas et al. (2017) | 200K | 27.40 ||\n|| Song et al. (2018) | 200K | 28.20 ||\n|| Guo et al. (2019) | 200K | 31.60 ||\n|| G2S-GGNN | 200K | <bold>32.23</bold> ||\n\nQuestion: Is it true that G2S-GGNN does not outperform others with the same amount of Gigaword sentences (200K), as shown in Table 3, with a BLEU score of 32.23?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e0ea0f70-dd7a-44e6-af9c-6da48ffb8b11", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\nTable:\n||  | Acc | Sim | PP | GM ||\n|| M0: shen-1 | 0.694 | 0.728 |  22.3 | 8.81 ||\n|| M1: M0  +para | 0.702 | 0.747 | 23.6 | 11.7 ||\n|| M2: M0  +cyc | 0.692 | 0.781 | 49.9 |  12.8 ||\n|| M3: M0  +cyc+lang | 0.698 | 0.754 | 39.2 | 12.0 ||\n|| M4: M0  +cyc+para | 0.702 | 0.757 | 33.9 |  12.8 ||\n|| M5: M0  +cyc+para+lang | 0.688 | 0.753 | 28.6 | 11.8 ||\n|| M6: M0  +cyc+2d | 0.704 |  0.794 | 63.2 |  12.8 ||\n|| M7: M6+  para+lang | 0.706 | 0.768 | 49.0 |  12.8 ||\n\nQuestion: Is it true that  When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "154f0d35-a7d3-4bc4-94c3-9f8a040f2fe9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Semantic Neural Machine Translation using AMR Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.\nTable:\n|| AMR Anno. | BLEU ||\n|| Automatic | 16.8 ||\n|| Gold |  *17.5* ||\n\nQuestion: Is it true that  The improvement from automatic AMR to gold AMR (+0.7 BLEU) is significant, which shows that the translation quality of our model can be further improved with an increase of AMR parsing accuracy?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "bd51e05d-94ea-4aaf-8572-7fff74309537", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.\nTable:\n||  Model |  T | #P | B | C ||\n|| Seq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5 ||\n|| DCGCN (ours) | S |  19.1M | 27.9 | 57.3 ||\n|| DCGCN (ours) | E | 92.5M |  30.4 |  59.6 ||\n\nQuestion: Is it true that This empirically shows that compared to recurrent graph encoders, DCGCNs can learn better representations for graphs?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "306fb06d-a734-46af-9d03-b1fd6bdb10a2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nTable:\n|| Method | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success ||\n|| GP-MBCM | 2.99 | 19.04 | 44.29 | 28.9 ||\n|| ACER | 10.49 | 77.98 | 62.83 | 50.8 ||\n|| PPO | 9.83 | 83.34 | 69.09 | 59.1 ||\n|| ALDM | 12.47 | 81.20 | 62.60 | 61.2 ||\n|| GDPL-sess |  7.49 | 88.39 | 77.56 | 76.4 ||\n|| GDPL-discr | 7.86 | 93.21 | 80.43 | 80.5 ||\n|| GDPL | 7.64 |  94.97 |  83.90 |  86.5 ||\n||  Human |  7.37 |  66.89 |  95.29 |  75.0 ||\n\nQuestion: Is it true that The performance of each approach that interacts with the agenda-based user simulator is shown in  Table 3?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3010d663-a981-41d4-8f6c-555026bb0257", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc∗: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.\nTable:\n|| Model | BLEU | Acc∗ ||\n|| fu-1 |  |  ||\n|| Multi-decoder | 7.6 | 0.792 ||\n|| Style embed. | 15.4 | 0.095 ||\n|| simple-transfer | simple-transfer | simple-transfer ||\n|| Template | 18.0 | 0.867 ||\n|| Delete/Retrieve | 12.6 | 0.909 ||\n|| yang2018unsupervised | yang2018unsupervised | yang2018unsupervised ||\n|| LM | 13.4 | 0.854 ||\n|| LM + classifier |  22.3 | 0.900 ||\n|| Untransferred |  31.4 | 0.024 ||\n\nQuestion: Is it true that We additionally find that supervised BLEU does not show a trade-off with Acc: for a single model type, higher Acc does not necessarily correspond to lower BLEU?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "8bc5a9aa-47c8-40c0-917e-6659a847af46", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.\nTable:\n|| Train | Test |  System |  BLEU |  NIST |  METEOR |  ROUGE-L |  CIDEr |  Add |  Miss |  Wrong |  SER ||\n|| Original |  Original | TGen− | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94 ||\n|| Original |  Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27 ||\n|| Original |  Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80 ||\n|| Original |  Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen− | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37 ||\n|| Cleaned missing |  Original | TGen− | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61 ||\n|| Cleaned missing |  Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53 ||\n|| Cleaned missing |  Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen− | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45 ||\n\nQuestion: Is it true that In other words,  However, the results in bottom halves  of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "e581bff7-189f-47c9-bc6e-ad38451ed188", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\nTable:\n||  Method |  R-1 |  R-2 |  R-SU ||\n|| First-1 | 26.83 | 7.25 | 6.46 ||\n|| First-2 | 35.99 | 10.17 | 12.06 ||\n|| First-3 | 39.41 | 11.77 | 14.51 ||\n|| LexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20 ||\n|| TextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50 ||\n|| MMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91 ||\n|| PG-Original Lebanoff et al. ( 2018 ) | 41.85 | 12.91 | 16.46 ||\n|| PG-MMR Lebanoff et al. ( 2018 ) | 40.55 | 12.36 | 15.87 ||\n|| PG-BRNN Gehrmann et al. ( 2018 ) | 42.80 | 14.19 | 16.75 ||\n|| CopyTransformer Gehrmann et al. ( 2018 ) |  43.57 | 14.03 | 17.37 ||\n|| Hi-MAP (Our Model) | 43.47 |  14.89 |  17.41 ||\n\nQuestion: Is it true that We observe an improvement in performance between PG-original and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "1a799142-09bf-4cb2-a6c1-1c85df1089a8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1\nTable:\n|| Dataset | Class | ˆ  piblack | ˆ  piwhite |  t |  p | ˆ  piblackˆ  piwhite ||\n||  Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505 ||\n||  | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724 ||\n||  Waseem | Racism | 0.001 | 0.001 | 0.035 |  | 1.001 ||\n||  | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993 ||\n||  | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120 ||\n||  Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573 ||\n||  | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653 ||\n||  Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396 ||\n||  Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812 ||\n||  | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239 ||\n||  | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854 ||\n\nQuestion: Is it true that Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "a5addb02-2aad-498e-99b8-e2e4dd7cf0b0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.\nTable:\n||  GCN +RC (2) | B 16.8 | C 48.1 |  GCN +RC+LA (2) | B 18.3 | C 47.9 ||\n|| +RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1 ||\n|| +RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8 ||\n|| +RC (9) |  21.1 | 50.5 | +RC+LA (9) |  22.0 | 52.6 ||\n|| +RC (10) | 20.7 |  50.7 | +RC+LA (10) | 21.2 |  52.9 ||\n|| DCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7 ||\n|| DCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) |  25.5 |  55.4 ||\n\nQuestion: Is it true that For example, GCN+RC+LA (10) achieves a BLEU score of 52.9, which is better than GCN+RC+LA (9)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "352ed084-1079-4116-b4ec-37b4d6ebe790", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\nTable:\n|| <bold>Model</bold> | REF ⇒ GEN <bold>ENT</bold> | REF ⇒ GEN <bold>CON</bold> | REF ⇒ GEN <bold>NEU</bold> ||\n|| S2S | 38.45 | 11.17 | 50.38 ||\n|| G2S-GIN | 49.78 | 9.80 | 40.42 ||\n|| G2S-GAT | 49.48 | 8.09 | 42.43 ||\n|| G2S-GGNN | 51.32 | 8.82 | 39.86 ||\n||  | GEN ⇒ REF | GEN ⇒ REF | GEN ⇒ REF ||\n|| <bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold> ||\n|| S2S | 73.79 | 12.75 | 13.46 ||\n|| G2S-GIN | 76.27 | 10.65 | 13.08 ||\n|| G2S-GAT | 77.54 | 8.54 | 13.92 ||\n|| G2S-GGNN | 77.64 | 9.64 | 12.72 ||\n\nQuestion: Is it true that All G2S models have lower entailment compared to S2S?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "4a18f8e5-71c1-4cf1-8638-bed82a865841", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nTable:\n|| Model | Method | Training Data | Overall | Easy | Hard | p-value (%) ||\n|| goodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8 ||\n|| gordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5 ||\n|| sasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8∗ ||\n|| Word frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8 ||\n|| BERT-large-FT | LM, NSP | COPA | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗ ||\n|| RoBERTa-large-FT | LM | COPA | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗ ||\n\nQuestion: Is it true that The difference between accuracy on Easy and Hard is more pronounced for RoBERTa, suggesting a reliance on superficial cues?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a5a4fd4a-41ee-417e-a619-d1398be5d04a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Method | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R ||\n|| CBOW/784 | 90.0 |  79.2 |  74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 |  78.1 ||\n|| CMOW/784 | 87.5 | 73.4 | 70.6 |  87.3 | 69.6 |  88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2 ||\n|| Hybrid |  90.2 | 78.7 | 73.7 |  87.3 |  72.7 | 87.6 |  79.4 |  79.6 |  43.3 |  63.4 | 77.8 ||\n|| cmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4% ||\n|| cmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1% ||\n\nQuestion: Is it true that Comparing the 784-dimensional models, again, CBOW and CMOW seem to complement each other?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ea7b772f-808f-4add-97cf-6a2b10dfe9a9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Dim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n|| 400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7 ||\n|| 400 | CMOW/400 |  34.4 | 68.8 | 80.1 |  79.9 |  59.8 | 81.9 |  79.2 |  70.7 |  50.3 | 70.7 ||\n|| 400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 |  87.2 ||\n|| 400 | H-CMOW | 32.3 |  70.8 |  81.3 | 76.0 | 59.6 |  82.3 | 77.4 | 70.0 | 50.2 | 38.2 ||\n|| 784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 |  89.5 ||\n|| 784 | CMOW/784 |  35.1 |  70.8 |  82.0 | 80.2 |  61.8 | 82.8 |  79.7 | 74.2 |  50.7 | 72.9 ||\n|| 800 | Hybrid | 35.0 |  70.8 | 81.7 |  81.0 | 59.4 |  84.4 | 79.0 |  74.3 | 49.3 | 87.6 ||\n|| - | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1% ||\n|| - | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9% ||\n\nQuestion: Is it true that The hybrid model does not yield scores close to or even above the better model of the two on all tasks?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "74e89463-6301-450d-97e1-7cdefad17983", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that  The performances of all models decrease as the diameters of the graphs increase?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "c815f80d-4626-4775-bfff-d8b3630c274d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\nTable:\n||  | Acc | Sim | PP | GM ||\n|| M0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0 ||\n|| M1: M0  +para | 0.819 | 0.734 | 26.3 | 14.2 ||\n|| M2: M0  +cyc | 0.813 | 0.770 | 36.4 | 18.8 ||\n|| M3: M0  +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5 ||\n|| M4: M0  +cyc+para | 0.798 | 0.783 | 39.7 | 19.2 ||\n|| M5: M0  +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3 ||\n|| M6: M0  +cyc+2d | 0.805 |  0.817 | 43.3 | 21.6 ||\n|| M7: M6+  para+lang | 0.818 | 0.805 |  29.0 |  22.8 ||\n\nQuestion: Is it true that  Table 2 shows that the model with paraphrase loss (M1) slightly improves Sim over M0 on both datasets under similar Acc?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "52d13569-4080-411e-a922-9afd23f2b1b1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1038 | 0.0170 | 0.0490 | 0.0641 | 0.0641 | 0.0613 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1282 | 0.0291 | 0.0410 | 0.0270 | 0.0270 | 0.1154 | 0.0661 ||\n|| P | PT | Europarl | 0.6185 | 0.3744 | 0.4144 | 0.4394 | 0.4394 |  0.7553 | 0.5676 ||\n||  | PT | Ted Talks | 0.6308 | 0.4124 | 0.4404 | 0.4515 | 0.4945 |  0.8609 | 0.5295 ||\n|| R | EN | Europarl |  0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0011 | 0.0008 | 0.0011 | 0.0008 | 0.0008 |  0.0030 | 0.0018 ||\n|| R | PT | Europarl | 0.0012 | 0.0008 | 0.0009 | 0.0010 | 0.0010 |  0.0016 | 0.0012 ||\n||  | PT | Ted Talks | 0.0003 | 0.0009 | 0.0009 | 0.0010 | 0.0010 |  0.0017 | 0.0011 ||\n|| F | EN | Europarl |  0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0022 | 0.0016 | 0.0022 | 0.0015 | 0.0015 |  0.0058 | 0.0036 ||\n|| F | PT | Europarl | 0.0024 | 0.0016 | 0.0018 | 0.0019 | 0.0019 |  0.0031 | 0.0023 ||\n||  | PT | Ted Talks | 0.0005 | 0.0018 | 0.0018 | 0.0020 | 0.0021 |  0.0034 | 0.0022 ||\n\nQuestion: Is it true that Despite filtering out multiple hypernyms, the recall values for the Portuguese corpora are still relatively high?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "bc61ec91-f1d8-4416-9fdb-87ade3bbd1e8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nTable:\n|| Model | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time ||\n|| Rocktäschel et al. ( 2016 ) | Rocktäschel et al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | - ||\n|| This | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 |  90.49 | 0.696 ||\n|| This | GRU | 6.41M |  85.71 | 0.245 |  86.05 | 0.419 |  90.29 | 0.529 | 90.10 | 0.695 ||\n|| This | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580 ||\n|| Work | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555 ||\n||  | LRN | 4.25M | 84.88 |  0.209 | 85.06 |  0.223 | 89.98 |  0.488 | 89.93 |  0.506 ||\n\nQuestion: Is it true that LRN obtains additional 4 percentage points gain with BERT and reaches an accuracy of around 89.9?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b3f85fb2-384a-484e-9e52-d0bc2a0efac5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\nTable:\n||  Complaints   Feature |  Complaints   r |  Not Complaints   Feature |  Not Complaints   r ||\n||  Unigrams |  Unigrams |  Unigrams |  Unigrams ||\n|| not | .154 | [URL] | .150 ||\n|| my | .131 | ! | .082 ||\n|| working | .124 | he | .069 ||\n|| still | .123 | thank | .067 ||\n|| on | .119 | , | .064 ||\n|| can’t | .113 | love | .064 ||\n|| service | .112 | lol | .061 ||\n|| customer | .109 | you | .060 ||\n|| why | .108 | great | .058 ||\n|| website | .107 | win | .058 ||\n|| no | .104 | ’ | .058 ||\n|| ? | .098 | she | .054 ||\n|| fix | .093 | : | .053 ||\n|| won’t | .092 | that | .053 ||\n|| been | .090 | more | .052 ||\n|| issue | .089 | it | .052 ||\n|| days | .088 | would | .051 ||\n|| error | .087 | him | .047 ||\n|| is | .084 | life | .046 ||\n|| charged | .083 | good | .046 ||\n||  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) ||\n|| VBN | .141 | UH | .104 ||\n|| $ | .118 | NNP | .098 ||\n|| VBZ | .114 | PRP | .076 ||\n|| NN_VBZ | .114 | HT | .076 ||\n|| PRP$ | .107 | PRP_. | .076 ||\n|| PRP$_NN | .105 | PRP_RB | .067 ||\n|| VBG | .093 | NNP_NNP | .062 ||\n|| CD | .092 | VBP_PRP | .054 ||\n|| WRB_VBZ | .084 | JJ | .053 ||\n|| VBZ_VBN | .084 | DT_JJ | .051 ||\n\nQuestion: Is it true that  However, words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are not significantly associated with tweets that are not complaints?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "1e1d2baf-44da-4531-9601-26027fdd859a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.\nTable:\n||  | M | F | B | O ||\n|| Random | 47.5 | 50.5 |  1.06 | 49.0 ||\n|| Token Distance | 50.6 | 47.5 |  0.94 | 49.1 ||\n|| Topical Entity | 50.2 | 47.3 |  0.94 | 48.8 ||\n|| Syntactic Distance | 66.7 | 66.7 |    1.00 | 66.7 ||\n|| Parallelism |  69.3 |  69.2 |    1.00 |  69.2 ||\n|| Parallelism+URL |  74.2 |  71.6 |    0.96 |  72.9 ||\n|| Transformer-Single | 59.6 | 56.6 |  0.95 | 58.1 ||\n|| Transformer-Multi | 62.9 | 61.7 |  0.98 | 62.3 ||\n\nQuestion: Is it true that RANDOM is indeed closer here to the expected 50% and other baselines are closer to gender-parity?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "18c4bae1-104d-4e43-bcbc-da4b289a9d62", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Assessing Gender Bias in Machine Translation – A Case Study with Google Translate Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table\nTable:\n|| Category | Female (%) | Male (%) | Neutral (%) ||\n|| Office and administrative support | 11.015 | 58.812 | 16.954 ||\n|| Architecture and engineering | 2.299 | 72.701 | 10.92 ||\n|| Farming, fishing, and forestry | 12.179 | 62.179 | 14.744 ||\n|| Management | 11.232 | 66.667 | 12.681 ||\n|| Community and social service | 20.238 | 62.5 | 10.119 ||\n|| Healthcare support | 25.0 | 43.75 | 17.188 ||\n|| Sales and related | 8.929 | 62.202 | 16.964 ||\n|| Installation, maintenance, and repair | 5.22 | 58.333 | 17.125 ||\n|| Transportation and material moving | 8.81 | 62.976 | 17.5 ||\n|| Legal | 11.905 | 72.619 | 10.714 ||\n|| Business and financial operations | 7.065 | 67.935 | 15.58 ||\n|| Life, physical, and social science | 5.882 | 73.284 | 10.049 ||\n|| Arts, design, entertainment, sports, and media | 10.36 | 67.342 | 11.486 ||\n|| Education, training, and library | 23.485 | 53.03 | 9.091 ||\n|| Building and grounds cleaning and maintenance | 12.5 | 68.333 | 11.667 ||\n|| Personal care and service | 18.939 | 49.747 | 18.434 ||\n|| Healthcare practitioners and technical | 22.674 | 51.744 | 15.116 ||\n|| Production | 14.331 | 51.199 | 18.245 ||\n|| Computer and mathematical | 4.167 | 66.146 | 14.062 ||\n|| Construction and extraction | 8.578 | 61.887 | 17.525 ||\n|| Protective service | 8.631 | 65.179 | 12.5 ||\n|| Food preparation and serving related | 21.078 | 58.333 | 17.647 ||\n|| Total | 11.76 | 58.93 | 15.939 ||\n\nQuestion: Is it true that What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "edeea560-96b7-4321-b28d-aa8e670d56ca", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.\nTable:\n|| Source | Target | Svm | Ra-Svm‡ | Ra-Cnn‡ | Trans† | Ra-Trans‡† | Ours‡† | Oracle† ||\n|| Beer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 |  79.53 | 80.29 ||\n|| Beer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 |  77.94 | 78.11 ||\n|| Beer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 |  75.24 | 75.50 ||\n\nQuestion: Is it true that We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) underperforms ORACLE by a large margin?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "aa63576f-129c-4fdd-a6d2-2f7410459284", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nTable:\n|| VS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L ||\n|| ACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18 ||\n|| PPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10 ||\n|| ALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15 ||\n\nQuestion: Is it true that GDPL outperforms three baselines significantly in all aspects (sign test, p-value < 0.01) except for the quality compared with ACER?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "4301bd18-8256-4555-b14e-6fa07d64c262", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nTable:\n|| Batch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear ||\n|| 1 | 46.7 | 27.3 | 7.6 ||\n|| 10 | 125.2 | 78.2 | 22.7 ||\n|| 25 | 129.7 | 83.1 | 45.4 ||\n\nQuestion: Is it true that Another interesting fact in Table 1 is that the training throughput on the linear dataset does not scale better than the throughput on the balanced dataset, as the batch size increases?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e2bf9a66-bb8d-4c47-b276-a33bcb67117c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 4: KL-divergence between different dialog policy and the human dialog KL(πturns||pturns), where πturns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy π and the agenda-based user simulator, and pturns for the real human-human dialog.\nTable:\n|| GP-MBCM | ACER | PPO | ALDM | GDPL ||\n|| 1.666 | 0.775 | 0.639 | 1.069 |  0.238 ||\n\nQuestion: Is it true that Table 4 shows that GDPL has the smallest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves more like the human?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "086ef478-1afa-472e-b71f-ca7b784c40fb", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: UKP TU-DA at GermEval 2017:Deep Learning for Aspect Based Sentiment Detection Table 5: Task B results\nTable:\n||  | Micro F1 ||\n|| Baseline | 0.709 ||\n|| W2V (<italic>d</italic>=50) | 0.736 ||\n|| W2V (<italic>d</italic>=500) | 0.753 ||\n|| S2V | 0.748 ||\n|| S2V + W2V (<italic>d</italic>=50) | 0.744 ||\n|| S2V + K + W2V(<italic>d</italic>=50) | 0.749 ||\n|| SIF (DE) | 0.759 ||\n|| SIF (DE-EN) | <bold>0.765</bold> ||\n\nQuestion: Is it true that For Task B, all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "091c653e-d166-4f19-a914-0f0a73b1b51e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE IX: Accuracies (%) for Sentiment Classification Task\nTable:\n|| GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed ||\n|| 77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26 ||\n\nQuestion: Is it true that Our proposed method outperforms Pretrained Word2Sense embeddings, despite the latter having the advantage of training on a larger corpus?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "3197a3a2-dcb8-4bbb-8be5-a2e8eee69365", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Syntactic Dependency Representations in Neural Relation Classification Table 1: Effect of using the shortest dependency path on each relation type.\nTable:\n||  Relation |  best F1 (in 5-fold) without sdp |  best F1 (in 5-fold) with sdp |  Diff. ||\n|| USAGE | 60.34 | 80.24 | + 19.90 ||\n|| MODEL-FEATURE | 48.89 | 70.00 | + 21.11 ||\n|| PART_WHOLE | 29.51 | 70.27 | +40.76 ||\n|| TOPIC | 45.80 | 91.26 | +45.46 ||\n|| RESULT | 54.35 | 81.58 | +27.23 ||\n|| COMPARE | 20.00 | 61.82 | + 41.82 ||\n|| macro-averaged | 50.10 | 76.10 | +26.00 ||\n\nQuestion: Is it true that However, the sdp information has a clear positive impact on all the relation types (Table 1)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "a8be9400-0253-4cda-ad4a-06b707c381b5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Neural End-to-End Learning for Computational Argumentation Mining Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by “:”. Layers from which tasks feed are indicated by respective numbers.\nTable:\n||  | C-F1 100% | C-F1 50% | R-F1 100% | R-F1 50% | F1 100% | F1 50% ||\n|| Y-3 | 49.59 | 65.37 | 26.28 | 37.00 | 34.35 | 47.25 ||\n|| Y-3:Y<italic>C</italic>-1 | 54.71 | 66.84 | 28.44 | 37.35 | 37.40 | 47.92 ||\n|| Y-3:Y<italic>R</italic>-1 | 51.32 | 66.49 | 26.92 | 37.18 | 35.31 | 47.69 ||\n|| Y-3:Y<italic>C</italic>-3 | <bold>54.58</bold> | 67.66 | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold> ||\n|| Y-3:Y<italic>R</italic>-3 | 53.31 | 66.71 | 26.65 | 35.86 | 35.53 | 46.64 ||\n|| Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95 | <bold>67.84</bold> | 27.90 | 39.71 | 36.54 | 50.09 ||\n|| Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55 | 67.60 | 28.30 | 38.26 | 37.26 | 48.86 ||\n\nQuestion: Is it true that Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally stronger:  as in Eq?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "9e0ce9f6-6eb5-4eda-97f7-ec6948db9bf5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.\nTable:\n|| <bold>Model</bold> | <bold>ADDED</bold> | <bold>MISS</bold> ||\n|| S2S | 47.34 | 37.14 ||\n|| G2S-GIN | 48.67 | 33.64 ||\n|| G2S-GAT | 48.24 | 33.73 ||\n|| G2S-GGNN | 48.66 | 34.06 ||\n|| GOLD | 50.77 | 28.35 ||\n||  |  |  ||\n\nQuestion: Is it true that As shown in Table 8, the S2S baseline outperforms the G2S approaches?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "bfb26805-c8f6-4854-9e1c-14b7534b396e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\nTable:\n||  System |  ROUGE-1   R (%) |  ROUGE-1   P (%) |  ROUGE-1   F (%) |  ROUGE-2   R (%) |  ROUGE-2   P (%) |  ROUGE-2   F (%) |  Sentence-Level   R (%) |  Sentence-Level   P (%) |  Sentence-Level   F (%) ||\n||  ILP | 24.5 | 41.1 | 29.3±0.5 | 7.9 | 15.0 | 9.9±0.5 | 13.6 | 22.6 | 15.6±0.4 ||\n||  Sum-Basic | 28.4 | 44.4 | 33.1±0.5 | 8.5 | 15.6 | 10.4±0.4 | 14.7 | 22.9 | 16.7±0.5 ||\n||  KL-Sum | 39.5 | 34.6 | 35.5±0.5 | 13.0 | 12.7 | 12.3±0.5 | 15.2 | 21.1 | 16.3±0.5 ||\n||  LexRank | 42.1 | 39.5 | 38.7±0.5 | 14.7 | 15.3 | 14.2±0.5 | 14.3 | 21.5 | 16.0±0.5 ||\n||  MEAD | 45.5 | 36.5 | 38.5± 0.5 | 17.9 | 14.9 | 15.4±0.5 | 27.8 | 29.2 | 26.8±0.5 ||\n||  SVM | 19.0 | 48.8 | 24.7±0.8 | 7.5 | 21.1 | 10.0±0.5 | 32.7 | 34.3 | 31.4±0.4 ||\n||  LogReg | 26.9 | 34.5 | 28.7±0.6 | 6.4 | 9.9 | 7.3±0.4 | 12.2 | 14.9 | 12.7±0.5 ||\n||  LogReg  r | 28.0 | 34.8 | 29.4±0.6 | 6.9 | 10.4 | 7.8±0.4 | 12.1 | 14.5 | 12.5±0.5 ||\n||  HAN | 31.0 | 42.8 | 33.7±0.7 | 11.2 | 17.8 | 12.7±0.5 | 26.9 | 34.1 | 32.4±0.5 ||\n||  HAN+pretrainT | 32.2 | 42.4 | 34.4±0.7 | 11.5 | 17.5 | 12.9±0.5 | 29.6 | 35.8 | 32.2±0.5 ||\n||  HAN+pretrainU | 32.1 | 42.1 | 33.8±0.7 | 11.6 | 17.6 | 12.9±0.5 | 30.1 | 35.6 | 32.3±0.5 ||\n||  HAN  r | 38.1 | 40.5 |  37.8±0.5 | 14.0 | 17.1 |  14.7±0.5 | 32.5 | 34.4 |  33.4±0.5 ||\n||  HAN+pretrainT  r | 37.9 | 40.4 |  37.6±0.5 | 13.5 | 16.8 |  14.4±0.5 | 32.5 | 34.4 |  33.4±0.5 ||\n||  HAN+pretrainU  r | 37.9 | 40.4 |  37.6±0.5 | 13.6 | 16.9 |  14.4±0.5 | 33.9 | 33.8 |  33.8±0.5 ||\n\nQuestion: Is it true that We suspect that there are not enough data to pretrain the models and that the thread classification task used to pretrain the HAN models may not be sophisticated enough to learn effective thread vectors?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "4c5f3d6e-0a82-4654-8825-545bffd70dd2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 2: F1 score results per relation type of the best performing models.\nTable:\n|| Relation type | Count | Intra-sentential co-occ.   ρ=0 | Intra-sentential co-occ.   ρ=5 | Intra-sentential co-occ.   ρ=10 | BoC(Wiki-PubMed-PMC) LR | BoC(Wiki-PubMed-PMC) SVM | BoC(Wiki-PubMed-PMC) ANN ||\n|| TherapyTiming(TP,TD) | 428 |  0.84 | 0.59 | 0.47 | 0.78 | 0.81 | 0.78 ||\n|| NextReview(Followup,TP) | 164 |  0.90 | 0.83 | 0.63 | 0.86 | 0.88 | 0.84 ||\n|| Toxicity(TP,CF/TR) | 163 |  0.91 | 0.77 | 0.55 | 0.85 | 0.86 | 0.86 ||\n|| TestTiming(TN,TD/TP) | 184 | 0.90 | 0.81 | 0.42 | 0.96 |  0.97 | 0.95 ||\n|| TestFinding(TN,TR) | 136 | 0.76 | 0.60 | 0.44 |  0.82 | 0.79 | 0.78 ||\n|| Threat(O,CF/TR) | 32 | 0.85 | 0.69 | 0.54 |  0.95 |  0.95 | 0.92 ||\n|| Intervention(TP,YR) | 5 |  0.88 | 0.65 | 0.47 | - | - | - ||\n|| EffectOf(Com,CF) | 3 |  0.92 | 0.62 | 0.23 | - | - | - ||\n|| Severity(CF,CS) | 75 |  0.61 | 0.53 | 0.47 | 0.52 | 0.55 | 0.51 ||\n|| RecurLink(YR,YR/CF) | 7 |  1.0 |  1.0 | 0.64 | - | - | - ||\n|| RecurInfer(NR/YR,TR) | 51 | 0.97 | 0.69 | 0.43 |  0.99 |  0.99 | 0.98 ||\n|| GetOpinion(Referral,CF/other) | 4 |  0.75 |  0.75 | 0.5 | - | - | - ||\n|| Context(Dis,DisCont) | 40 |  0.70 | 0.63 | 0.53 | 0.60 | 0.41 | 0.57 ||\n|| TestToAssess(TN,CF/TR) | 36 | 0.76 | 0.66 | 0.36 |  0.92 |  0.92 | 0.91 ||\n|| TimeStamp(TD,TP) | 221 |  0.88 | 0.83 | 0.50 | 0.86 | 0.85 | 0.83 ||\n|| TimeLink(TP,TP) | 20 |  0.92 | 0.85 | 0.45 | 0.91 |  0.92 | 0.90 ||\n|| Overall | 1569 | 0.90 | 0.73 | 0.45 | 0.92 |  0.93 | 0.91 ||\n\nQuestion: Is it true that  As the results of applying the co-occurrence baseline (ρ = 0) shows (Table 2), the semantic relations in this data are strongly concentrated within a sentence boundary, especially for the relation of RecurLink, with an F1 of 1.0?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "9faf0fb8-7f04-487b-8c21-c849d0edd997", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. ∗: statistically significantly better than all comparisons (randomization approximation test Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\nTable:\n||  |  w/ System Retrieval   B-2 |  w/ System Retrieval   B-4 |  w/ System Retrieval   R-2 |  w/ System Retrieval   MTR |  w/ System Retrieval   #Word |  w/ System Retrieval   #Sent |  w/ Oracle Retrieval   B-2 |  w/ Oracle Retrieval   B-4 |  w/ Oracle Retrieval   R-2 |  w/ Oracle Retrieval   MTR |  w/ Oracle Retrieval   #Word |  w/ Oracle Retrieval   #Sent ||\n|| Human | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22 ||\n|| Retrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21 ||\n||  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  Comparisons |  |  ||\n|| Seq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 ||\n|| Seq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14 ||\n||  w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12 ||\n|| H&W Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12 ||\n||  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  Our Models |  |  ||\n|| CANDELA | 12.02∗ |  2.99∗ |  14.93∗ |  16.92∗ | 119 | 22 | 15.80∗ |  5.00∗ |  23.75 |  20.18 | 116 | 22 ||\n||  w/o psg |  12.33∗ | 2.86∗ | 14.53∗ | 16.60∗ | 123 | 23 |  16.33∗ | 4.98∗ | 23.65 | 19.94 | 123 | 23 ||\n\nQuestion: Is it true that Interestingly, we observe a decrease of ROUGE and METEOR, but a marginal increase of BLEU-2 by removing passages from our model input?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "54edbc96-7e26-4732-9e3c-08ce9c75397e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\nTable:\n||  | Acc | Sim | PP | GM ||\n|| M0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0 ||\n|| M1: M0  +para | 0.819 | 0.734 | 26.3 | 14.2 ||\n|| M2: M0  +cyc | 0.813 | 0.770 | 36.4 | 18.8 ||\n|| M3: M0  +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5 ||\n|| M4: M0  +cyc+para | 0.798 | 0.783 | 39.7 | 19.2 ||\n|| M5: M0  +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3 ||\n|| M6: M0  +cyc+2d | 0.805 |  0.817 | 43.3 | 21.6 ||\n|| M7: M6+  para+lang | 0.818 | 0.805 |  29.0 |  22.8 ||\n\nQuestion: Is it true that For Yelp, M0 has better Acc and PP than M1 at comparable semantic similarity?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "31b8b6fe-df87-467d-9695-321d94ad69f9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A context sensitive real-time Spell Checker with language adaptability TABLE II: Synthetic Data Performance results\nTable:\n||  Language |  # Test |  P@1 |  P@3 |  P@5 |  P@10 |  MRR ||\n||  Language |  Samples |  P@1 |  P@3 |  P@5 |  P@10 |  MRR ||\n|| Bengali | 140000 | 91.30 | 97.83 | 98.94 | 99.65 | 94.68 ||\n|| Czech | 94205 | 95.84 | 98.72 | 99.26 | 99.62 | 97.37 ||\n|| Danish | 140000 | 85.84 | 95.19 | 97.28 | 98.83 | 90.85 ||\n|| Dutch | 140000 | 86.83 | 95.01 | 97.04 | 98.68 | 91.32 ||\n|| English | 140000 | 97.08 | 99.39 | 99.67 | 99.86 | 98.27 ||\n|| Finnish | 140000 | 97.77 | 99.58 | 99.79 | 99.90 | 98.69 ||\n|| French | 140000 | 86.52 | 95.66 | 97.52 | 98.83 | 91.38 ||\n|| German | 140000 | 87.58 | 96.16 | 97.86 | 99.05 | 92.10 ||\n|| Greek | 30022 | 84.95 | 94.99 | 96.88 | 98.44 | 90.27 ||\n|| Hebrew | 132596 | 94.00 | 98.26 | 99.05 | 99.62 | 96.24 ||\n|| Hindi | 140000 | 82.19 | 93.71 | 96.28 | 98.30 | 88.40 ||\n|| Indonesian | 140000 | 95.01 | 98.98 | 99.50 | 99.84 | 97.04 ||\n|| Italian | 140000 | 89.93 | 97.31 | 98.54 | 99.38 | 93.76 ||\n|| Marathi | 140000 | 93.01 | 98.16 | 99.06 | 99.66 | 95.69 ||\n|| Polish | 140000 | 95.65 | 99.17 | 99.62 | 99.86 | 97.44 ||\n|| Portuguese | 140000 | 86.73 | 96.29 | 97.94 | 99.10 | 91.74 ||\n|| Romanian | 140000 | 95.52 | 98.79 | 99.32 | 99.68 | 97.22 ||\n|| Russian | 140000 | 94.85 | 98.74 | 99.33 | 99.71 | 96.86 ||\n|| Spanish | 140000 | 85.91 | 95.35 | 97.18 | 98.57 | 90.92 ||\n|| Swedish | 140000 | 88.86 | 96.40 | 98.00 | 99.14 | 92.87 ||\n|| Tamil | 140000 | 98.05 | 99.70 | 99.88 | 99.98 | 98.88 ||\n|| Telugu | 140000 | 97.11 | 99.68 | 99.92 | 99.99 | 98.38 ||\n|| Thai | 12403 | 98.73 | 99.71 | 99.78 | 99.85 | 99.22 ||\n|| Turkish | 140000 | 97.13 | 99.51 | 99.78 | 99.92 | 98.33 ||\n\nQuestion: Is it true that The system does not perform well on synthetic dataset with a minimum of 80% P@1 and 98% P@10?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "8667413d-d331-4b2e-bae7-01f3a106b373", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326 ||\n|| P | EN | Ted Talks |  0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162 ||\n|| P | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 |  0.8052 | 0.4058 ||\n||  | PT | Ted Talks |  0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698 ||\n|| R | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 |  0.6490 | 0.0017 | 0.0003 ||\n|| R | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 |  0.6467 | 0.6332 | 0.0967 | 0.0003 ||\n|| R | PT | Europarl | 0.0002 | 0.1562 | 0.5157 |  0.7255 | 0.5932 | 0.0032 | 0.0001 ||\n||  | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 |  0.7000 | 0.5887 | 0.1390 | 0.0002 ||\n|| F | EN | Europarl | 0.0073 | 0.0162 | 0.0268 |  0.0293 |  0.0293 | 0.0033 | 0.0006 ||\n|| F | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 |  0.0520 | 0.0005 ||\n|| F | PT | Europarl | 0.0005 | 0.1733 | 0.4412 |  0.6240 | 0.5109 | 0.0064 | 0.0002 ||\n||  | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 |  0.6040 | 0.5149 | 0.2261 | 0.0004 ||\n\nQuestion: Is it true that As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods, except for the Portuguese Europarl corpus, where DocSub had the highest value?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "447eeb5c-f007-4096-b630-ce70255d1c14", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\nTable:\n|| <bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1 ||\n|| <bold>Baselines</bold> |  |  |  |  |  |  |  |  |  |  ||\n|| Cluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5 ||\n|| CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73 ||\n|| KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69 ||\n|| Cluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6 ||\n|| <bold>Model Variants</bold> |  |  |  |  |  |  |  |  |  |  ||\n|| Disjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5 ||\n|| Joint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold> ||\n\nQuestion: Is it true that  The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b920ceff-a2e2-4cc1-877c-73c2b3404336", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.\nTable:\n||  | Ours | Refresh | ExtAbsRL ||\n|| Avg. Human Rating |  2.52 | 2.27 | 1.66 ||\n|| Best% |  70.0 | 33.3 | 6.7 ||\n\nQuestion: Is it true that Also, the average human rating for Refresh is not significantly higher (p (cid:28) 0.01) than ExtAbsRL?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "80fcba83-634a-4705-a314-22a36d228ec5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\nTable:\n||  | Acc | Sim | PP | GM ||\n|| M0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0 ||\n|| M1: M0  +para | 0.819 | 0.734 | 26.3 | 14.2 ||\n|| M2: M0  +cyc | 0.813 | 0.770 | 36.4 | 18.8 ||\n|| M3: M0  +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5 ||\n|| M4: M0  +cyc+para | 0.798 | 0.783 | 39.7 | 19.2 ||\n|| M5: M0  +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3 ||\n|| M6: M0  +cyc+2d | 0.805 |  0.817 | 43.3 | 21.6 ||\n|| M7: M6+  para+lang | 0.818 | 0.805 |  29.0 |  22.8 ||\n\nQuestion: Is it true that For Yelp, M1 has better Acc and PP than M0 at comparable semantic similarity?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7a281d71-ec5a-4086-bb9e-b1a04976b77b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| -{4} dense block | 24.8 | 54.9 ||\n|| -{3, 4} dense blocks | 23.8 | 54.1 ||\n|| -{2, 3, 4} dense blocks | 23.2 | 53.1 ||\n\nQuestion: Is it true that The full model gives 25.5 BLEU points on the AMR15 dev set?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b87e736b-b577-4883-9cd3-271efb940ee7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTable:\n|| Topic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI |  OD-w2v ARI |  OD-d2v ARI | TF-IDF   Sil. | WMD   Sil. | Sent2vec   Sil. | Doc2vec   Sil. | BERT   Sil. |  OD-w2v   Sil. |  OD-d2v   Sil. ||\n|| Affirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 |  0.14 |  0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 |  0.06 |  0.01 ||\n|| Atheism | 116 |  0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 |  0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 |  0.05 |  0.07 ||\n|| Austerity Measures | 20 |  0.04 |  0.04 | -0.01 | -0.05 | 0.04 |  0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 |  0.19 | 0.1 ||\n|| Democratization | 76 | 0.02 | -0.01 | 0.00 |  0.09 | -0.01 |  0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 |  0.16 |  0.11 ||\n|| Education Voucher Scheme | 30 |  0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 |  0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 |  0.38 |  0.40 ||\n|| Gambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 |  0.35 |  0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 |  0.30 |  0.22 ||\n|| Housing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 |  0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 |  0.13 |  0.13 ||\n|| Hydroelectric Dams | 110 |  0.47 |  0.45 |  0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 |  0.26 |  0.09 ||\n|| Intellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 |  0.05 |  0.14 | 0.01 |  0.04 | 0.03 | 0.01 | 0.03 |  0.04 |  0.12 ||\n|| Keystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 |  0.07 | -0.01 |  0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 |  0.05 |  0.02 ||\n|| Monarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 |  0.15 |  0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 |  0.11 |  0.09 ||\n|| National Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 |  0.31 |  0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 |  0.25 |  0.25 ||\n|| One-child policy China | 67 | -0.05 | 0.01 |  0.11 | -0.02 | 0.02 |  0.11 | 0.01 | 0.01 | 0.02 |  0.04 | -0.01 | 0.03 |  0.07 | -0.02 ||\n|| Open-source Software | 48 | -0.02 | -0.01 |  0.05 | 0.01 | 0.12 |  0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 |  0.18 | 0.01 ||\n|| Pornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 |  0.41 |  0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 |  0.47 |  0.41 ||\n|| Seanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 |  0.32 |  0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 |  0.15 |  0.31 ||\n|| Trades Unions | 19 |  0.44 |  0.44 |  0.60 | -0.05 | 0.44 |  0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 |  0.48 |  0.32 ||\n|| Video Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 |  0.40 |  0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 |  0.32 |  0.42 ||\n|| Average | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 |  0.22 |  0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 |  0.20 |  0.17 ||\n\nQuestion: Is it true that The semantic threshold for OD-d2v is set at 0.3 while for OD-w2v is set at 0.6?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "bb0c415e-fa38-4ace-aa92-8f8480f1b2ab", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.\nTable:\n||  System |  Full UAS |  PPA Acc. ||\n|| RBG | 94.17 | 88.51 ||\n|| RBG + HPCD (full) | 94.19 | 89.59 ||\n|| RBG + LSTM-PP | 94.14 | 86.35 ||\n|| RBG + OntoLSTM-PP | 94.30 | 90.11 ||\n|| RBG + Oracle PP | 94.60 | 98.97 ||\n\nQuestion: Is it true that However, when gold PP attachment are used, we note a large potential improve  ment of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "05fde2b1-5561-41b9-b324-49eb1967cf32", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.\nTable:\n|| Dataset | Accuracy | Fleiss’ kappa   k ||\n|| Original COPA | 100.0 | 0.973 ||\n|| Balanced COPA | 97.0 | 0.798 ||\n\nQuestion: Is it true that The human evaluation shows that our mirrored instances are not as difficult as the original ones (see Table 3)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e042f4df-12c4-467c-95bd-5043fd5e178e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).\nTable:\n||  | dev CS | dev mono | test CS | test mono ||\n|| CS-only-LM | 45.20 | 65.87 | 43.20 | 62.80 ||\n|| Fine-Tuned-LM | 49.60 | 72.67 | 47.60 | 71.33 ||\n|| CS-only-disc |  75.60 | 70.40 | 70.80 | 70.53 ||\n|| Fine-Tuned-disc | 70.80 |  74.40 |  75.33 |  75.87 ||\n\nQuestion: Is it true that The CS-ONLY-DISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "6532fb08-f821-4080-912e-391b0e279557", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\nTable:\n||  | Prec. | Rec. | F1 ||\n|| (A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531 ||\n|| (A2) Standard attention | 0.466 | 0.638 | 0.539 ||\n|| (A3) Window size (  ws)=5 | 0.507 | 0.652 |  0.571 ||\n|| (A4) Window size (  ws)=10 | 0.510 | 0.640 | 0.568 ||\n|| (A5) Softmax | 0.490 | 0.658 | 0.562 ||\n|| (A6) Max-pool | 0.492 | 0.600 | 0.541 ||\n\nQuestion: Is it true that Increasing the window size to 10 increases the F1 score marginally (A3−A4)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ef19ef2f-f972-4231-b2ce-9604c2f0de42", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nTable:\n|| Method | En→It best | En→It avg | En→It iters | En→De best | En→De avg | En→De iters | En→Fi best | En→Fi avg | En→Fi iters | En→Es best | En→Es avg | En→Es iters ||\n|| Artetxe et al., 2018b |  48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808 ||\n|| Noise-aware Alignment |  48.53 |  48.20 | 471 |  49.67 |  48.89 | 568 |  33.98 |  33.68 | 502 |  38.40 |  37.79 | 551 ||\n\nQuestion: Is it true that In contrast, the noise-aware model requires more iterations to converge?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "689f8a9c-3097-4448-b010-e9413fcaeabc", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.\nTable:\n|| Corpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| Europarl | TotalTerms: | 957 | 1,000 | 1,000 | 1,000 | 1,000 | 836 | 1,000 ||\n|| Europarl | TotalRoots: | 44 | 1 | 1 | 1 | 1 | 43 | 1 ||\n|| Europarl | NumberRels: | 1,588 | 1,025 | 1,028 | 1,185 | 1,103 | 1,184 | 999 ||\n|| Europarl | MaxDepth: | 21 | 921 | 901 | 788 | 835 | 8 | 15 ||\n|| Europarl | MinDepth: | 1 | 921 | 901 | 788 | 835 | 1 | 1 ||\n|| Europarl | AvgDepth: | 11.82 | 921 | 901 | 788 | 835 | 3.05 | 8.46 ||\n|| Europarl | DepthCohesion: | 1.78 | 1 | 1 | 1 | 1 | 2.62 | 1.77 ||\n|| Europarl | MaxWidth: | 20 | 2 | 3 | 4 | 3 | 88 | 41 ||\n|| Europarl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| Europarl | AvgWidth: | 1.99 | 1.03 | 1.03 | 1.19 | 1.10 | 4.20 | 2.38 ||\n|| TED Talks | TotalTerms: | 476 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 ||\n|| TED Talks | TotalRoots: | 164 | 2 | 1 | 1 | 1 | 1 | 1 ||\n|| TED Talks | NumberRels: | 521 | 1,029 | 1,331 | 3,025 | 3,438 | 3,802 | 1,009 ||\n|| TED Talks | MaxDepth: | 16 | 915 | 658 | 454 | 395 | 118 | 12 ||\n|| TED Talks | MinDepth: | 1 | 913 | 658 | 454 | 395 | 110 | 1 ||\n|| TED Talks | AvgDepth: | 5.82 | 914 | 658 | 454 | 395 | 112.24 | 5.95 ||\n|| TED Talks | DepthCohesion: | 2.75 | 1 | 1 | 1 | 1 | 1.05 | 2.02 ||\n|| TED Talks | MaxWidth: | 25 | 2 | 77 | 13 | 12 | 66 | 98 ||\n|| TED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| TED Talks | AvgWidth: | 1.83 | 1.03 | 1.36 | 3.03 | 3.44 | 6.64 | 2.35 ||\n\nQuestion: Is it true that  For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 789 terms with different values of term frequency, while having 211 that share the same value of term frequency with other terms?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f8dc8a50-467a-4ac0-8ec0-796d2151a87d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\nTable:\n|| <bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold> ||\n|| LDC2015E86 | LDC2015E86 | LDC2015E86 ||\n|| Konstas et al. (2017) | 22.00 | - ||\n|| Song et al. (2018) | 23.28 | 30.10 ||\n|| Cao et al. (2019) | 23.50 | - ||\n|| Damonte et al.(2019) | 24.40 | 23.60 ||\n|| Guo et al. (2019) | <bold>25.70</bold> | - ||\n|| S2S | 22.55 ± 0.17 | 29.90 ± 0.31 ||\n|| G2S-GIN | 22.93 ± 0.20 | 29.72 ± 0.09 ||\n|| G2S-GAT | 23.42 ± 0.16 | 29.87 ± 0.14 ||\n|| G2S-GGNN | 24.32 ± 0.16 | <bold>30.53</bold> ± 0.30 ||\n|| LDC2017T10 | LDC2017T10 | LDC2017T10 ||\n|| Back et al. (2018) | 23.30 | - ||\n|| Song et al. (2018) | 24.86 | 31.56 ||\n|| Damonte et al.(2019) | 24.54 | 24.07 ||\n|| Cao et al. (2019) | 26.80 | - ||\n|| Guo et al. (2019) | 27.60 | - ||\n|| S2S | 22.73 ± 0.18 | 30.15 ± 0.14 ||\n|| G2S-GIN | 26.90 ± 0.19 | 32.62 ± 0.04 ||\n|| G2S-GAT | 26.72 ± 0.20 | 32.52 ± 0.02 ||\n|| G2S-GGNN | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 ||\n\nQuestion: Is it true that  For both datasets, our approach substantially outperforms the baselines?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "6c15ac43-fcb9-4598-a50f-607a89c8074f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nTable:\n|| Model | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time ||\n|| Rocktäschel et al. ( 2016 ) | Rocktäschel et al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | - ||\n|| This | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 |  90.49 | 0.696 ||\n|| This | GRU | 6.41M |  85.71 | 0.245 |  86.05 | 0.419 |  90.29 | 0.529 | 90.10 | 0.695 ||\n|| This | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580 ||\n|| Work | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555 ||\n||  | LRN | 4.25M | 84.88 |  0.209 | 85.06 |  0.223 | 89.98 |  0.488 | 89.93 |  0.506 ||\n\nQuestion: Is it true that Overall results show that ATR achieves the best performance and consumes the least training time?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "9846b931-84f9-407f-9a32-b37c96b7c9f1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.\nTable:\n|| Metric | Method of validation | Yelp | Lit. ||\n|| Acc | % of machine and human judgments that match | 94 | 84 ||\n|| Sim | Spearman’s   ρ b/w Sim and human ratings of semantic preservation | 0.79 | 0.75 ||\n|| PP | Spearman’s   ρ b/w negative PP and human ratings of fluency | 0.81 | 0.67 ||\n\nQuestion: Is it true that  To validate Acc, human annotators were asked to judge the style of 100 transferred sentences  We then compute the percentage of machine and human judgments that match?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "e8f0310f-c946-4b95-88b3-d257d8ea56f7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.\nTable:\n||  Model |  Type |  English-German #P |  English-German B |  English-German C |  English-Czech #P |  English-Czech B |  English-Czech C ||\n|| BoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | - ||\n|| CNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | - ||\n|| BiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | - ||\n|| PB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3 ||\n|| DCGCN (ours) | Single |   29.7M |  19.0 |  44.1 |   28.3M |  12.1 |  37.1 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9 ||\n|| DCGCN (ours) | Ensemble |   149M |  20.5 |  45.8 |   142M |  13.1 |  37.8 ||\n\nQuestion: Is it true that Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "c0e96242-c3ea-48c3-a932-693d83be5c5c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nTable:\n|| Method | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success ||\n|| GP-MBCM | 2.99 | 19.04 | 44.29 | 28.9 ||\n|| ACER | 10.49 | 77.98 | 62.83 | 50.8 ||\n|| PPO | 9.83 | 83.34 | 69.09 | 59.1 ||\n|| ALDM | 12.47 | 81.20 | 62.60 | 61.2 ||\n|| GDPL-sess |  7.49 | 88.39 | 77.56 | 76.4 ||\n|| GDPL-discr | 7.86 | 93.21 | 80.43 | 80.5 ||\n|| GDPL | 7.64 |  94.97 |  83.90 |  86.5 ||\n||  Human |  7.37 |  66.89 |  95.29 |  75.0 ||\n\nQuestion: Is it true that  Surprisingly, GDPL even outperforms human in completing the task, and its average dialog turns are close to those of humans, though GDPL is inferior in terms of match rate?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "922444a0-578f-4b72-a5f4-e457f6e26693", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\nTable:\n||  System |  ROUGE-1   R (%) |  ROUGE-1   P (%) |  ROUGE-1   F (%) |  ROUGE-2   R (%) |  ROUGE-2   P (%) |  ROUGE-2   F (%) |  Sentence-Level   R (%) |  Sentence-Level   P (%) |  Sentence-Level   F (%) ||\n||  ILP | 24.5 | 41.1 | 29.3±0.5 | 7.9 | 15.0 | 9.9±0.5 | 13.6 | 22.6 | 15.6±0.4 ||\n||  Sum-Basic | 28.4 | 44.4 | 33.1±0.5 | 8.5 | 15.6 | 10.4±0.4 | 14.7 | 22.9 | 16.7±0.5 ||\n||  KL-Sum | 39.5 | 34.6 | 35.5±0.5 | 13.0 | 12.7 | 12.3±0.5 | 15.2 | 21.1 | 16.3±0.5 ||\n||  LexRank | 42.1 | 39.5 | 38.7±0.5 | 14.7 | 15.3 | 14.2±0.5 | 14.3 | 21.5 | 16.0±0.5 ||\n||  MEAD | 45.5 | 36.5 | 38.5± 0.5 | 17.9 | 14.9 | 15.4±0.5 | 27.8 | 29.2 | 26.8±0.5 ||\n||  SVM | 19.0 | 48.8 | 24.7±0.8 | 7.5 | 21.1 | 10.0±0.5 | 32.7 | 34.3 | 31.4±0.4 ||\n||  LogReg | 26.9 | 34.5 | 28.7±0.6 | 6.4 | 9.9 | 7.3±0.4 | 12.2 | 14.9 | 12.7±0.5 ||\n||  LogReg  r | 28.0 | 34.8 | 29.4±0.6 | 6.9 | 10.4 | 7.8±0.4 | 12.1 | 14.5 | 12.5±0.5 ||\n||  HAN | 31.0 | 42.8 | 33.7±0.7 | 11.2 | 17.8 | 12.7±0.5 | 26.9 | 34.1 | 32.4±0.5 ||\n||  HAN+pretrainT | 32.2 | 42.4 | 34.4±0.7 | 11.5 | 17.5 | 12.9±0.5 | 29.6 | 35.8 | 32.2±0.5 ||\n||  HAN+pretrainU | 32.1 | 42.1 | 33.8±0.7 | 11.6 | 17.6 | 12.9±0.5 | 30.1 | 35.6 | 32.3±0.5 ||\n||  HAN  r | 38.1 | 40.5 |  37.8±0.5 | 14.0 | 17.1 |  14.7±0.5 | 32.5 | 34.4 |  33.4±0.5 ||\n||  HAN+pretrainT  r | 37.9 | 40.4 |  37.6±0.5 | 13.5 | 16.8 |  14.4±0.5 | 32.5 | 34.4 |  33.4±0.5 ||\n||  HAN+pretrainU  r | 37.9 | 40.4 |  37.6±0.5 | 13.6 | 16.9 |  14.4±0.5 | 33.9 | 33.8 |  33.8±0.5 ||\n\nQuestion: Is it true that HAN models outperform both LogReg and SVM using the current set of features?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3d29f63f-a0e4-422c-8299-a1e9ebd041b4", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving Generalization by Incorporating Coverage in Natural Language Inference Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\nTable:\n||  | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK ||\n|| MQAN | 72.30 | 60.91 | 41.82 | 53.95 ||\n|| + coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold> ||\n|| ESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37 ||\n|| + coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold> ||\n\nQuestion: Is it true that The results show that coverage information does not improve the generalization of both examined models across various NLI datasets?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "bcd7aad2-e4b4-4978-ad9e-8aee69123d1c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\nTable:\n||  Emoji alias |  N |  emoji # |  emoji % |  no-emoji # |  no-emoji % |  Δ% ||\n|| mask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27 ||\n|| two_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59 ||\n|| heart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91 ||\n|| heart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75 ||\n|| rage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04 ||\n|| cry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07 ||\n|| sob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67 ||\n|| unamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00 ||\n|| weary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49 ||\n|| joy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05 ||\n|| sweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80 ||\n|| confused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60 ||\n\nQuestion: Is it true that  Further, contrary to intuition, the sob emoji contributes less than cry, despite representing a stronger emotion?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b8dd43d6-532b-40e2-8758-5c8e8fd69a02", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Method | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R ||\n|| CBOW/784 | 90.0 |  79.2 |  74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 |  78.1 ||\n|| CMOW/784 | 87.5 | 73.4 | 70.6 |  87.3 | 69.6 |  88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2 ||\n|| Hybrid |  90.2 | 78.7 | 73.7 |  87.3 |  72.7 | 87.6 |  79.4 |  79.6 |  43.3 |  63.4 | 77.8 ||\n|| cmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4% ||\n|| cmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1% ||\n\nQuestion: Is it true that However, CMOW generally outperforms CBOW embeddings?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ed79ce87-cdaa-4117-94e3-e9003fdbdb66", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\nTable:\n||  System |  ROUGE-1   R (%) |  ROUGE-1   P (%) |  ROUGE-1   F (%) |  ROUGE-2   R (%) |  ROUGE-2   P (%) |  ROUGE-2   F (%) |  Sentence-Level   R (%) |  Sentence-Level   P (%) |  Sentence-Level   F (%) ||\n||  ILP | 24.5 | 41.1 | 29.3±0.5 | 7.9 | 15.0 | 9.9±0.5 | 13.6 | 22.6 | 15.6±0.4 ||\n||  Sum-Basic | 28.4 | 44.4 | 33.1±0.5 | 8.5 | 15.6 | 10.4±0.4 | 14.7 | 22.9 | 16.7±0.5 ||\n||  KL-Sum | 39.5 | 34.6 | 35.5±0.5 | 13.0 | 12.7 | 12.3±0.5 | 15.2 | 21.1 | 16.3±0.5 ||\n||  LexRank | 42.1 | 39.5 | 38.7±0.5 | 14.7 | 15.3 | 14.2±0.5 | 14.3 | 21.5 | 16.0±0.5 ||\n||  MEAD | 45.5 | 36.5 | 38.5± 0.5 | 17.9 | 14.9 | 15.4±0.5 | 27.8 | 29.2 | 26.8±0.5 ||\n||  SVM | 19.0 | 48.8 | 24.7±0.8 | 7.5 | 21.1 | 10.0±0.5 | 32.7 | 34.3 | 31.4±0.4 ||\n||  LogReg | 26.9 | 34.5 | 28.7±0.6 | 6.4 | 9.9 | 7.3±0.4 | 12.2 | 14.9 | 12.7±0.5 ||\n||  LogReg  r | 28.0 | 34.8 | 29.4±0.6 | 6.9 | 10.4 | 7.8±0.4 | 12.1 | 14.5 | 12.5±0.5 ||\n||  HAN | 31.0 | 42.8 | 33.7±0.7 | 11.2 | 17.8 | 12.7±0.5 | 26.9 | 34.1 | 32.4±0.5 ||\n||  HAN+pretrainT | 32.2 | 42.4 | 34.4±0.7 | 11.5 | 17.5 | 12.9±0.5 | 29.6 | 35.8 | 32.2±0.5 ||\n||  HAN+pretrainU | 32.1 | 42.1 | 33.8±0.7 | 11.6 | 17.6 | 12.9±0.5 | 30.1 | 35.6 | 32.3±0.5 ||\n||  HAN  r | 38.1 | 40.5 |  37.8±0.5 | 14.0 | 17.1 |  14.7±0.5 | 32.5 | 34.4 |  33.4±0.5 ||\n||  HAN+pretrainT  r | 37.9 | 40.4 |  37.6±0.5 | 13.5 | 16.8 |  14.4±0.5 | 32.5 | 34.4 |  33.4±0.5 ||\n||  HAN+pretrainU  r | 37.9 | 40.4 |  37.6±0.5 | 13.6 | 16.9 |  14.4±0.5 | 33.9 | 33.8 |  33.8±0.5 ||\n\nQuestion: Is it true that When redundancy removal was applied to LogReg, it produces significant improvement?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "4e7fe7a0-0c7a-4e48-b4e9-0aaa1d9a2c28", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).\nTable:\n||  |  Model |  dev mean |  dev best |  test mean |  test best |  α ||\n|| single | text | 86.54 | 86.80 | 86.47 | 86.96 | – ||\n|| single | raw | 35.00 | 37.33 | 35.78 | 37.70 | – ||\n|| single | innovations | 80.86 | 81.51 | 80.28 | 82.15 | – ||\n|| early | text + raw | 86.46 | 86.65 | 86.24 | 86.53 | – ||\n|| early | text + innovations | 86.53 | 86.77 | 86.54 | 87.00 | – ||\n|| early | text + raw + innovations | 86.35 | 86.69 | 86.55 | 86.44 | – ||\n|| late | text + raw | 86.71 | 87.05 | 86.35 | 86.71 | 0.2 ||\n|| late | text + innovations |  86.98 |  87.48 |  86.68 |  87.02 | 0.5 ||\n|| late | text + raw + innovations | 86.95 | 87.30 | 86.60 | 86.87 | 0.5 ||\n\nQuestion: Is it true that We found that innovations are not helpful in both early and late fusion frameworks, and late fusion does not perform better on average?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "cdad25c1-2680-41e3-b279-9383fc241c09", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\nTable:\n||  | Prec. | Rec. | F1 ||\n|| (A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531 ||\n|| (A2) Standard attention | 0.466 | 0.638 | 0.539 ||\n|| (A3) Window size (  ws)=5 | 0.507 | 0.652 |  0.571 ||\n|| (A4) Window size (  ws)=10 | 0.510 | 0.640 | 0.568 ||\n|| (A5) Softmax | 0.490 | 0.658 | 0.562 ||\n|| (A6) Max-pool | 0.492 | 0.600 | 0.541 ||\n\nQuestion: Is it true that Replacing the attention normalizing function with softmax operation increases the F1 score marginally (A3−A5)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "aa179cf6-fac1-48d2-8bcf-e5ad45f72d25", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 3: ARI and Silhouette coefficient scores.\nTable:\n|| Methods | Seanad Abolition ARI | Seanad Abolition   Sil | Video Games ARI | Video Games   Sil | Pornography ARI | Pornography   Sil ||\n|| TF-IDF | 0.23 | 0.02 | -0.01 | 0.01 | -0.02 | 0.01 ||\n|| WMD | 0.09 | 0.01 | 0.01 | 0.01 | -0.02 | 0.01 ||\n|| Sent2vec | -0.01 | -0.01 | 0.11 | 0.06 | 0.01 | 0.02 ||\n|| Doc2vec | -0.01 | -0.03 | -0.01 | 0.01 | 0.02 | -0.01 ||\n|| BERT | 0.03 | -0.04 | 0.08 | 0.05 | -0.01 | 0.03 ||\n|| OD-parse | 0.01 | -0.04 | -0.01 | 0.02 | 0.07 | 0.05 ||\n|| OD |  0.54 |  0.31 |  0.56 |  0.42 |  0.41 |  0.41 ||\n\nQuestion: Is it true that  A notable exception is the \"Seanad Abolition\" dataset, where TF-IDF performs relatively better than WMD, Sent2vec and Doc2vec?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "058d2a40-7061-4dfa-bb41-bf00efd55f2c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.\nTable:\n||  | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction ||\n|| Same Gender | 0.442 | 0.434 | 0.424 | – | 0.491 | 0.478 | 0.446 | – ||\n|| Different Gender | 0.385 | 0.421 | 0.415 | – | 0.415 | 0.435 | 0.403 | – ||\n|| difference | 0.057 | 0.013 | 0.009 |  91.67% | 0.076 | 0.043 | 0.043 |  100% ||\n\nQuestion: Is it true that In German, we get a reduction of 100%?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ca96fdb8-2600-46e2-b129-d6c81562945f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| -{4} dense block | 24.8 | 54.9 ||\n|| -{3, 4} dense blocks | 23.8 | 54.1 ||\n|| -{2, 3, 4} dense blocks | 23.2 | 53.1 ||\n\nQuestion: Is it true that Although these four models have the same number of layers, dense connections allow the model to achieve much better performance?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "87b2347d-2e19-4a25-bb66-a742f0e8ebfd", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nTable:\n|| Model | Training data | Overall | Easy | Hard ||\n|| BERT-large | B-COPA | 70.5 (± 2.5) | 72.6 (± 2.3) |  69.1 (± 2.7) ||\n|| BERT-large | B-COPA (50%) | 69.9 (± 1.9) | 71.2 (± 1.3) | 69.0 (± 3.5) ||\n|| BERT-large | COPA |  71.7 (± 0.5) |  80.5 (± 0.4) | 66.3 (± 0.8) ||\n|| RoBERTa-large | B-COPA |  76.7 (± 0.8) | 73.3 (± 1.5) |  78.8 (± 2.0) ||\n|| RoBERTa-large | B-COPA (50%) | 72.4 (± 2.0) | 72.1 (± 1.7) | 72.6 (± 2.1) ||\n|| RoBERTa-large | COPA | 76.4 (± 0.7) |  79.6 (± 1.0) | 74.4 (± 1.1) ||\n|| BERT-base-NSP | None |  66.4 | 66.2 |  66.7 ||\n|| BERT-large-NSP | None | 65.0 |  66.9 | 62.1 ||\n\nQuestion: Is it true that The relatively low accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are not well-equipped to perform this task \"out-of-the-box\"?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "6943a94d-d91a-4fb2-965f-97aa1fa957ce", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that Longer sentences pose additional challenges to the models?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "e8e28650-51c4-4fbe-b946-b7966b1625a2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.\nTable:\n|| Batch size | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Training | Throughput (instances/s) Training | Throughput (instances/s) Training ||\n|| Batch size | Iter | Recur | Fold | Iter | Recur | Fold ||\n|| 1 | 19.2 | 81.4 | 16.5 | 2.5 | 4.8 | 9.0 ||\n|| 10 | 49.3 | 217.9 | 52.2 | 4.0 | 4.2 | 37.5 ||\n|| 25 | 72.1 | 269.9 | 61.6 | 5.5 | 3.6 | 54.7 ||\n\nQuestion: Is it true that  As a result, the folding technique performs better than the recursive approach for the training task?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "755c48ed-ff95-42cf-9e30-670ae9546e4d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "1cd4c25a-774f-4e75-a511-1dead6a68155", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Sparse and Structured Visual Attention Table 2: Human evaluation results on MSCOCO.\nTable:\n||  | caption | attention relevance ||\n|| softmax | 3.50 | 3.38 ||\n|| sparsemax | 3.71 | 3.89 ||\n|| TVmax |  3.87 |  4.10 ||\n\nQuestion: Is it true that The inferior score on attention relevance shows that TVMAX is worse at selecting the relevant features and its output is less interpretable?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "cd3bfb66-26db-4ea5-bbc7-33e9dd881d74", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.\nTable:\n||  Block |  n |  m | B | C ||\n|| 1 | 1 | 1 | 17.6 | 48.3 ||\n|| 1 | 1 | 2 | 19.2 | 50.3 ||\n|| 1 | 2 | 1 | 18.4 | 49.1 ||\n|| 1 | 1 | 3 | 19.6 | 49.4 ||\n|| 1 | 3 | 1 | 20.0 | 50.5 ||\n|| 1 | 3 | 3 | 21.4 | 51.0 ||\n|| 1 | 3 | 6 | 21.8 | 51.7 ||\n|| 1 | 6 | 3 | 21.7 | 51.5 ||\n|| 1 | 6 | 6 | 22.0 | 52.1 ||\n|| 2 | 3 | 6 |  23.5 | 53.3 ||\n|| 2 | 6 | 3 | 23.3 |  53.4 ||\n|| 2 | 6 | 6 | 22.0 | 52.1 ||\n\nQuestion: Is it true that We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give significantly different results for both 1 DCGCN block and 2 DCGCN blocks?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "4d0e6a67-dd03-4a72-9b81-f8e50e9faae3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).\nTable:\n||  |  Model |  dev mean |  dev best |  test mean |  test best |  α ||\n|| single | text | 86.54 | 86.80 | 86.47 | 86.96 | – ||\n|| single | raw | 35.00 | 37.33 | 35.78 | 37.70 | – ||\n|| single | innovations | 80.86 | 81.51 | 80.28 | 82.15 | – ||\n|| early | text + raw | 86.46 | 86.65 | 86.24 | 86.53 | – ||\n|| early | text + innovations | 86.53 | 86.77 | 86.54 | 87.00 | – ||\n|| early | text + raw + innovations | 86.35 | 86.69 | 86.55 | 86.44 | – ||\n|| late | text + raw | 86.71 | 87.05 | 86.35 | 86.71 | 0.2 ||\n|| late | text + innovations |  86.98 |  87.48 |  86.68 |  87.02 | 0.5 ||\n|| late | text + raw + innovations | 86.95 | 87.30 | 86.60 | 86.87 | 0.5 ||\n\nQuestion: Is it true that The interpolation weight α for the late fusion experiments is high when innovations are used, which further indicates that innovation features are useful in overall prediction?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "82060521-bd71-4f0d-90fd-6b0e9de930b3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that The performances of all models increase as the diameters of the graphs increase?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "92d4db6a-df9b-45a3-bb55-a309229fec18", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: On the difficulty of a distributional semantics of spoken language Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.\nTable:\n||  | Recall@10 (%) | Median rank | RSAimage ||\n|| VGS | 27 | 6 | 0.4 ||\n|| SegMatch |  10 |  37 |  0.5 ||\n|| Audio2vec-U | 5 | 105 | 0.0 ||\n|| Audio2vec-C | 2 | 647 | 0.0 ||\n|| Mean MFCC | 1 | 1,414 | 0.0 ||\n|| Chance | 0 | 3,955 | 0.0 ||\n\nQuestion: Is it true that It does not come close to VGS on paraphrase retrieval, but it does correlate with the visual modality even better?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ad0e53cd-7e67-4eb5-bb8d-33949d1f8e6a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 5: Textual similarity scores (asymmetric, Multi30k).\nTable:\n||  | EN → DE R@1 | EN → DE R@5 | EN → DE R@10 | DE → EN R@1 | DE → EN R@5 | DE → EN R@10 ||\n|| FME | 51.4 | 76.4 | 84.5 | 46.9 | 71.2 | 79.1 ||\n|| AME |  51.7 |  76.7 |  85.1 |  49.1 |  72.6 |  80.5 ||\n\nQuestion: Is it true that AME outperforms the FME model, confirming the importance of word embeddings adaptation?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "67184fb9-20ca-445e-8366-7d03160cce3a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.\nTable:\n||  Model |  T | #P | B | C ||\n|| Seq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5 ||\n|| DCGCN (ours) | S |  19.1M | 27.9 | 57.3 ||\n|| DCGCN (ours) | E | 92.5M |  30.4 |  59.6 ||\n\nQuestion: Is it true that The single DCGCN model achieves a BLEU score of 30.4 and a CHRF++ score of 59.6, outperforming the ensemble approach based on combining five DCGCN models initialized with different random seeds?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "292204b8-c7d0-4b68-8a35-f392930d4194", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 4: Results of Self-Play Evaluation.\nTable:\n|| System | TGPC Succ. (%) | TGPC #Turns | CWC Succ. (%) | CWC #Turns ||\n|| Retrieval  | 7.16 | 4.17 | 0 | - ||\n|| Retrieval-Stgy  | 47.80 | 6.7 | 44.6 | 7.42 ||\n|| PMI  | 35.36 | 6.38 | 47.4 | 5.29 ||\n|| Neural  | 54.76 | 4.73 | 47.6 | 5.16 ||\n|| Kernel  | 62.56 | 4.65 | 53.2 | 4.08 ||\n|| DKRN (ours) |  89.0 | 5.02 |  84.4 | 4.20 ||\n\nQuestion: Is it true that This superior confirms the effectiveness of our approach?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "026892e8-1d2c-412c-8881-af5a2b4400b2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nTable:\n|| VS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L ||\n|| ACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18 ||\n|| PPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10 ||\n|| ALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15 ||\n\nQuestion: Is it true that Among all the baselines, GDPL does not obtain the most preference against PPO?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "69b2e0e5-215d-4de9-840b-6752ca98311a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.\nTable:\n||  Model |  Type |  English-German #P |  English-German B |  English-German C |  English-Czech #P |  English-Czech B |  English-Czech C ||\n|| BoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | - ||\n|| CNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | - ||\n|| BiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | - ||\n|| PB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3 ||\n|| DCGCN (ours) | Single |   29.7M |  19.0 |  44.1 |   28.3M |  12.1 |  37.1 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9 ||\n|| DCGCN (ours) | Ensemble |   149M |  20.5 |  45.8 |   142M |  13.1 |  37.8 ||\n\nQuestion: Is it true that The DCGCN models do not achieve the highest BLEU points on the En-De and En-Cs tasks, respectively?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "5cd2b818-6cbd-43ff-a379-23d4544992da", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\nTable:\n||  ResNet-34 |  Eval set % |  #param ||\n|| Baseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M ||\n|| SA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M ||\n|| SA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M ||\n||  SA (S: 1,2,3 - B: 3) |  55.27 | } 0.107M ||\n\nQuestion: Is it true that We showed that it is not possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "5a52f554-0dbd-442d-af13-23015337b5f0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Building a Production Model for Retrieval-Based Chatbots Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.\nTable:\n||  Model |  Parameters |  Validation AUC@0.05 |  Test AUC@0.05 ||\n|| Base | 8.0M |  0.871 | 0.816 ||\n|| 4L SRU → 2L LSTM | 7.3M | 0.864 |  0.829 ||\n|| 4L SRU → 2L SRU | 7.8M | 0.856 |  0.829 ||\n|| Flat → hierarchical | 12.4M | 0.825 | 0.559 ||\n|| Cross entropy → hinge loss | 8.0M | 0.765 | 0.693 ||\n|| 6.6M → 1M examples | 8.0M | 0.835 | 0.694 ||\n|| 6.6M → 100K examples | 8.0M | 0.565 | 0.417 ||\n|| 200 → 100 negatives | 8.0M | 0.864 | 0.647 ||\n|| 200 → 10 negatives | 8.0M | 0.720 | 0.412 ||\n\nQuestion: Is it true that We observed an advantage to using a hierachical encoder,  Finally, we see that a 2 layer LSTM performs worse than either a 4 layer or a 2 layer SRU with a comparable number of parameters?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ef8adc9d-e855-47d4-b3d1-f5099f9892f7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.\nTable:\n||  Model |  PPA Acc. ||\n|| full | 89.7 ||\n|| - sense priors | 88.4 ||\n|| - attention | 87.5 ||\n\nQuestion: Is it true that The second row in Table 3 shows the test accuracy of a system trained without sense priors  and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations, but still grounded in WordNet?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "58fa3a44-8ffc-4329-bddb-79ec2c2b9b21", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.\nTable:\n||  |  Present |  Not Present ||\n|| Emoji | 4805 (76.6%) | 23952 (68.0%) ||\n|| Hashtags | 2122 (70.5%) | 26635 (69.4%) ||\n\nQuestion: Is it true that  Hashtags also have a  positive effect on classification performance, however it is less significant?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "284e66c0-e63c-4e04-99a7-e91ee70cdd14", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.\nTable:\n|| Source | Target | Svm | Ra-Svm‡ | Ra-Cnn‡ | Trans† | Ra-Trans‡† | Ours‡† | Oracle† ||\n|| Beer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 |  79.53 | 80.29 ||\n|| Beer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 |  77.94 | 78.11 ||\n|| Beer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 |  75.24 | 75.50 ||\n\nQuestion: Is it true that Our model (OURS) obtains substantial gains in accuracy over the baselines across all three target aspects?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "9c92823f-0db0-4455-b09c-2636c5e90d5d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| Encoder Modules |  |  ||\n|| -Linear Combination | 23.7 | 53.2 ||\n|| -Global Node | 24.2 | 54.6 ||\n|| -Direction Aggregation | 24.6 | 54.6 ||\n|| -Graph Attention | 24.9 | 54.7 ||\n|| -Global Node&Linear Combination | 22.9 | 52.4 ||\n|| Decoder Modules |  |  ||\n|| -Coverage Mechanism | 23.8 | 53.0 ||\n\nQuestion: Is it true that With the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a6442b25-639a-4e9f-acc1-2af93942e266", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Two Causal Principles for Improving Visual Dialog Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.\nTable:\n|| Model | LF  | HCIAE  | CoAtt  | RvA  ||\n|| baseline | 57.21 | 56.98 | 56.46 | 56.74 ||\n|| +P1 | 61.88 | 60.12 | 60.27 | 61.02 ||\n|| +P2 | 72.65 | 71.50 | 71.41 | 71.44 ||\n|| +P1+P2 |  73.63 | 71.99 | 71.87 | 72.88 ||\n\nQuestion: Is it true that Note that the effectiveness of P1 and P2 are not necessarily additive, as combining P1 and P2 does not always perform the best?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "5b31abdf-f132-46a7-8da5-490adbe8d469", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.\nTable:\n||  m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1 ||\n|| 1 | 0.541 | 0.595 |  0.566 | 0.495 | 0.621 | 0.551 ||\n|| 2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555 ||\n|| 3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564 ||\n|| 4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 |  0.571 ||\n|| 5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567 ||\n\nQuestion: Is it true that We observe that for the NYT10 dataset, m = 4 gives the highest F1 score?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "2096086d-1f21-4a72-992f-724d69319e5d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\nTable:\n||  Training data |  Add |  Miss |  Wrong |  Disfl ||\n|| Original | 0 | 22 | 0 | 14 ||\n|| Cleaned added | 0 | 23 | 0 | 14 ||\n|| Cleaned missing | 0 | 1 | 0 | 2 ||\n|| Cleaned | 0 | 0 | 0 | 5 ||\n\nQuestion: Is it true that The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "e02742aa-05d4-4e91-ab12-6f4aaa5409ee", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Two Causal Principles for Improving Visual Dialog Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.\nTable:\n|| Model | baseline | QT | S   R0 | S   R1 | S   R2 | S   R3 | D ||\n|| LF  | 57.21 | 58.97 | 67.82 | 71.27 | 72.04 | 72.36 | 72.65 ||\n|| LF +P1 | 61.88 | 62.87 | 69.47 | 72.16 | 72.85 | 73.42 |  73.63 ||\n\nQuestion: Is it true that Overall, all of the implementations can improve the performances of base models?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7dad5701-2235-4fc8-b66b-306812347530", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Dim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n|| 400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7 ||\n|| 400 | CMOW/400 |  34.4 | 68.8 | 80.1 |  79.9 |  59.8 | 81.9 |  79.2 |  70.7 |  50.3 | 70.7 ||\n|| 400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 |  87.2 ||\n|| 400 | H-CMOW | 32.3 |  70.8 |  81.3 | 76.0 | 59.6 |  82.3 | 77.4 | 70.0 | 50.2 | 38.2 ||\n|| 784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 |  89.5 ||\n|| 784 | CMOW/784 |  35.1 |  70.8 |  82.0 | 80.2 |  61.8 | 82.8 |  79.7 | 74.2 |  50.7 | 72.9 ||\n|| 800 | Hybrid | 35.0 |  70.8 | 81.7 |  81.0 | 59.4 |  84.4 | 79.0 |  74.3 | 49.3 | 87.6 ||\n|| - | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1% ||\n|| - | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9% ||\n\nQuestion: Is it true that In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks except WC?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "38bfa8f1-7948-49d5-bc60-08c75e385df9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Two Causal Principles for Improving Visual Dialog Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table 1. Note that only applying P2 is implemented by the implementations in Section 5 with the history shortcut.\nTable:\n|| Model | LF  | HCIAE  | CoAtt  | RvA  ||\n|| baseline | 57.21 | 56.98 | 56.46 | 56.74 ||\n|| +P1 | 61.88 | 60.12 | 60.27 | 61.02 ||\n|| +P2 | 72.65 | 71.50 | 71.41 | 71.44 ||\n|| +P1+P2 |  73.63 | 71.99 | 71.87 | 72.88 ||\n\nQuestion: Is it true that Note that the effectiveness of P1 and P2 are additive, which means combining P1 and P2 performs the best?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0abaf60d-6117-4b20-8493-f3678aadd259", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\nTable:\n||  | MSCOCO spice | MSCOCO cider | MSCOCO rouge  L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep↓ | Flickr30k spice | Flickr30k cider | Flickr30k rouge  L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep↓ ||\n|| softmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09 ||\n|| sparsemax |  18.9 |  0.990 |  53.5 |  31.5 |  25.3 | 3.69 |  13.7 |  0.444 |  44.3 |  20.7 |  19.3 | 5.84 ||\n|| TVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 |  3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 |  3.97 ||\n\nQuestion: Is it true that Selective attention mechanisms like sparsemax and especially TVMAX do not reduce repetition, as measured by the REP metric reported in Table 1?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "c38e08e0-2ef9-4fdc-bf65-6218e5f62a85", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.\nTable:\n|| Train | Test |  System |  BLEU |  NIST |  METEOR |  ROUGE-L |  CIDEr |  Add |  Miss |  Wrong |  SER ||\n|| Original |  Original | TGen− | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94 ||\n|| Original |  Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27 ||\n|| Original |  Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80 ||\n|| Original |  Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen− | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37 ||\n|| Cleaned missing |  Original | TGen− | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61 ||\n|| Cleaned missing |  Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53 ||\n|| Cleaned missing |  Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen− | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45 ||\n\nQuestion: Is it true that The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "db48fd7d-eac7-402b-986a-1295738e6236", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Domain Adaptive Inference for Neural Machine Translation Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.\nTable:\n||  Decoder configuration |  es-en   Health |  es-en   Bio |  en-de   News |  en-de   TED |  en-de   IT ||\n|| Oracle model | 35.9 | 37.8 | 37.8 | 27.0 | 57.0 ||\n|| Uniform | 36.0 | 36.4 |  38.9 | 26.0 | 43.5 ||\n|| BI + IS |  36.2 |  38.0 | 38.7 |  26.1 |  56.4 ||\n\nQuestion: Is it true that  EWC models do not perform as well as uniform ensembling, as evidenced by the fact that in some cases, uniform ensembling outperforms the oracle?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ac84d56e-4583-4f9c-ae2a-0731f62551ba", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\nTable:\n||  Method |  R-1 |  R-2 |  R-SU ||\n|| First-1 | 26.83 | 7.25 | 6.46 ||\n|| First-2 | 35.99 | 10.17 | 12.06 ||\n|| First-3 | 39.41 | 11.77 | 14.51 ||\n|| LexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20 ||\n|| TextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50 ||\n|| MMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91 ||\n|| PG-Original Lebanoff et al. ( 2018 ) | 41.85 | 12.91 | 16.46 ||\n|| PG-MMR Lebanoff et al. ( 2018 ) | 40.55 | 12.36 | 15.87 ||\n|| PG-BRNN Gehrmann et al. ( 2018 ) | 42.80 | 14.19 | 16.75 ||\n|| CopyTransformer Gehrmann et al. ( 2018 ) |  43.57 | 14.03 | 17.37 ||\n|| Hi-MAP (Our Model) | 43.47 |  14.89 |  17.41 ||\n\nQuestion: Is it true that Our model outperforms PG-MMR when trained and tested on the Multi-News dataset?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "bb3c6b73-b0df-4c55-a45f-44744409c0cf", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Zero-Shot Grounding of Objects from Natural Language Queries Table 3: Category-wise performance with the default split of Flickr30k Entities.\nTable:\n|| Method | Overall | people | clothing | bodyparts | animals | vehicles | instruments | scene | other ||\n|| QRC - VGG(det) | 60.21 | 75.08 | 55.9 | 20.27 | 73.36 | 68.95 | 45.68 | 65.27 | 38.8 ||\n|| CITE - VGG(det) | 61.89 |  75.95 | 58.50 | 30.78 |  77.03 |  79.25 | 48.15 | 58.78 | 43.24 ||\n|| ZSGNet - VGG (cls) | 60.12 | 72.52 | 60.57 | 38.51 | 63.61 | 64.47 | 49.59 | 64.66 | 41.09 ||\n|| ZSGNet - Res50 (cls) |  63.39 | 73.87 |  66.18 |  45.27 | 73.79 | 71.38 |  58.54 |  66.49 |  45.53 ||\n\nQuestion: Is it true that However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet shows much better performance?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "6eaf2d50-5277-4860-8a5f-c43beb58c9e3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| Encoder Modules |  |  ||\n|| -Linear Combination | 23.7 | 53.2 ||\n|| -Global Node | 24.2 | 54.6 ||\n|| -Direction Aggregation | 24.6 | 54.6 ||\n|| -Graph Attention | 24.9 | 54.7 ||\n|| -Global Node&Linear Combination | 22.9 | 52.4 ||\n|| Decoder Modules |  |  ||\n|| -Coverage Mechanism | 23.8 | 53.0 ||\n\nQuestion: Is it true that The coverage mechanism is not effective in our models?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "54e7aefb-6033-47d9-9435-ec4661f93470", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Low-supervision urgency detection and transfer in short crisis messages TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal\nTable:\n|| System | Accuracy | Precision | Recall | F-Measure ||\n|| Local | 63.97% | 64.27% | 64.50% | 63.93% ||\n|| Manual | 64.25% |  70.84%∗∗ | 48.50% | 57.11% ||\n|| Wiki | 67.25% | 66.51% | 69.50% | 67.76% ||\n|| Local-Manual | 65.75% | 67.96% | 59.50% | 62.96% ||\n|| Wiki-Local | 67.40% | 65.54% | 68.50% | 66.80% ||\n|| Wiki-Manual | 67.75% | 70.38% | 63.00% | 65.79% ||\n||  Our Approach |  69.25%∗∗∗ | 68.76% |  70.50%∗∗ |  69.44%∗∗∗ ||\n\nQuestion: Is it true that Similarly, manual features reduce recall, but help the system to improve accuracy and precision (sometimes considerably)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "549bac5e-c1c3-4601-9908-b900f7c78abd", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nTable:\n|| Model | Diversity | App | Good% | OK% | Invalid% ||\n|| DAMD | 3.12 | 2.50 | 56.5% |  37.4% | 6.1% ||\n|| DAMD (+) |  3.65 |  2.53 |  63.0% | 27.1% | 9.9% ||\n|| HDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% |  10.0% ||\n\nQuestion: Is it true that  However, the slightly increased invalid response percentage  We also observe our DAMD model outperforms HDSA in both diversity and appropriateness scores?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f248d065-435c-4785-bd05-398870db94b1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\nTable:\n||  ResNet-34 |  Eval set % |  #param ||\n|| Baseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M ||\n|| SA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M ||\n|| SA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M ||\n||  SA (S: 1,2,3 - B: 3) |  55.27 | } 0.107M ||\n\nQuestion: Is it true that  We notice small improvements relative to the baseline showing that self-attention alone does improve the VQA task?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "af376ecf-fa75-4847-bbcc-3b39da24aa06", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.\nTable:\n|| Method | STS12 | STS13 | STS14 | STS15 | STS16 ||\n|| CBOW | 43.5 |  50.0 |  57.7 |  63.2 | 61.0 ||\n|| CMOW | 39.2 | 31.9 | 38.7 | 49.7 | 52.2 ||\n|| Hybrid |  49.6 | 46.0 | 55.1 | 62.4 |  62.1 ||\n|| cmp. CBOW | +14.6% | -8% | -4.5% | -1.5% | +1.8% ||\n|| cmp. CMOW | +26.5% | +44.2% | +42.4 | +25.6% | +19.0% ||\n\nQuestion: Is it true that The hybrid model is not able to repair this deficit, increasing the difference to 8%?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "f21cc3b7-2533-4f2f-a259-ac3cd13583d2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.\nTable:\n|| Train | Test |  System |  BLEU |  NIST |  METEOR |  ROUGE-L |  CIDEr |  Add |  Miss |  Wrong |  SER ||\n|| Original |  Original | TGen− | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94 ||\n|| Original |  Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27 ||\n|| Original |  Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80 ||\n|| Original |  Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen− | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37 ||\n|| Cleaned missing |  Original | TGen− | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61 ||\n|| Cleaned missing |  Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53 ||\n|| Cleaned missing |  Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen− | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45 ||\n\nQuestion: Is it true that However, the main improvement of SER comes from training on cleaned data with up to 94% error reduction without the ranker and 97% with.11 just cleaning the training data has a much less dramatic effect than using a semantic control mechanism, such as the reranker (4.27% vs. 0.97% SER)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "3a75d020-da89-4447-ab2b-ae91ec897986", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\nTable:\n||  ResNet-34 |  Eval set % |  #param ||\n|| SA (S: 3 - M: 1) | 55.25 | } 0.082M ||\n||  SA (S: 3 - B: 3) |  55.42 | } 0.082M ||\n|| SA (S: 3 - B: 4) | 55.33 | } 0.082M ||\n|| SA (S: 3 - B: 6) | 55.31 | } 0.082M ||\n|| SA (S: 3 - B: 1,3,5) | 55.45 | } 0.245M ||\n||  SA (S: 3 - B: 2,4,6) |  55.56 | } 0.245M ||\n\nQuestion: Is it true that Though the improvement is slim, it is encouraging to continue researching into visual modulation?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "435103bb-73be-4283-91b1-b429a3b988b7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\nTable:\n|| # of Heads | Accuracy | Val. Loss | Effect ||\n|| 1 | 89.44% | 0.2811 | -6.84% ||\n|| 2 | 91.20% | 0.2692 | -5.08% ||\n|| 4 | 93.85% | 0.2481 | -2.43% ||\n|| 8 | 96.02% | 0.2257 | -0.26% ||\n|| 10 | 96.28% | 0.2197 |  ||\n|| 16 | 96.32% | 0.2190 | +0.04 ||\n\nQuestion: Is it true that Using only one attention head, thereby attending to only one context position at once, degrades the performance to less than the performance of 10 heads using the standard finetuning scheme?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3de653b7-7800-41e0-8431-4f7ea3574f5d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326 ||\n|| P | EN | Ted Talks |  0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162 ||\n|| P | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 |  0.8052 | 0.4058 ||\n||  | PT | Ted Talks |  0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698 ||\n|| R | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 |  0.6490 | 0.0017 | 0.0003 ||\n|| R | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 |  0.6467 | 0.6332 | 0.0967 | 0.0003 ||\n|| R | PT | Europarl | 0.0002 | 0.1562 | 0.5157 |  0.7255 | 0.5932 | 0.0032 | 0.0001 ||\n||  | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 |  0.7000 | 0.5887 | 0.1390 | 0.0002 ||\n|| F | EN | Europarl | 0.0073 | 0.0162 | 0.0268 |  0.0293 |  0.0293 | 0.0033 | 0.0006 ||\n|| F | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 |  0.0520 | 0.0005 ||\n|| F | PT | Europarl | 0.0005 | 0.1733 | 0.4412 |  0.6240 | 0.5109 | 0.0064 | 0.0002 ||\n||  | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 |  0.6040 | 0.5149 | 0.2261 | 0.0004 ||\n\nQuestion: Is it true that When increasing the number of terms to 10,000, the DocSub models using Europarl corpora performed better than when using TED Talks corpora?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7e8e2c4f-92ea-4c80-a27c-84cae2005a80", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: On the difficulty of a distributional semantics of spoken language Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.\nTable:\n||  | Recall@10 (%) | Median rank | RSAimage ||\n|| VGS | 27 | 6 | 0.4 ||\n|| SegMatch |  10 |  37 |  0.5 ||\n|| Audio2vec-U | 5 | 105 | 0.0 ||\n|| Audio2vec-C | 2 | 647 | 0.0 ||\n|| Mean MFCC | 1 | 1,414 | 0.0 ||\n|| Chance | 0 | 3,955 | 0.0 ||\n\nQuestion: Is it true that SegMatch works slightly better than Audio2vec according to both criteria?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "6ff4558a-e94b-4bf7-88fc-ab7a7f88c1fa", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\nTable:\n||  Method |  R-1 |  R-2 |  R-SU ||\n|| First-1 | 26.83 | 7.25 | 6.46 ||\n|| First-2 | 35.99 | 10.17 | 12.06 ||\n|| First-3 | 39.41 | 11.77 | 14.51 ||\n|| LexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20 ||\n|| TextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50 ||\n|| MMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91 ||\n|| PG-Original Lebanoff et al. ( 2018 ) | 41.85 | 12.91 | 16.46 ||\n|| PG-MMR Lebanoff et al. ( 2018 ) | 40.55 | 12.36 | 15.87 ||\n|| PG-BRNN Gehrmann et al. ( 2018 ) | 42.80 | 14.19 | 16.75 ||\n|| CopyTransformer Gehrmann et al. ( 2018 ) |  43.57 | 14.03 | 17.37 ||\n|| Hi-MAP (Our Model) | 43.47 |  14.89 |  17.41 ||\n\nQuestion: Is it true that Our model does not outperform PG-MMR when trained and tested on the Multi-News dataset?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a3e27840-583c-4bc7-a60e-1acc2790dea2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Domain Adaptive Inference for Neural Machine Translation Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.\nTable:\n||  Decoder configuration |  es-en   Health |  es-en   Bio |  en-de   News |  en-de   TED |  en-de   IT ||\n|| Oracle model | 35.9 | 36.1 | 37.8 | 24.1 | 39.6 ||\n|| Uniform | 33.1 | 36.4 | 21.9 | 18.4 | 38.9 ||\n|| Identity-BI | 35.0 | 36.6 | 32.7 | 25.3 | 42.6 ||\n|| BI | 35.9 | 36.5 | 38.0 | 26.1 |  44.7 ||\n|| IS |  36.0 | 36.8 | 37.5 | 25.6 | 43.3 ||\n|| BI + IS |  36.0 |  36.9 |  38.4 |  26.4 |  44.7 ||\n\nQuestion: Is it true that Table 5 shows that uniform ensembling outperforms all oracle models except es-en Bio, especially on general domains?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "5879912f-a133-4644-b984-422b306d3d34", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 7: Comparisons of different DCGCN models under almost the same parameter budget.\nTable:\n||  Model | D | #P | B | C ||\n|| DCGCN(1) | 300 | 10.9M | 20.9 | 52.0 ||\n|| DCGCN(2) | 180 | 10.9M |  22.2 |  52.3 ||\n|| DCGCN(2) | 240 | 11.3M | 22.8 | 52.8 ||\n|| DCGCN(4) | 180 | 11.4M |  23.4 |  53.4 ||\n|| DCGCN(1) | 420 | 12.6M | 22.2 | 52.4 ||\n|| DCGCN(2) | 300 | 12.5M | 23.8 | 53.8 ||\n|| DCGCN(3) | 240 | 12.3M |  23.9 |  54.1 ||\n|| DCGCN(2) | 360 | 14.0M | 24.2 |  54.4 ||\n|| DCGCN(3) | 300 | 14.0M |  24.4 | 54.2 ||\n|| DCGCN(2) | 420 | 15.6M | 24.1 | 53.7 ||\n|| DCGCN(4) | 300 | 15.6M |  24.6 |  54.8 ||\n|| DCGCN(3) | 420 | 18.6M | 24.5 | 54.6 ||\n|| DCGCN(4) | 360 | 18.4M |  25.5 |  55.4 ||\n\nQuestion: Is it true that In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "08c3b235-a0d2-41d2-a7f1-046b2f03f269", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\nTable:\n||  | Acc | Sim | PP | GM ||\n|| M0: shen-1 | 0.694 | 0.728 |  22.3 | 8.81 ||\n|| M1: M0  +para | 0.702 | 0.747 | 23.6 | 11.7 ||\n|| M2: M0  +cyc | 0.692 | 0.781 | 49.9 |  12.8 ||\n|| M3: M0  +cyc+lang | 0.698 | 0.754 | 39.2 | 12.0 ||\n|| M4: M0  +cyc+para | 0.702 | 0.757 | 33.9 |  12.8 ||\n|| M5: M0  +cyc+para+lang | 0.688 | 0.753 | 28.6 | 11.8 ||\n|| M6: M0  +cyc+2d | 0.704 |  0.794 | 63.2 |  12.8 ||\n|| M7: M6+  para+lang | 0.706 | 0.768 | 49.0 |  12.8 ||\n\nQuestion: Is it true that  When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "2861b4ef-ab1a-4f84-b5ff-37cfcd01f477", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that Despite the models having fewer examples of bigger graphs to learn from, this does not lead to worse performance when handling graphs with higher diameters?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "952269e3-91ba-422b-9157-7a84243d785f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664 ||\n|| P | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 |  0.7311 | 0.5676 ||\n||  | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 |  0.6533 | 0.5656 ||\n|| R | EN | Europarl | 0.0396 | 0.3999 | 0.5499 |  0.6045 | 0.5887 | 0.0023 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 |  0.6077 | 0.2666 | 0.0019 ||\n|| R | PT | Europarl | 0.0111 | 0.3554 | 0.5795 |  0.6727 | 0.5184 | 0.0053 | 0.0012 ||\n||  | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 |  0.6877 | 0.5515 | 0.4706 | 0.0011 ||\n|| F | EN | Europarl | 0.0591 | 0.0671 | 0.0922 |  0.1015 | 0.1003 | 0.0044 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 |  0.1121 | 0.0037 ||\n|| F | PT | Europarl | 0.0217 | 0.3438 | 0.5513 |  0.6403 | 0.5555 | 0.0105 | 0.0024 ||\n||  | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 |  0.6475 | 0.5819 | 0.5471 | 0.0022 ||\n\nQuestion: Is it true that Analyzing Table 3, we can observe that all values of precision using the English corpora have higher scores when compared with the Portuguese corpora?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "b9d6270e-87be-44f9-982a-a037c7cfd0bd", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\nTable:\n||  Representation |  Hyper parameters Filter size |  Hyper parameters Num. Feature maps |  Hyper parameters Activation func. |  Hyper parameters L2 Reg. |  Hyper parameters Learning rate |  Hyper parameters Dropout Prob. |  F1.(avg. in 5-fold) with default values |  F1.(avg. in 5-fold) with optimal values ||\n|| CoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49 ||\n|| SB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 |  75.05 ||\n|| UD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57 ||\n\nQuestion: Is it true that The results furthermore show that the sdps based on the Stanford Basic (SB) representation do not provide the best performance, followed by the CoNLL08 representation?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "48134843-e7f0-4987-98fd-e830f58e1b3a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\nTable:\n||  Emoji alias |  N |  emoji # |  emoji % |  no-emoji # |  no-emoji % |  Δ% ||\n|| mask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27 ||\n|| two_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59 ||\n|| heart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91 ||\n|| heart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75 ||\n|| rage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04 ||\n|| cry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07 ||\n|| sob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67 ||\n|| unamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00 ||\n|| weary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49 ||\n|| joy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05 ||\n|| sweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80 ||\n|| confused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60 ||\n\nQuestion: Is it true that  When removing sweat smile and confused accuracy increased,?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "84aa63b6-4dad-4cc3-885a-d5d43259dae9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1\nTable:\n|| Dataset | Class | ˆ  piblack | ˆ  piwhite |  t |  p | ˆ  piblackˆ  piwhite ||\n||  Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505 ||\n||  | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724 ||\n||  Waseem | Racism | 0.001 | 0.001 | 0.035 |  | 1.001 ||\n||  | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993 ||\n||  | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120 ||\n||  Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573 ||\n||  | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653 ||\n||  Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396 ||\n||  Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812 ||\n||  | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239 ||\n||  | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854 ||\n\nQuestion: Is it true that For Waseem (2016) we see that there is a significant difference in the estimated rates at which tweets are classified as racist across groups, with higher rates for the white group?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "7b320190-e9bb-4603-8a20-b4a8f2c7fdbc", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\nTable:\n||  Model |  External | B ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | - | 22.0 ||\n|| GraphLSTM (Song et al.,  2018 ) | - | 23.3 ||\n|| GCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4 ||\n|| DCGCN(single) | - | 25.9 ||\n|| DCGCN(ensemble) | - |  28.2 ||\n|| TSP (Song et al.,  2016 ) | ALL | 22.4 ||\n|| PBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9 ||\n|| Tree2Str (Flanigan et al.,  2016 ) | ALL | 23.0 ||\n|| SNRG (Song et al.,  2017 ) | ALL | 25.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4 ||\n|| GraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2 ||\n|| DCGCN(single) | 0.1M | 29.0 ||\n|| DCGCN(single) | 0.2M |  31.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3 ||\n|| GraphLSTM (Song et al.,  2018 ) | 2M | 33.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8 ||\n|| DCGCN(single) | 0.3M | 33.2 ||\n|| DCGCN(ensemble) | 0.3M |  35.3 ||\n\nQuestion: Is it true that DCGCN model is not able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a higher score of 33.6 by using 2M data and Seq2SeqK achieves an even higher score of 33.8 by using 20M data?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "423aa89a-9bca-4bf2-a8aa-78154555b63b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nTable:\n|| Cue | App. | Prod. | Cov. ||\n|| in | 47 | 55.3 | 9.40 ||\n|| was | 55 | 61.8 | 11.0 ||\n|| to | 82 | 40.2 | 16.4 ||\n|| the | 85 | 38.8 | 17.0 ||\n|| a | 106 | 57.5 | 21.2 ||\n\nQuestion: Is it true that For example, a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21.2% of COPA training instances?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "deefe413-93b4-46a5-915c-e5ff31c48ab8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Keyphrase Generation for Scientific Articles using GANs Table 2: α-nDCG@5 metrics\nTable:\n|| Model | Inspec | Krapivin | NUS | KP20k ||\n|| Catseq | 0.87803 | 0.781 | 0.82118 | 0.804 ||\n|| Catseq-RL | 0.8602 |  0.786 | 0.83 | 0.809 ||\n|| GAN |  0.891 | 0.771 |  0.853 |  0.85 ||\n\nQuestion: Is it true that Our model does not obtain the best performance on three out of the four datasets?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e0687e8f-e6c6-43ea-854d-2257d5ec9015", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 1: The scores of our three submitted runs for similarity threshold 50%.\nTable:\n|| Run ID | Official score | Score with correction ||\n|| ep_1 | 60.29 | 66.76 ||\n|| ep_2 |  60.90 |  67.35 ||\n|| ep_3 | 60.61 | 67.07 ||\n\nQuestion: Is it true that The system's official score was 60.9% (micro-F1)  af  Therefore, we report both the official score (from our second submission) and the result of re-scoring our second submission after replacing these 10 files with the ones from our first submission?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "22787daa-80c1-4204-aced-20a547c65df4", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nTable:\n|| Type | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num ||\n|| Full | 8.413 | 903 | 10.59 | 450 | 11.18 | 865 ||\n|| Other | -99.95 | 76 | -48.15 | 99 | -71.62 | 135 ||\n\nQuestion: Is it true that  It can be observed that the learned reward function has good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ac541d7d-74e8-4def-b326-4c2e2c8fc6bd", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.\nTable:\n||  Model |  Type |  English-German #P |  English-German B |  English-German C |  English-Czech #P |  English-Czech B |  English-Czech C ||\n|| BoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | - ||\n|| CNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | - ||\n|| BiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | - ||\n|| PB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3 ||\n|| DCGCN (ours) | Single |   29.7M |  19.0 |  44.1 |   28.3M |  12.1 |  37.1 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9 ||\n|| DCGCN (ours) | Ensemble |   149M |  20.5 |  45.8 |   142M |  13.1 |  37.8 ||\n\nQuestion: Is it true that Our single model DCGCN(single) achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively, significantly outperforming all the single models?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "bab35b27-fc3c-4a34-802c-79f633a9de4f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Building a Production Model for Retrieval-Based Chatbots Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.\nTable:\n||  Model |  Parameters |  Validation AUC@0.05 |  Test AUC@0.05 ||\n|| Base | 8.0M |  0.871 | 0.816 ||\n|| 4L SRU → 2L LSTM | 7.3M | 0.864 |  0.829 ||\n|| 4L SRU → 2L SRU | 7.8M | 0.856 |  0.829 ||\n|| Flat → hierarchical | 12.4M | 0.825 | 0.559 ||\n|| Cross entropy → hinge loss | 8.0M | 0.765 | 0.693 ||\n|| 6.6M → 1M examples | 8.0M | 0.835 | 0.694 ||\n|| 6.6M → 100K examples | 8.0M | 0.565 | 0.417 ||\n|| 200 → 100 negatives | 8.0M | 0.864 | 0.647 ||\n|| 200 → 10 negatives | 8.0M | 0.720 | 0.412 ||\n\nQuestion: Is it true that The model performs significantly worse when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "582f8cd2-36bc-478b-a34b-95e07733d714", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.\nTable:\n|| Source | Target | Svm | Ra-Svm‡ | Ra-Cnn‡ | Trans† | Ra-Trans‡† | Ours‡† | Oracle† ||\n|| Beer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 |  79.53 | 80.29 ||\n|| Beer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 |  77.94 | 78.11 ||\n|| Beer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 |  75.24 | 75.50 ||\n\nQuestion: Is it true that We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) outperforms ORACLE in all aspects?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "7a24d9e2-8e4f-4318-83fc-7fdad95942a0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\nTable:\n||  Model |  External | B ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | - | 22.0 ||\n|| GraphLSTM (Song et al.,  2018 ) | - | 23.3 ||\n|| GCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4 ||\n|| DCGCN(single) | - | 25.9 ||\n|| DCGCN(ensemble) | - |  28.2 ||\n|| TSP (Song et al.,  2016 ) | ALL | 22.4 ||\n|| PBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9 ||\n|| Tree2Str (Flanigan et al.,  2016 ) | ALL | 23.0 ||\n|| SNRG (Song et al.,  2017 ) | ALL | 25.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4 ||\n|| GraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2 ||\n|| DCGCN(single) | 0.1M | 29.0 ||\n|| DCGCN(single) | 0.2M |  31.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3 ||\n|| GraphLSTM (Song et al.,  2018 ) | 2M | 33.6 ||\n|| Seq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8 ||\n|| DCGCN(single) | 0.3M | 33.2 ||\n|| DCGCN(ensemble) | 0.3M |  35.3 ||\n\nQuestion: Is it true that When using the same amount of 0.2M data, the performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ba7d16da-1cc2-4350-aac4-6bc6e31ada78", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\nTable:\n||  Model | R | MUC P |  F1 | R | B3 P |  F1 | R | CEAF-  e P |  F1 | CoNLL   F1 ||\n||  Baselines |  |  |  |  |  |  |  |  |  |  ||\n|| Cluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5 ||\n|| CV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73 ||\n|| KCP Kenyon-Dean et al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69 ||\n|| Cluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6 ||\n||  Model Variants |  |  |  |  |  |  |  |  |  |  ||\n|| Disjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5 ||\n|| Joint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 |  79.5 ||\n\nQuestion: Is it true that The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "08e785fd-5276-4bfa-89cb-743853a254f3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nTable:\n|| Model | #Params | BLEU | Train | Decode ||\n|| GNMT | - | 24.61 | - | - ||\n|| GRU | 206M | 26.28 | 2.67 | 45.35 ||\n|| ATR | 122M | 25.70 | 1.33 |  34.40 ||\n|| SRU | 170M | 25.91 | 1.34 | 42.84 ||\n|| LRN | 143M | 26.26 |  0.99 | 36.50 ||\n|| oLRN | 164M |  26.73 | 1.15 | 40.19 ||\n\nQuestion: Is it true that The results in Table 3 show that translation quality of LRN is significantly worse than that of GRU (-0.57 BLEU)?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "966334f3-986a-4d6b-8dcd-efda97f994b6", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\nTable:\n||  | MSCOCO spice | MSCOCO cider | MSCOCO rouge  L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep↓ | Flickr30k spice | Flickr30k cider | Flickr30k rouge  L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep↓ ||\n|| softmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09 ||\n|| sparsemax |  18.9 |  0.990 |  53.5 |  31.5 |  25.3 | 3.69 |  13.7 |  0.444 |  44.3 |  20.7 |  19.3 | 5.84 ||\n|| TVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 |  3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 |  3.97 ||\n\nQuestion: Is it true that  Selective attention mechanisms like sparsemax and especially TVMAX reduce repetition, as measured by the REP metric reported in Table 1?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3a961802-e664-47df-a85d-d017f7b3250f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\nTable:\n|| <bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold> ||\n|| LDC2015E86 | LDC2015E86 | LDC2015E86 ||\n|| Konstas et al. (2017) | 22.00 | - ||\n|| Song et al. (2018) | 23.28 | 30.10 ||\n|| Cao et al. (2019) | 23.50 | - ||\n|| Damonte et al.(2019) | 24.40 | 23.60 ||\n|| Guo et al. (2019) | <bold>25.70</bold> | - ||\n|| S2S | 22.55 ± 0.17 | 29.90 ± 0.31 ||\n|| G2S-GIN | 22.93 ± 0.20 | 29.72 ± 0.09 ||\n|| G2S-GAT | 23.42 ± 0.16 | 29.87 ± 0.14 ||\n|| G2S-GGNN | 24.32 ± 0.16 | <bold>30.53</bold> ± 0.30 ||\n|| LDC2017T10 | LDC2017T10 | LDC2017T10 ||\n|| Back et al. (2018) | 23.30 | - ||\n|| Song et al. (2018) | 24.86 | 31.56 ||\n|| Damonte et al.(2019) | 24.54 | 24.07 ||\n|| Cao et al. (2019) | 26.80 | - ||\n|| Guo et al. (2019) | 27.60 | - ||\n|| S2S | 22.73 ± 0.18 | 30.15 ± 0.14 ||\n|| G2S-GIN | 26.90 ± 0.19 | 32.62 ± 0.04 ||\n|| G2S-GAT | 26.72 ± 0.20 | 32.52 ± 0.02 ||\n|| G2S-GGNN | <bold>27.87</bold> ± 0.15 | <bold>33.21</bold> ± 0.15 ||\n\nQuestion: Is it true that On the same dataset, we have competitive results to Damonte and Cohen (2019)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0caf7d9c-4732-4d3f-9f66-d8b7b5105251", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\nTable:\n||  Complaints   Feature |  Complaints   r |  Not Complaints   Feature |  Not Complaints   r ||\n||  Unigrams |  Unigrams |  Unigrams |  Unigrams ||\n|| not | .154 | [URL] | .150 ||\n|| my | .131 | ! | .082 ||\n|| working | .124 | he | .069 ||\n|| still | .123 | thank | .067 ||\n|| on | .119 | , | .064 ||\n|| can’t | .113 | love | .064 ||\n|| service | .112 | lol | .061 ||\n|| customer | .109 | you | .060 ||\n|| why | .108 | great | .058 ||\n|| website | .107 | win | .058 ||\n|| no | .104 | ’ | .058 ||\n|| ? | .098 | she | .054 ||\n|| fix | .093 | : | .053 ||\n|| won’t | .092 | that | .053 ||\n|| been | .090 | more | .052 ||\n|| issue | .089 | it | .052 ||\n|| days | .088 | would | .051 ||\n|| error | .087 | him | .047 ||\n|| is | .084 | life | .046 ||\n|| charged | .083 | good | .046 ||\n||  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) ||\n|| VBN | .141 | UH | .104 ||\n|| $ | .118 | NNP | .098 ||\n|| VBZ | .114 | PRP | .076 ||\n|| NN_VBZ | .114 | HT | .076 ||\n|| PRP$ | .107 | PRP_. | .076 ||\n|| PRP$_NN | .105 | PRP_RB | .067 ||\n|| VBG | .093 | NNP_NNP | .062 ||\n|| CD | .092 | VBP_PRP | .054 ||\n|| WRB_VBZ | .084 | JJ | .053 ||\n|| VBZ_VBN | .084 | DT_JJ | .051 ||\n\nQuestion: Is it true that  In addition, other words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are also distinctive for tweets that are not complaints?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "851a3937-519e-4d91-8e18-bb809245164e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nTable:\n|| Dataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb ||\n||  Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 |  76.41 ||\n||  WinoCoref | AntePre | 68.37 | 74.32 | —– | 88.48 | 88.95 |  89.32 ||\n\nQuestion: Is it true that On the WinoCoref dataset, it improves by 15%?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "4e3c4acc-419f-40ad-99d3-b60a432f2e43", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nTable:\n|| Cue | App. | Prod. | Cov. ||\n|| in | 47 | 55.3 | 9.40 ||\n|| was | 55 | 61.8 | 11.0 ||\n|| to | 82 | 40.2 | 16.4 ||\n|| the | 85 | 38.8 | 17.0 ||\n|| a | 106 | 57.5 | 21.2 ||\n\nQuestion: Is it true that Its productivity of 57.5% expresses that it appears in incorrect alternatives 7.5% more often than expected by random chance?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "43802ed8-5f82-4f5a-b44a-fa832e72a6c7", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 6: Results on SimLex-999 and WordSim-353, in Italian and German, before and after debiasing.\nTable:\n||  | Italian Orig | Italian Debias | German Orig | German Debias ||\n|| SimLex | 0.280 |  0.288 | 0.343 |  0.356 ||\n|| WordSim | 0.548 |  0.577 | 0.547 |  0.553 ||\n\nQuestion: Is it true that In both cases, the original embeddings perform better than the new ones?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "1515785d-efa6-40d2-af51-95a8c13ee95c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Low-supervision urgency detection and transfer in short crisis messages TABLE II: Details on datasets used for experiments.\nTable:\n|| Dataset | Unlabeled / Labeled Messages | Urgent / Non-urgent Messages | Unique Tokens | Avg. Tokens / Message | Time Range ||\n|| Nepal | 6,063/400 | 201/199 | 1,641 | 14 | 04/05/2015-05/06/2015 ||\n|| Macedonia | 0/205 | 92/113 | 129 | 18 | 09/18/2018-09/21/2018 ||\n|| Kerala | 92,046/400 | 125/275 | 19,393 | 15 | 08/17/2018-08/22/2018 ||\n\nQuestion: Is it true that Table II shows that Nepal and Macedonia are roughly balanced, while Kerala is imbalanced?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "862ee0dc-0f85-4a85-a1c3-b15e962ba324", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers Table 1: Impact of linguistic features on deep-coref models on the CoNLL development set.\nTable:\n||  | MUC | <italic>B</italic>3 | CEAF<italic>e</italic> | CoNLL | LEA ||\n|| ranking | 74.31 | 64.23 | 59.73 | 66.09 | 60.47 ||\n|| +linguistic | 74.35 | 63.96 | 60.19 | 66.17 | 60.20 ||\n|| top-pairs | 73.95 | 63.98 | 59.52 | 65.82 | 60.07 ||\n|| +linguistic | 74.32 | 64.45 | 60.19 | 66.32 | 60.62 ||\n\nQuestion: Is it true that  However, it does not improve significantly over \"ranking\"?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "fd89ce86-c88f-4273-882a-21c474839874", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.\nTable:\n||  | M | F | B | O ||\n|| Random | 47.5 | 50.5 |  1.06 | 49.0 ||\n|| Token Distance | 50.6 | 47.5 |  0.94 | 49.1 ||\n|| Topical Entity | 50.2 | 47.3 |  0.94 | 48.8 ||\n|| Syntactic Distance | 66.7 | 66.7 |    1.00 | 66.7 ||\n|| Parallelism |  69.3 |  69.2 |    1.00 |  69.2 ||\n|| Parallelism+URL |  74.2 |  71.6 |    0.96 |  72.9 ||\n|| Transformer-Single | 59.6 | 56.6 |  0.95 | 58.1 ||\n|| Transformer-Multi | 62.9 | 61.7 |  0.98 | 62.3 ||\n\nQuestion: Is it true that RANDOM is the best performing baseline here, and other baselines are far from gender-parity?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "20733675-9d80-4d43-9f7d-92d3d2a434bf", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\nTable:\n||  Model | R | MUC P |  F1 | R | B3 P |  F1 | R | CEAF-  e P |  F1 | CoNLL   F1 ||\n||  Baselines |  |  |  |  |  |  |  |  |  |  ||\n|| Cluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5 ||\n|| CV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73 ||\n|| KCP Kenyon-Dean et al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69 ||\n|| Cluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6 ||\n||  Model Variants |  |  |  |  |  |  |  |  |  |  ||\n|| Disjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5 ||\n|| Joint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 |  79.5 ||\n\nQuestion: Is it true that  The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "5142bc85-da69-4450-bd65-28cd5ce2831e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.\nTable:\n||  | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction ||\n|| Same Gender | 0.442 | 0.434 | 0.424 | – | 0.491 | 0.478 | 0.446 | – ||\n|| Different Gender | 0.385 | 0.421 | 0.415 | – | 0.415 | 0.435 | 0.403 | – ||\n|| difference | 0.057 | 0.013 | 0.009 |  91.67% | 0.076 | 0.043 | 0.043 |  100% ||\n\nQuestion: Is it true that  As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much lower?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "9064b2de-b304-408b-9aa9-81c8f1be3e65", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: The MeMAD Submission to the WMT18 Multimodal Translation Task Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.\nTable:\n||  | en-fr | flickr16 | flickr17 | mscoco17 ||\n|| A | subs1M    H+MS-COCO | 66.3 | 60.5 | 52.1 ||\n|| A | +domain-tuned | 66.8 | 60.6 | 52.0 ||\n|| A | +labels |  67.2 | 60.4 | 51.7 ||\n|| T | subs1M    LM+MS-COCO | 66.9 | 60.3 |  52.8 ||\n|| T | +labels |  67.2 |  60.9 | 52.7 ||\n||  | en-de | flickr16 | flickr17 | mscoco17 ||\n|| A | subs1M    H+MS-COCO | 43.1 | 39.0 | 35.1 ||\n|| A | +domain-tuned | 43.9 | 39.4 | 35.8 ||\n|| A | +labels | 43.2 | 39.3 | 34.3 ||\n|| T | subs1M    LM+MS-COCO |  44.4 | 39.4 | 35.0 ||\n|| T | +labels | 44.1 |  39.8 |  36.5 ||\n\nQuestion: Is it true that For Marian amun, the effect of adding domain labels is significant as we can see in Table 3?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "d8b4c1e7-8cf5-4916-87af-b646b8ef4b6b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.\nTable:\n||  |  Present |  Not Present ||\n|| Emoji | 4805 (76.6%) | 23952 (68.0%) ||\n|| Hashtags | 2122 (70.5%) | 26635 (69.4%) ||\n\nQuestion: Is it true that Tweets containing emoji seem to be harder for the model to classify than those without?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "55e35248-14b5-4372-91ab-184449934829", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\nTable:\n||  Representation |  Hyper parameters Filter size |  Hyper parameters Num. Feature maps |  Hyper parameters Activation func. |  Hyper parameters L2 Reg. |  Hyper parameters Learning rate |  Hyper parameters Dropout Prob. |  F1.(avg. in 5-fold) with default values |  F1.(avg. in 5-fold) with optimal values ||\n|| CoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49 ||\n|| SB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 |  75.05 ||\n|| UD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57 ||\n\nQuestion: Is it true that We observe that the results for the UD representation are quite a bit lower than the two others?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "adadfacf-0744-4462-951f-d4678d921ee0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Domain Adaptive Inference for Neural Machine Translation Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.\nTable:\n||  Decoder configuration |  es-en   Health |  es-en   Bio |  en-de   News |  en-de   TED |  en-de   IT ||\n|| Oracle model | 35.9 | 37.8 | 37.8 | 27.0 | 57.0 ||\n|| Uniform | 36.0 | 36.4 |  38.9 | 26.0 | 43.5 ||\n|| BI + IS |  36.2 |  38.0 | 38.7 |  26.1 |  56.4 ||\n\nQuestion: Is it true that  EWC models perform well over multiple domains, so the improvement over uniform ensembling is less striking than for unadapted models?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "84408bed-7687-4049-9b6b-35bed42eda8f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= “b*tch”\nTable:\n|| Dataset | Class | ˆ  piblack | ˆ  piwhite |  t |  p | ˆ  piblackˆ  piwhite ||\n||  Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 |  | 0.978 ||\n||  | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020 ||\n||  Waseem | Racism | 0.011 | 0.011 | -1.254 |  | 0.955 ||\n||  | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203 ||\n||  | Racism and sexism | 0.012 | 0.012 | -0.162 |  | 0.995 ||\n||  Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152 ||\n||  | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997 ||\n||  Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091 ||\n||  Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728 ||\n||  | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956 ||\n||  | Spam | 0.010 | 0.010 | 0.000 |  | 1.000 ||\n\nQuestion: Is it true that In both cases the classifiers trained upon their data are still more likely to flag white-aligned tweets as sexism?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ac292ba0-cc9c-4235-8e92-4901c60e2903", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.\nTable:\n|| Source | Target | Svm | Ra-Svm‡ | Ra-Cnn‡ | Trans† | Ra-Trans‡† | Ours‡† | Oracle† ||\n|| Beer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 |  79.53 | 80.29 ||\n|| Beer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 |  77.94 | 78.11 ||\n|| Beer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 |  75.24 | 75.50 ||\n\nQuestion: Is it true that It closely matches the performance of ORACLE with only 0.40% absolute difference?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f4879cd6-63b6-4f55-9bca-036a3a0a0b90", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain low degree nodes?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "b5999ce8-28d9-47e9-a847-e7d72dcfde52", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nTable:\n|| Model | #Params | BLEU | Train | Decode ||\n|| GNMT | - | 24.61 | - | - ||\n|| GRU | 206M | 26.28 | 2.67 | 45.35 ||\n|| ATR | 122M | 25.70 | 1.33 |  34.40 ||\n|| SRU | 170M | 25.91 | 1.34 | 42.84 ||\n|| LRN | 143M | 26.26 |  0.99 | 36.50 ||\n|| oLRN | 164M |  26.73 | 1.15 | 40.19 ||\n\nQuestion: Is it true that Still, both LRN and oLRN translate sentences faster than SRU (+15%/+6%)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "d69416ad-80be-4be2-bc76-6806f2a74b90", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table 1, all rewards in this table do not require reference summaries.\nTable:\n|| Model | Encoder |  Reg. loss (Eq. ( 1 ))   ρ |  Reg. loss (Eq. ( 1 ))   r |  Reg. loss (Eq. ( 1 )) G-Pre |  Reg. loss (Eq. ( 1 )) G-Rec |  Pref. loss (Eq. ( 3 ))   ρ |  Pref. loss (Eq. ( 3 ))   r |  Pref. loss (Eq. ( 3 )) G-Pre |  Pref. loss (Eq. ( 3 )) G-Rec ||\n|| MLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524 ||\n|| MLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556 ||\n|| MLP | BERT |  .487 |  .526 |  .544 |  .597 |  .505 |  .531 |  .556 |  .608 ||\n|| SimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549 ||\n|| SimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551 ||\n|| SimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533 ||\n|| Peyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174 ||\n\nQuestion: Is it true that MLP with BERT as encoder does not have the best overall performance?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "9b0e1193-f48e-4334-b899-f5e92f4df3da", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 7: Cross-lingual embedding alignment in Italian and in German, before and after debiasing.\nTable:\n||  | Italian → En | Italian En → | German → En | German En → ||\n|| Orig | 58.73 | 59.68 | 47.58 | 50.48 ||\n|| Debias |  60.03 |  60.96 |  47.89 |  51.76 ||\n\nQuestion: Is it true that The results reported in Table 7 show that precision on BDI indeed increases as a result of the reduced effect of grammatical gender on the embeddings for German and Italian, i.e?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "6a0f3f3f-8f25-43cb-938d-3808d00199ac", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\nTable:\n||  | Prec. | Rec. | F1 ||\n|| (A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531 ||\n|| (A2) Standard attention | 0.466 | 0.638 | 0.539 ||\n|| (A3) Window size (  ws)=5 | 0.507 | 0.652 |  0.571 ||\n|| (A4) Window size (  ws)=10 | 0.510 | 0.640 | 0.568 ||\n|| (A5) Softmax | 0.490 | 0.658 | 0.562 ||\n|| (A6) Max-pool | 0.492 | 0.600 | 0.541 ||\n\nQuestion: Is it true that Replacing the attention normalizing function with softmax operation also reduces the F1 score marginally (A3−A5)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "6f84236f-e476-4ea2-9bba-f83a1b157df1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\nTable:\n||  Complaints   Feature |  Complaints   r |  Not Complaints   Feature |  Not Complaints   r ||\n||  Unigrams |  Unigrams |  Unigrams |  Unigrams ||\n|| not | .154 | [URL] | .150 ||\n|| my | .131 | ! | .082 ||\n|| working | .124 | he | .069 ||\n|| still | .123 | thank | .067 ||\n|| on | .119 | , | .064 ||\n|| can’t | .113 | love | .064 ||\n|| service | .112 | lol | .061 ||\n|| customer | .109 | you | .060 ||\n|| why | .108 | great | .058 ||\n|| website | .107 | win | .058 ||\n|| no | .104 | ’ | .058 ||\n|| ? | .098 | she | .054 ||\n|| fix | .093 | : | .053 ||\n|| won’t | .092 | that | .053 ||\n|| been | .090 | more | .052 ||\n|| issue | .089 | it | .052 ||\n|| days | .088 | would | .051 ||\n|| error | .087 | him | .047 ||\n|| is | .084 | life | .046 ||\n|| charged | .083 | good | .046 ||\n||  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) ||\n|| VBN | .141 | UH | .104 ||\n|| $ | .118 | NNP | .098 ||\n|| VBZ | .114 | PRP | .076 ||\n|| NN_VBZ | .114 | HT | .076 ||\n|| PRP$ | .107 | PRP_. | .076 ||\n|| PRP$_NN | .105 | PRP_RB | .067 ||\n|| VBG | .093 | NNP_NNP | .062 ||\n|| CD | .092 | VBP_PRP | .054 ||\n|| WRB_VBZ | .084 | JJ | .053 ||\n|| VBZ_VBN | .084 | DT_JJ | .051 ||\n\nQuestion: Is it true that  A distinctive part-of-speech pattern common in complaints is possessive pronouns followed by nouns (PRP$ NN) which refer to items of services possessed by the complainer (e.g., my account, my order)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f5a1f7ce-a335-4908-a0e1-baea0d34c863", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Distant Learning for Entity Linking with Automatic Noise Detection Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)\nTable:\n|| System | All LOC | All ORG | All PER | All MISC | In   E+ LOC | In   E+ ORG | In   E+ PER | In   E+ MISC ||\n|| Name matching | 96.26 | 89.48 | 57.38 | 96.60 | 92.32 | 76.87 | 47.40 | 76.29 ||\n|| MIL | 57.09 |  76.30 | 41.35 | 93.35 | 11.90 |  47.90 | 27.60 | 53.61 ||\n|| MIL-ND | 57.15 | 77.15 | 35.95 | 92.47 | 12.02 | 49.77 | 20.94 | 47.42 ||\n||  τMIL-ND |  55.15 | 76.56 |  34.03 |  92.15 |  11.14 | 51.18 |  20.59 |  40.00 ||\n|| Supervised learning | 55.58 | 61.32 | 24.98 | 89.96 | 8.80 | 14.95 | 7.40 | 29.90 ||\n\nQuestion: Is it true that  For LOC, it turns out that candidate selection is not a bottleneck: when candidate selection was flawless, the models made only about 55% errors, down from about 96%?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ebd1548f-ff05-41b1-917c-9a5e04da6635", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664 ||\n|| P | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 |  0.7311 | 0.5676 ||\n||  | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 |  0.6533 | 0.5656 ||\n|| R | EN | Europarl | 0.0396 | 0.3999 | 0.5499 |  0.6045 | 0.5887 | 0.0023 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 |  0.6077 | 0.2666 | 0.0019 ||\n|| R | PT | Europarl | 0.0111 | 0.3554 | 0.5795 |  0.6727 | 0.5184 | 0.0053 | 0.0012 ||\n||  | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 |  0.6877 | 0.5515 | 0.4706 | 0.0011 ||\n|| F | EN | Europarl | 0.0591 | 0.0671 | 0.0922 |  0.1015 | 0.1003 | 0.0044 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 |  0.1121 | 0.0037 ||\n|| F | PT | Europarl | 0.0217 | 0.3438 | 0.5513 |  0.6403 | 0.5555 | 0.0105 | 0.0024 ||\n||  | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 |  0.6475 | 0.5819 | 0.5471 | 0.0022 ||\n\nQuestion: Is it true that TF has the best values of recall and f-measure for all corpora but the English version of TED Talks which has in DF the best value of recall and in DocSub the best value of f-measure?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "5ecc4b82-ccbe-480e-af2d-c5e567617179", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Semantic Neural Machine Translation using AMR Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.\nTable:\n|| AMR Anno. | BLEU ||\n|| Automatic | 16.8 ||\n|| Gold |  *17.5* ||\n\nQuestion: Is it true that Table 4 shows the BLEU scores of our Dual2seq model taking gold or automatic AMRs as inputs?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ed36c8eb-dda1-48b8-8fa3-7a9456fdfd05", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Zero-Shot Grounding of Objects from Natural Language Queries Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600×600\nTable:\n|| Model | Accuracy on RefClef ||\n|| BM + Softmax | 48.54 ||\n|| BM + BCE | 55.20 ||\n|| BM + FL | 57.13 ||\n|| BM + FL + Img-Resize |  61.75 ||\n\nQuestion: Is it true that  However, the highest accuracy was achieved by using Binary Cross Entropy, with a score of 55.20?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "8dd76692-4ea1-4658-9ce7-d242e640238e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Sparse and Structured Visual Attention Table 2: Human evaluation results on MSCOCO.\nTable:\n||  | caption | attention relevance ||\n|| softmax | 3.50 | 3.38 ||\n|| sparsemax | 3.71 | 3.89 ||\n|| TVmax |  3.87 |  4.10 ||\n\nQuestion: Is it true that Despite performing slightly worse than sparsemax under automatic metrics, TVMAX does not outperform sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "7b4f6a72-1867-4e14-b9ff-5414a76d5834", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nTable:\n|| Cue | App. | Prod. | Cov. ||\n|| in | 47 | 55.3 | 9.40 ||\n|| was | 55 | 61.8 | 11.0 ||\n|| to | 82 | 40.2 | 16.4 ||\n|| the | 85 | 38.8 | 17.0 ||\n|| a | 106 | 57.5 | 21.2 ||\n\nQuestion: Is it true that For example, the is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 17.0% of COPA training instances?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "a62ed321-045e-4c34-9771-274b95b428c9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).\nTable:\n|| Train | Test |  System |  BLEU |  NIST |  METEOR |  ROUGE-L |  CIDEr |  Add |  Miss |  Wrong |  SER ||\n|| Original |  Cleaned | TGen− | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31 ||\n|| Original |  Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05 ||\n|| Original |  Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24 ||\n|| Original |  Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen− | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94 ||\n|| Cleaned missing |  Cleaned | TGen− | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52 ||\n|| Cleaned missing |  Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86 ||\n|| Cleaned missing |  Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen− | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88 ||\n\nQuestion: Is it true that The results for testing on cleaned data (Table 3, top half) do not confirm the positive impact of cleaned training data and also show that the cleaned test data is not more challenging (cf?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "388ba17a-36df-4217-ac48-845683103ee5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Language Independent Sequence Labelling for Opinion Target Extraction Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores.\nTable:\n|| Language | System | F1 ||\n|| es | GTI | 68.51 ||\n|| es | L +   CW600 + W2VW300 |  69.92 ||\n|| es | Baseline | 51.91 ||\n|| fr | IIT-T | 66.67 ||\n|| fr | L +   CW100 |  69.50 ||\n|| fr | Baseline | 45.45 ||\n|| nl | IIT-T | 56.99 ||\n|| nl | L +   W2VW400 |  66.39 ||\n|| nl | Baseline | 50.64 ||\n|| ru | Danii. | 33.47 ||\n|| ru | L +   CW500 |  65.53 ||\n|| ru | Baseline | 49.31 ||\n|| tr | L +   BW |  60.22 ||\n|| tr | Baseline | 41.86 ||\n\nQuestion: Is it true that Table 6 shows that our system outperforms the best previous approaches across the five languages?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "295db104-a00f-4932-b5c0-740e1efa09b9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 4: Lexicon member coverage (%)\nTable:\n|| target | VN | WN-V | WN-N ||\n|| type | 81 | 66 | 47 ||\n|| x+POS | 54 | 39 | 43 ||\n|| lemma | 88 | 76 | 53 ||\n|| x+POS | 79 | 63 | 50 ||\n|| shared | 54 | 39 | 41 ||\n\nQuestion: Is it true that POS-disambiguation, in turn, fragments the vocabulary and consistently reduces the coverage with the effect being less pronounced for lemmatized targets?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0773f240-5761-43fd-a7d3-d55af3879cfd", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 7: Comparisons of different DCGCN models under almost the same parameter budget.\nTable:\n||  Model | D | #P | B | C ||\n|| DCGCN(1) | 300 | 10.9M | 20.9 | 52.0 ||\n|| DCGCN(2) | 180 | 10.9M |  22.2 |  52.3 ||\n|| DCGCN(2) | 240 | 11.3M | 22.8 | 52.8 ||\n|| DCGCN(4) | 180 | 11.4M |  23.4 |  53.4 ||\n|| DCGCN(1) | 420 | 12.6M | 22.2 | 52.4 ||\n|| DCGCN(2) | 300 | 12.5M | 23.8 | 53.8 ||\n|| DCGCN(3) | 240 | 12.3M |  23.9 |  54.1 ||\n|| DCGCN(2) | 360 | 14.0M | 24.2 |  54.4 ||\n|| DCGCN(3) | 300 | 14.0M |  24.4 | 54.2 ||\n|| DCGCN(2) | 420 | 15.6M | 24.1 | 53.7 ||\n|| DCGCN(4) | 300 | 15.6M |  24.6 |  54.8 ||\n|| DCGCN(3) | 420 | 18.6M | 24.5 | 54.6 ||\n|| DCGCN(4) | 360 | 18.4M |  25.5 |  55.4 ||\n\nQuestion: Is it true that For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "f2f7734e-5b8b-4a24-bae8-acd7f6758765", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\nTable:\n|| <bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1 ||\n|| <bold>Baselines</bold> |  |  |  |  |  |  |  |  |  |  ||\n|| Cluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5 ||\n|| CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73 ||\n|| KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69 ||\n|| Cluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6 ||\n|| <bold>Model Variants</bold> |  |  |  |  |  |  |  |  |  |  ||\n|| Disjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5 ||\n|| Joint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold> ||\n\nQuestion: Is it true that Our joint model outperforms all the base  The results reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "62eed765-15d0-4c11-9d2b-d12a4be5f764", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\nTable:\n||  | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall ||\n|| softmax | ✓ |  | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97 ||\n|| sparsemax | ✓ |  | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94 ||\n|| soft-TVmax | ✓ |  | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11 ||\n|| sparse-TVmax | ✓ |  | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17 ||\n|| softmax |  | ✓ | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04 ||\n|| sparsemax |  | ✓ |  85.40 |  50.87 | 58.67 | 68.79 |  85.80 | 50.18 | 59.08 | 69.19 ||\n|| softmax | ✓ | ✓ | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17 ||\n|| sparse-TVmax | ✓ | ✓ | 85.35 | 50.52 |  59.15 |  68.96 | 85.72 |  50.66 |  59.22 |  69.28 ||\n\nQuestion: Is it true that Moreover, the model using TVMAX in the final attention layer achieves the highest accuracy, showing that features obtained using the TVMAX transformation are a better complement to bounding box features?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "ec6aee2f-4336-4c2a-8da2-10020466e7fc", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTable:\n|| Topic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI |  OD-w2v ARI |  OD-d2v ARI | TF-IDF   Sil. | WMD   Sil. | Sent2vec   Sil. | Doc2vec   Sil. | BERT   Sil. |  OD-w2v   Sil. |  OD-d2v   Sil. ||\n|| Affirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 |  0.14 |  0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 |  0.06 |  0.01 ||\n|| Atheism | 116 |  0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 |  0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 |  0.05 |  0.07 ||\n|| Austerity Measures | 20 |  0.04 |  0.04 | -0.01 | -0.05 | 0.04 |  0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 |  0.19 | 0.1 ||\n|| Democratization | 76 | 0.02 | -0.01 | 0.00 |  0.09 | -0.01 |  0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 |  0.16 |  0.11 ||\n|| Education Voucher Scheme | 30 |  0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 |  0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 |  0.38 |  0.40 ||\n|| Gambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 |  0.35 |  0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 |  0.30 |  0.22 ||\n|| Housing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 |  0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 |  0.13 |  0.13 ||\n|| Hydroelectric Dams | 110 |  0.47 |  0.45 |  0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 |  0.26 |  0.09 ||\n|| Intellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 |  0.05 |  0.14 | 0.01 |  0.04 | 0.03 | 0.01 | 0.03 |  0.04 |  0.12 ||\n|| Keystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 |  0.07 | -0.01 |  0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 |  0.05 |  0.02 ||\n|| Monarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 |  0.15 |  0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 |  0.11 |  0.09 ||\n|| National Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 |  0.31 |  0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 |  0.25 |  0.25 ||\n|| One-child policy China | 67 | -0.05 | 0.01 |  0.11 | -0.02 | 0.02 |  0.11 | 0.01 | 0.01 | 0.02 |  0.04 | -0.01 | 0.03 |  0.07 | -0.02 ||\n|| Open-source Software | 48 | -0.02 | -0.01 |  0.05 | 0.01 | 0.12 |  0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 |  0.18 | 0.01 ||\n|| Pornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 |  0.41 |  0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 |  0.47 |  0.41 ||\n|| Seanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 |  0.32 |  0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 |  0.15 |  0.31 ||\n|| Trades Unions | 19 |  0.44 |  0.44 |  0.60 | -0.05 | 0.44 |  0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 |  0.48 |  0.32 ||\n|| Video Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 |  0.40 |  0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 |  0.32 |  0.42 ||\n|| Average | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 |  0.22 |  0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 |  0.20 |  0.17 ||\n\nQuestion: Is it true that  In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly well compared to TF-IDF?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "c0b78cbb-c152-43d1-9c65-f0c6632ad296", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Predicting Discourse Structure using Distant Supervision from Sentiment Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined\nTable:\n|| Approach | RST-DTtest | Instr-DTtest ||\n|| Right Branching | 54.64 | 58.47 ||\n|| Left Branching | 53.73 | 48.15 ||\n|| Hier. Right Branch. |  70.82 |  67.86 ||\n|| Hier. Left Branch. | 70.58 | 63.49 ||\n||  Intra-Domain Evaluation |  Intra-Domain Evaluation |  Intra-Domain Evaluation ||\n|| HILDAHernault et al. ( 2010 ) | 83.00 | — ||\n|| DPLPJi and Eisenstein ( 2014 ) | 82.08 | — ||\n|| CODRAJoty et al. ( 2015 ) | 83.84 |  82.88 ||\n|| Two-StageWang et al. ( 2017 ) |  86.00 | 77.28 ||\n||  Inter-Domain Evaluation |  Inter-Domain Evaluation |  Inter-Domain Evaluation ||\n|| Two-StageRST-DT | × | 73.65 ||\n|| Two-StageInstr-DT | 74.48 | × ||\n|| Two-StageOurs(avg) | 76.42 |  74.22 ||\n|| Two-StageOurs(max) |  77.24 | 73.12 ||\n|| Human Morey et al. ( 2017 ) | 88.30 | — ||\n\nQuestion: Is it true that The first set of results in Table 3 shows that the completely right/left branching baselines dominate the hierarchical right/left branching ones?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "d6ea265e-a56d-4792-928f-4db6f95be5ef", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. “Reduction” stands for gap reduction when removing gender signals from the context.\nTable:\n||  | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction ||\n|| Same Gender | 0.442 | 0.434 | 0.424 | – | 0.491 | 0.478 | 0.446 | – ||\n|| Different Gender | 0.385 | 0.421 | 0.415 | – | 0.415 | 0.435 | 0.403 | – ||\n|| difference | 0.057 | 0.013 | 0.009 |  91.67% | 0.076 | 0.043 | 0.043 |  100% ||\n\nQuestion: Is it true that In Italian, we get a reduction of 91.67% of the gap with respect to English?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "027fddad-0ece-4a91-a14f-dff863674aa2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.\nTable:\n||  m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1 ||\n|| 1 | 0.541 | 0.595 |  0.566 | 0.495 | 0.621 | 0.551 ||\n|| 2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555 ||\n|| 3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564 ||\n|| 4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 |  0.571 ||\n|| 5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567 ||\n\nQuestion: Is it true that On the NYT11 dataset, m = 5 gives the best performance?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "d66c90ac-981c-4740-bc02-d771a854990f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Building a Production Model for Retrieval-Based Chatbots Table 3: AUC and AUC@p of our model on the propriety help desk dataset.\nTable:\n||  Metric |  Validation |  Test ||\n|| AUC | 0.991 | 0.977 ||\n|| AUC@0.1 | 0.925 | 0.885 ||\n|| AUC@0.05 | 0.871 | 0.816 ||\n|| AUC@0.01 | 0.677 | 0.630 ||\n\nQuestion: Is it true that The high AUC indicates that our model can easily distinguish between the true response and negative responses?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b4c82e00-9bdb-4181-afaa-cdb6340d36f6", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nTable:\n|| Batch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear ||\n|| 1 | 46.7 | 27.3 | 7.6 ||\n|| 10 | 125.2 | 78.2 | 22.7 ||\n|| 25 | 129.7 | 83.1 | 45.4 ||\n\nQuestion: Is it true that For all batch sizes, the training throughput on the linear dataset is the highest, while the throughput on the balanced dataset is the lowest?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "3490236e-fba6-4622-8f84-7a5db25b3965", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.\nTable:\n|| Corpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| Europarl | TotalTerms: | 957 | 1,000 | 1,000 | 1,000 | 1,000 | 836 | 1,000 ||\n|| Europarl | TotalRoots: | 44 | 1 | 1 | 1 | 1 | 43 | 1 ||\n|| Europarl | NumberRels: | 1,588 | 1,025 | 1,028 | 1,185 | 1,103 | 1,184 | 999 ||\n|| Europarl | MaxDepth: | 21 | 921 | 901 | 788 | 835 | 8 | 15 ||\n|| Europarl | MinDepth: | 1 | 921 | 901 | 788 | 835 | 1 | 1 ||\n|| Europarl | AvgDepth: | 11.82 | 921 | 901 | 788 | 835 | 3.05 | 8.46 ||\n|| Europarl | DepthCohesion: | 1.78 | 1 | 1 | 1 | 1 | 2.62 | 1.77 ||\n|| Europarl | MaxWidth: | 20 | 2 | 3 | 4 | 3 | 88 | 41 ||\n|| Europarl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| Europarl | AvgWidth: | 1.99 | 1.03 | 1.03 | 1.19 | 1.10 | 4.20 | 2.38 ||\n|| TED Talks | TotalTerms: | 476 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 ||\n|| TED Talks | TotalRoots: | 164 | 2 | 1 | 1 | 1 | 1 | 1 ||\n|| TED Talks | NumberRels: | 521 | 1,029 | 1,331 | 3,025 | 3,438 | 3,802 | 1,009 ||\n|| TED Talks | MaxDepth: | 16 | 915 | 658 | 454 | 395 | 118 | 12 ||\n|| TED Talks | MinDepth: | 1 | 913 | 658 | 454 | 395 | 110 | 1 ||\n|| TED Talks | AvgDepth: | 5.82 | 914 | 658 | 454 | 395 | 112.24 | 5.95 ||\n|| TED Talks | DepthCohesion: | 2.75 | 1 | 1 | 1 | 1 | 1.05 | 2.02 ||\n|| TED Talks | MaxWidth: | 25 | 2 | 77 | 13 | 12 | 66 | 98 ||\n|| TED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| TED Talks | AvgWidth: | 1.83 | 1.03 | 1.36 | 3.03 | 3.44 | 6.64 | 2.35 ||\n\nQuestion: Is it true that The Patt model was able to generate relations for all terms in the Europarl and TED Talks corpora, as evidenced by the metrics in Table 6?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "2d32e95f-0000-4e62-933b-42e10261b687", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 4: Lexicon member coverage (%)\nTable:\n|| target | VN | WN-V | WN-N ||\n|| type | 81 | 66 | 47 ||\n|| x+POS | 54 | 39 | 43 ||\n|| lemma | 88 | 76 | 53 ||\n|| x+POS | 79 | 63 | 50 ||\n|| shared | 54 | 39 | 41 ||\n\nQuestion: Is it true that WN-N shows low coverage containing many low-frequency members?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "1c0742fa-63b7-44ed-b226-bfa550dabe1c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set\nTable:\n|| Method | WER (%) | DCE ||\n|| No enhancement | 38.4 | 0.958 ||\n|| Wiener filter | 41.0 | 0.775 ||\n|| Minimizing DCE | 31.1 |  0.392 ||\n|| FSEGAN | 29.1 | 0.421 ||\n|| AAS (  wAC=1,  wAD=0) | 27.7 | 0.476 ||\n|| AAS (  wAC=1,  wAD=105) |  26.1 | 0.462 ||\n|| Clean speech | 9.3 | 0.0 ||\n\nQuestion: Is it true that acoustic supervision (27.7%) and multi-task learning (26.1%) show lower WER than minimizing DCE (31.1%) and FSEGAN (29.1%))?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "5655d55f-686a-4173-ae58-87964c81a390", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Neural End-to-End Learning for Computational Argumentation Mining Table 4: C-F1 (100%) in % for the two indicated systems; essay vs. paragraph level. Note that the mean performances are lower than the majority performances over the runs given in Table 2.\nTable:\n||  | STagBLCC | LSTM-Parser ||\n|| Essay | 60.62±3.54 | 9.40±13.57 ||\n|| Paragraph | 64.74±1.97 | 56.24±2.87 ||\n\nQuestion: Is it true that The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "d5553a4a-b710-4865-adb7-3ae9adb2f279", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= “b*tch”\nTable:\n|| Dataset | Class | ˆ  piblack | ˆ  piwhite |  t |  p | ˆ  piblackˆ  piwhite ||\n||  Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 |  | 0.978 ||\n||  | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020 ||\n||  Waseem | Racism | 0.011 | 0.011 | -1.254 |  | 0.955 ||\n||  | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203 ||\n||  | Racism and sexism | 0.012 | 0.012 | -0.162 |  | 0.995 ||\n||  Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152 ||\n||  | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997 ||\n||  Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091 ||\n||  Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728 ||\n||  | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956 ||\n||  | Spam | 0.010 | 0.010 | 0.000 |  | 1.000 ||\n\nQuestion: Is it true that In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "54b24d05-5f41-461c-a0d5-53d4d9bb2b16", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664 ||\n|| P | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 |  0.7311 | 0.5676 ||\n||  | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 |  0.6533 | 0.5656 ||\n|| R | EN | Europarl | 0.0396 | 0.3999 | 0.5499 |  0.6045 | 0.5887 | 0.0023 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 |  0.6077 | 0.2666 | 0.0019 ||\n|| R | PT | Europarl | 0.0111 | 0.3554 | 0.5795 |  0.6727 | 0.5184 | 0.0053 | 0.0012 ||\n||  | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 |  0.6877 | 0.5515 | 0.4706 | 0.0011 ||\n|| F | EN | Europarl | 0.0591 | 0.0671 | 0.0922 |  0.1015 | 0.1003 | 0.0044 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 |  0.1121 | 0.0037 ||\n|| F | PT | Europarl | 0.0217 | 0.3438 | 0.5513 |  0.6403 | 0.5555 | 0.0105 | 0.0024 ||\n||  | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 |  0.6475 | 0.5819 | 0.5471 | 0.0022 ||\n\nQuestion: Is it true that As we can observe, it seems that clustering semantically related terms does not necessarily increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "368b72b0-fb17-4ab7-b573-ba6f59ddc2a4", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\nTable:\n|| <bold>Model</bold> | REF ⇒ GEN <bold>ENT</bold> | REF ⇒ GEN <bold>CON</bold> | REF ⇒ GEN <bold>NEU</bold> ||\n|| S2S | 38.45 | 11.17 | 50.38 ||\n|| G2S-GIN | 49.78 | 9.80 | 40.42 ||\n|| G2S-GAT | 49.48 | 8.09 | 42.43 ||\n|| G2S-GGNN | 51.32 | 8.82 | 39.86 ||\n||  | GEN ⇒ REF | GEN ⇒ REF | GEN ⇒ REF ||\n|| <bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold> ||\n|| S2S | 73.79 | 12.75 | 13.46 ||\n|| G2S-GIN | 76.27 | 10.65 | 13.08 ||\n|| G2S-GAT | 77.54 | 8.54 | 13.92 ||\n|| G2S-GGNN | 77.64 | 9.64 | 12.72 ||\n\nQuestion: Is it true that G2S models also generate sentences that contradict the reference sentences less?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "41e926f4-dc54-48d5-a985-eb56c45a2131", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving Generalization by Incorporating Coverage in Natural Language Inference Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\nTable:\n||  | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK ||\n|| MQAN | 72.30 | 60.91 | 41.82 | 53.95 ||\n|| + coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold> ||\n|| ESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37 ||\n|| + coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold> ||\n\nQuestion: Is it true that The resulting cross-dataset improvements on the SNLI and Glockner datasets are not larger than those on the SICK dataset?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ec93a8d7-8518-41cb-be5e-f605bd53b660", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that G2S-GAT has a better performance in handling graphs with node out-degrees higher than 9?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "535d1def-2141-41d5-8a69-da4175cacf77", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set\nTable:\n|| Method | WER (%) | DCE ||\n|| No enhancement | 38.4 | 0.958 ||\n|| Wiener filter | 41.0 | 0.775 ||\n|| Minimizing DCE | 31.1 |  0.392 ||\n|| FSEGAN | 29.1 | 0.421 ||\n|| AAS (  wAC=1,  wAD=0) | 27.7 | 0.476 ||\n|| AAS (  wAC=1,  wAD=105) |  26.1 | 0.462 ||\n|| Clean speech | 9.3 | 0.0 ||\n\nQuestion: Is it true that The Wiener filtering method shows lower DCE, but higher WER than no enhancement?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "c4fe7068-9584-4aac-900d-e743f0919833", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\nTable:\n||  | Difference Function | Seanad Abolition | Video Games | Pornography ||\n|| OD-parse | Absolute | 0.01 | -0.01 | 0.07 ||\n|| OD-parse | JS div. | 0.01 | -0.01 | -0.01 ||\n|| OD-parse | EMD | 0.07 | 0.01 | -0.01 ||\n|| OD | Absolute |  0.54 |  0.56 |  0.41 ||\n|| OD | JS div. | 0.07 | -0.01 | -0.02 ||\n|| OD | EMD | 0.26 | -0.01 | 0.01 ||\n|| OD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04 ||\n|| OD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02 ||\n|| OD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01 ||\n\nQuestion: Is it true that This is evident from the insignificant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "c2032a31-8e78-411f-aa54-87bf791b98b3", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.\nTable:\n||  Model |  Acc |  F1 |  AUC ||\n|| Most Frequent Class | 64.2 | 39.1 | 0.500 ||\n|| LR-All Features – Original Data | 80.5 | 78.0 | 0.873 ||\n|| Dist. Supervision + Pooling | 77.2 | 75.7 | 0.853 ||\n|| Dist. Supervision + EasyAdapt |  81.2 |  79.0 |  0.885 ||\n\nQuestion: Is it true that Results presented in Table 7 show that the domain adaptation approach further boosts F1 by 1 point to 79 (t-test, p<0.5) and ROC AUC by 0.012?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "91e9f947-9e27-4fe2-9913-b45c570f1d05", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. “Effect” refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\nTable:\n|| # of Heads | Accuracy | Val. Loss | Effect ||\n|| 1 | 89.44% | 0.2811 | -6.84% ||\n|| 2 | 91.20% | 0.2692 | -5.08% ||\n|| 4 | 93.85% | 0.2481 | -2.43% ||\n|| 8 | 96.02% | 0.2257 | -0.26% ||\n|| 10 | 96.28% | 0.2197 |  ||\n|| 16 | 96.32% | 0.2190 | +0.04 ||\n\nQuestion: Is it true that This shows that more attention heads, thereby attending to multiple different contexts at once, does not necessarily lead to state-of-the-art results?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "24a53156-9a34-46d3-8bd8-acd2a28bdb82", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with † use labeled data from source aspects. Models with ‡ use human rationales on the target aspect.\nTable:\n|| Source | Target | Svm | Ra-Svm‡ | Ra-Cnn‡ | Trans† | Ra-Trans‡† | Ours‡† | Oracle† ||\n|| Beer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 |  79.53 | 80.29 ||\n|| Beer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 |  77.94 | 78.11 ||\n|| Beer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 |  75.24 | 75.50 ||\n\nQuestion: Is it true that Our model (OURS) does not obtain substantial gains in accuracy over the baselines across all three target aspects?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "2a8dabe5-b9db-45df-a2d3-8f7ad54d75d2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 5: Performance of different agents on the neural user simulator.\nTable:\n|| Method | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success ||\n|| ACER | 22.35 | 55.13 | 33.08 | 18.6 ||\n|| PPO |  19.23 |  56.31 | 33.08 | 18.3 ||\n|| ALDM | 26.90 | 54.37 | 24.15 | 16.4 ||\n|| GDPL | 22.43 | 52.58 |  36.21 |  19.7 ||\n\nQuestion: Is it true that In comparison, GDPL is still comparable with ACER and PPO, obtains a better match rate, and even achieves higher task success?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "893265ca-c355-4c56-914b-a7e0fc559077", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with ∗ indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nTable:\n|| Model | Method | Training Data | Overall | Easy | Hard | p-value (%) ||\n|| goodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8 ||\n|| gordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5 ||\n|| sasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8∗ ||\n|| Word frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8 ||\n|| BERT-large-FT | LM, NSP | COPA | 76.5 (± 2.7) | 83.9 (± 4.4) | 71.9 (± 2.5) | 0.0∗ ||\n|| RoBERTa-large-FT | LM | COPA | 87.7 (± 0.9) | 91.6 (± 1.1) | 85.3 (± 2.0) | 0.0∗ ||\n\nQuestion: Is it true that However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is only 2.9 percent (71.9% vs. 69.0%)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "202d9083-e874-49ff-8926-97f4f3f5bc91", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\nTable:\n|| <bold>Model</bold> | REF ⇒ GEN <bold>ENT</bold> | REF ⇒ GEN <bold>CON</bold> | REF ⇒ GEN <bold>NEU</bold> ||\n|| S2S | 38.45 | 11.17 | 50.38 ||\n|| G2S-GIN | 49.78 | 9.80 | 40.42 ||\n|| G2S-GAT | 49.48 | 8.09 | 42.43 ||\n|| G2S-GGNN | 51.32 | 8.82 | 39.86 ||\n||  | GEN ⇒ REF | GEN ⇒ REF | GEN ⇒ REF ||\n|| <bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold> ||\n|| S2S | 73.79 | 12.75 | 13.46 ||\n|| G2S-GIN | 76.27 | 10.65 | 13.08 ||\n|| G2S-GAT | 77.54 | 8.54 | 13.92 ||\n|| G2S-GGNN | 77.64 | 9.64 | 12.72 ||\n\nQuestion: Is it true that G2S models generate sentences that contradict the reference sentences more?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "8d5740c5-abf5-47c2-beb2-4150cb29d77f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: The MeMAD Submission to the WMT18 Multimodal Translation Task Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.\nTable:\n||  | en-fr | flickr16 | flickr17 | mscoco17 ||\n|| A | subs1M    H+MS-COCO | 66.3 | 60.5 | 52.1 ||\n|| A | +domain-tuned | 66.8 | 60.6 | 52.0 ||\n|| A | +labels |  67.2 | 60.4 | 51.7 ||\n|| T | subs1M    LM+MS-COCO | 66.9 | 60.3 |  52.8 ||\n|| T | +labels |  67.2 |  60.9 | 52.7 ||\n||  | en-de | flickr16 | flickr17 | mscoco17 ||\n|| A | subs1M    H+MS-COCO | 43.1 | 39.0 | 35.1 ||\n|| A | +domain-tuned | 43.9 | 39.4 | 35.8 ||\n|| A | +labels | 43.2 | 39.3 | 34.3 ||\n|| T | subs1M    LM+MS-COCO |  44.4 | 39.4 | 35.0 ||\n|| T | +labels | 44.1 |  39.8 |  36.5 ||\n\nQuestion: Is it true that  For Marian amun, the effect is negligible as we can see in Table 3?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "cad7b2c8-2a39-4cb7-84fc-68280ec753d8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.\nTable:\n||  Model |  T | #P | B | C ||\n|| Seq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5 ||\n|| DCGCN (ours) | S |  19.1M | 27.9 | 57.3 ||\n|| DCGCN (ours) | E | 92.5M |  30.4 |  59.6 ||\n\nQuestion: Is it true that For example, on AMR17, the ensemble model of Seq2SeqB is 1 BLEU point higher than the single DCGCN model?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "20f98547-11fd-48bd-a892-284b3df13a83", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\nTable:\n||  Method |  R-1 |  R-2 |  R-SU ||\n|| First-1 | 26.83 | 7.25 | 6.46 ||\n|| First-2 | 35.99 | 10.17 | 12.06 ||\n|| First-3 | 39.41 | 11.77 | 14.51 ||\n|| LexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20 ||\n|| TextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50 ||\n|| MMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91 ||\n|| PG-Original Lebanoff et al. ( 2018 ) | 41.85 | 12.91 | 16.46 ||\n|| PG-MMR Lebanoff et al. ( 2018 ) | 40.55 | 12.36 | 15.87 ||\n|| PG-BRNN Gehrmann et al. ( 2018 ) | 42.80 | 14.19 | 16.75 ||\n|| CopyTransformer Gehrmann et al. ( 2018 ) |  43.57 | 14.03 | 17.37 ||\n|| Hi-MAP (Our Model) | 43.47 |  14.89 |  17.41 ||\n\nQuestion: Is it true that The Transformer performs best in terms of R-1 while Hi-MAP does not outperform it on R-2 and R-SU?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "e76cbbbb-e973-4cdf-9ab0-a1a39fec7cfc", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\nTable:\n|| <bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1 ||\n|| <bold>Baselines</bold> |  |  |  |  |  |  |  |  |  |  ||\n|| Cluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5 ||\n|| CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73 ||\n|| KCP Kenyon-Dean et al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69 ||\n|| Cluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6 ||\n|| <bold>Model Variants</bold> |  |  |  |  |  |  |  |  |  |  ||\n|| Disjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5 ||\n|| Joint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold> ||\n\nQuestion: Is it true that  Our model achieves state-of-the-art results, outperforming previous models by 9.9 CoNLL F1 points on events?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "6a39e1b2-806a-4c81-953d-0a1db9b7d962", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\nTable:\n||  Training data |  Add |  Miss |  Wrong |  Disfl ||\n|| Original | 0 | 22 | 0 | 14 ||\n|| Cleaned added | 0 | 23 | 0 | 14 ||\n|| Cleaned missing | 0 | 1 | 0 | 2 ||\n|| Cleaned | 0 | 0 | 0 | 5 ||\n\nQuestion: Is it true that All fluency problems we found were very slight, but added and wrong-valued slots were still found, so missed slots are not the only problem?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "f480c688-06c4-459b-affc-8737fc822e2b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 4: Results of Self-Play Evaluation.\nTable:\n|| System | TGPC Succ. (%) | TGPC #Turns | CWC Succ. (%) | CWC #Turns ||\n|| Retrieval  | 7.16 | 4.17 | 0 | - ||\n|| Retrieval-Stgy  | 47.80 | 6.7 | 44.6 | 7.42 ||\n|| PMI  | 35.36 | 6.38 | 47.4 | 5.29 ||\n|| Neural  | 54.76 | 4.73 | 47.6 | 5.16 ||\n|| Kernel  | 62.56 | 4.65 | 53.2 | 4.08 ||\n|| DKRN (ours) |  89.0 | 5.02 |  84.4 | 4.20 ||\n\nQuestion: Is it true that This table refutes the effectiveness of our approach?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "681f0a81-e820-45f2-9d6b-64137a6a6c7c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.\nTable:\n||  Type |  Reparandum Length   1-2 |  Reparandum Length   3-5 ||\n|| content-content | 0.61 (30%) | 0.58 (52%) ||\n|| content-function | 0.77 (20%) | 0.66 (17%) ||\n|| function-function | 0.83 (50%) | 0.80 (32%) ||\n\nQuestion: Is it true that We found that rephrase disfluencies that contain content words are harder for the model to detect, compared to rephrases with function words only, and error increases for longer disfluencies?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "249235b7-0bb2-431e-b55c-3adcbc63a9d2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Filling Conversation Ellipsis for Better Social Dialog Understanding Table 6: Dialog act prediction performance using different selection methods.\nTable:\n||  Selection Method |  Prec.(%) |  Rec.(%) |  F1(%) ||\n|| Max Logits | 80.19 | 80.50 | 79.85 ||\n|| Add Logits | 81.30 | 81.28 | 80.85 ||\n|| Add Logits+Expert |  81.30 |  81.41 |  80.90 ||\n|| Concat Hidden | 80.24 | 80.04 | 79.65 ||\n|| Max Hidden | 80.30 | 80.04 | 79.63 ||\n|| Add Hidden | 80.82 | 80.28 | 80.08 ||\n\nQuestion: Is it true that We can see from Table 6 that empirically adding logits from two models after classifiers performs the best?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "5eb5ab6e-0556-435d-b7c3-f73a75086415", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| Encoder Modules |  |  ||\n|| -Linear Combination | 23.7 | 53.2 ||\n|| -Global Node | 24.2 | 54.6 ||\n|| -Direction Aggregation | 24.6 | 54.6 ||\n|| -Graph Attention | 24.9 | 54.7 ||\n|| -Global Node&Linear Combination | 22.9 | 52.4 ||\n|| Decoder Modules |  |  ||\n|| -Coverage Mechanism | 23.8 | 53.0 ||\n\nQuestion: Is it true that Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "2a748b24-0923-494a-b41b-5b290c77df35", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTable:\n|| Topic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI |  OD-w2v ARI |  OD-d2v ARI | TF-IDF   Sil. | WMD   Sil. | Sent2vec   Sil. | Doc2vec   Sil. | BERT   Sil. |  OD-w2v   Sil. |  OD-d2v   Sil. ||\n|| Affirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 |  0.14 |  0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 |  0.06 |  0.01 ||\n|| Atheism | 116 |  0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 |  0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 |  0.05 |  0.07 ||\n|| Austerity Measures | 20 |  0.04 |  0.04 | -0.01 | -0.05 | 0.04 |  0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 |  0.19 | 0.1 ||\n|| Democratization | 76 | 0.02 | -0.01 | 0.00 |  0.09 | -0.01 |  0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 |  0.16 |  0.11 ||\n|| Education Voucher Scheme | 30 |  0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 |  0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 |  0.38 |  0.40 ||\n|| Gambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 |  0.35 |  0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 |  0.30 |  0.22 ||\n|| Housing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 |  0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 |  0.13 |  0.13 ||\n|| Hydroelectric Dams | 110 |  0.47 |  0.45 |  0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 |  0.26 |  0.09 ||\n|| Intellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 |  0.05 |  0.14 | 0.01 |  0.04 | 0.03 | 0.01 | 0.03 |  0.04 |  0.12 ||\n|| Keystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 |  0.07 | -0.01 |  0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 |  0.05 |  0.02 ||\n|| Monarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 |  0.15 |  0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 |  0.11 |  0.09 ||\n|| National Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 |  0.31 |  0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 |  0.25 |  0.25 ||\n|| One-child policy China | 67 | -0.05 | 0.01 |  0.11 | -0.02 | 0.02 |  0.11 | 0.01 | 0.01 | 0.02 |  0.04 | -0.01 | 0.03 |  0.07 | -0.02 ||\n|| Open-source Software | 48 | -0.02 | -0.01 |  0.05 | 0.01 | 0.12 |  0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 |  0.18 | 0.01 ||\n|| Pornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 |  0.41 |  0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 |  0.47 |  0.41 ||\n|| Seanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 |  0.32 |  0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 |  0.15 |  0.31 ||\n|| Trades Unions | 19 |  0.44 |  0.44 |  0.60 | -0.05 | 0.44 |  0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 |  0.48 |  0.32 ||\n|| Video Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 |  0.40 |  0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 |  0.32 |  0.42 ||\n|| Average | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 |  0.22 |  0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 |  0.20 |  0.17 ||\n\nQuestion: Is it true that The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are not statistically significant (paired t-test) with respect to baselines at significance level 0.005?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "f2719604-1c66-4880-9cef-38422fcdc053", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.\nTable:\n||  Model |  T | #P | B | C ||\n|| Seq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4 ||\n|| Seq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5 ||\n|| GGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5 ||\n|| DCGCN (ours) | S |  19.1M | 27.9 | 57.3 ||\n|| DCGCN (ours) | E | 92.5M |  30.4 |  59.6 ||\n\nQuestion: Is it true that The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "c1eced18-5360-4a8e-af31-06277e4a832e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Syntactic Dependency Representations in Neural Relation Classification Table 1: Effect of using the shortest dependency path on each relation type.\nTable:\n||  Relation |  best F1 (in 5-fold) without sdp |  best F1 (in 5-fold) with sdp |  Diff. ||\n|| USAGE | 60.34 | 80.24 | + 19.90 ||\n|| MODEL-FEATURE | 48.89 | 70.00 | + 21.11 ||\n|| PART_WHOLE | 29.51 | 70.27 | +40.76 ||\n|| TOPIC | 45.80 | 91.26 | +45.46 ||\n|| RESULT | 54.35 | 81.58 | +27.23 ||\n|| COMPARE | 20.00 | 61.82 | + 41.82 ||\n|| macro-averaged | 50.10 | 76.10 | +26.00 ||\n\nQuestion: Is it true that We find that the effect of syntactic structure varies between the different relation types?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "a63f189d-7408-49c3-bc3c-2b89a01e30cf", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nTable:\n|| Model | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time ||\n|| Rocktäschel et al. ( 2016 ) | Rocktäschel et al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | - ||\n|| This | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 |  90.49 | 0.696 ||\n|| This | GRU | 6.41M |  85.71 | 0.245 |  86.05 | 0.419 |  90.29 | 0.529 | 90.10 | 0.695 ||\n|| This | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580 ||\n|| Work | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555 ||\n||  | LRN | 4.25M | 84.88 |  0.209 | 85.06 |  0.223 | 89.98 |  0.488 | 89.93 |  0.506 ||\n\nQuestion: Is it true that LRN is still the fastest model, outperforming other recurrent units by 8%∼27%?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "97d49102-06a2-4887-84ba-121e1a200ed6", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Table 5: Comparison on hard and soft alignments.\nTable:\n|| Metrics | cs-en | de-en | fi-en | lv-en ||\n|| RUSE | 0.624 | 0.644 | 0.750 | 0.697 ||\n|| Hmd-F1 + BERT | 0.655 | 0.681 | 0.821 | 0.712 ||\n|| Hmd-Recall + BERT | 0.651 | 0.658 | 0.788 | 0.681 ||\n|| Hmd-Prec + BERT | 0.624 | 0.669 | 0.817 | 0.707 ||\n|| Wmd-unigram + BERT | 0.651 | 0.686 | <bold>0.823</bold> | 0.710 ||\n|| Wmd-bigram + BERT | <bold>0.665</bold> | <bold>0.688</bold> | 0.821 | <bold>0.712</bold> ||\n\nQuestion: Is it true that We also observe that WMD-UNIGRAMS slightly outperforms WMD-BIGRAMS on 3 out of 4 language pairs?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "9a337795-1c06-4d0e-91f3-3ec46743dc82", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 3: Results of Turn-level Evaluation.\nTable:\n|| Dataset | System | Keyword Prediction   Rw@1 | Keyword Prediction   Rw@3 | Keyword Prediction   Rw@5 | Keyword Prediction P@1 | Response Retrieval   R20@1 | Response Retrieval   R20@3 | Response Retrieval   R20@5 | Response Retrieval MRR ||\n|| TGPC | Retrieval  | - | - | - | - | 0.5063 | 0.7615 | 0.8676 | 0.6589 ||\n|| TGPC | PMI  | 0.0585 | 0.1351 | 0.1872 | 0.0871 | 0.5441 | 0.7839 | 0.8716 | 0.6847 ||\n|| TGPC | Neural  | 0.0708 | 0.1438 | 0.1820 | 0.1321 | 0.5311 | 0.7905 | 0.8800 | 0.6822 ||\n|| TGPC | Kernel  | 0.0632 | 0.1377 | 0.1798 | 0.1172 | 0.5386 | 0.8012 | 0.8924 | 0.6877 ||\n|| TGPC | DKRN (ours) |  0.0909 |  0.1903 |  0.2477 |  0.1685 |  0.5729 |  0.8132 |  0.8966 |  0.7110 ||\n|| CWC | Retrieval  | - | - | - | - | 0.5785 | 0.8101 | 0.8999 | 0.7141 ||\n|| CWC | PMI  | 0.0555 | 0.1001 | 0.1212 | 0.0969 | 0.5945 | 0.8185 | 0.9054 | 0.7257 ||\n|| CWC | Neural  | 0.0654 | 0.1194 | 0.1450 | 0.1141 | 0.6044 | 0.8233 | 0.9085 | 0.7326 ||\n|| CWC | Kernel  | 0.0592 | 0.1113 | 0.1337 | 0.1011 | 0.6017 | 0.8234 | 0.9087 | 0.7320 ||\n|| CWC | DKRN (ours) |  0.0680 |  0.1254 |  0.1548 |  0.1185 |  0.6324 |  0.8416 |  0.9183 |  0.7533 ||\n\nQuestion: Is it true that Our approach DKRN outperforms all state-of-the-art methods in terms of all metrics on both datasets with two tasks?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "84d8d7a2-2811-41a4-b5a3-1777e9b8af8a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 5: Performance of different agents on the neural user simulator.\nTable:\n|| Method | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success ||\n|| ACER | 22.35 | 55.13 | 33.08 | 18.6 ||\n|| PPO |  19.23 |  56.31 | 33.08 | 18.3 ||\n|| ALDM | 26.90 | 54.37 | 24.15 | 16.4 ||\n|| GDPL | 22.43 | 52.58 |  36.21 |  19.7 ||\n\nQuestion: Is it true that ALDM even gets worse performance than ACER and PPO?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "c9d32538-fe25-40c4-a010-0b5e2a167331", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\nTable:\n||  Emoji alias |  N |  emoji # |  emoji % |  no-emoji # |  no-emoji % |  Δ% ||\n|| mask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27 ||\n|| two_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59 ||\n|| heart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91 ||\n|| heart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75 ||\n|| rage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04 ||\n|| cry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07 ||\n|| sob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67 ||\n|| unamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00 ||\n|| weary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49 ||\n|| joy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05 ||\n|| sweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80 ||\n|| confused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60 ||\n\nQuestion: Is it true that Contrary to intuition, the sob emoji contributes more than cry, despite representing a stronger emotion?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "0988097c-eeaa-4876-91cc-424a0e4d7f65", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Assessing Gender Bias in Machine Translation – A Case Study with Google Translate Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table\nTable:\n|| Category | Female (%) | Male (%) | Neutral (%) ||\n|| Office and administrative support | 11.015 | 58.812 | 16.954 ||\n|| Architecture and engineering | 2.299 | 72.701 | 10.92 ||\n|| Farming, fishing, and forestry | 12.179 | 62.179 | 14.744 ||\n|| Management | 11.232 | 66.667 | 12.681 ||\n|| Community and social service | 20.238 | 62.5 | 10.119 ||\n|| Healthcare support | 25.0 | 43.75 | 17.188 ||\n|| Sales and related | 8.929 | 62.202 | 16.964 ||\n|| Installation, maintenance, and repair | 5.22 | 58.333 | 17.125 ||\n|| Transportation and material moving | 8.81 | 62.976 | 17.5 ||\n|| Legal | 11.905 | 72.619 | 10.714 ||\n|| Business and financial operations | 7.065 | 67.935 | 15.58 ||\n|| Life, physical, and social science | 5.882 | 73.284 | 10.049 ||\n|| Arts, design, entertainment, sports, and media | 10.36 | 67.342 | 11.486 ||\n|| Education, training, and library | 23.485 | 53.03 | 9.091 ||\n|| Building and grounds cleaning and maintenance | 12.5 | 68.333 | 11.667 ||\n|| Personal care and service | 18.939 | 49.747 | 18.434 ||\n|| Healthcare practitioners and technical | 22.674 | 51.744 | 15.116 ||\n|| Production | 14.331 | 51.199 | 18.245 ||\n|| Computer and mathematical | 4.167 | 66.146 | 14.062 ||\n|| Construction and extraction | 8.578 | 61.887 | 17.525 ||\n|| Protective service | 8.631 | 65.179 | 12.5 ||\n|| Food preparation and serving related | 21.078 | 58.333 | 17.647 ||\n|| Total | 11.76 | 58.93 | 15.939 ||\n\nQuestion: Is it true that What we have found is that Google Translate does not always translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, as evidenced by the data in Table 6?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "59876715-8c94-4df5-8027-281cc74e8292", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nTable:\n|| VS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L ||\n|| ACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18 ||\n|| PPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10 ||\n|| ALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15 ||\n\nQuestion: Is it true that Among all the baselines, GDPL obtains the most preference against PPO?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "0633fe26-997d-4980-b1c3-69077f797d1e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. “Pref%”: in how many percentage of documents a system receives the higher human rating.\nTable:\n|| Reward | R-1 | R-2 | R-L | Human | Pref% ||\n|| R-L (original) | 40.9 | 17.8 | 38.5 | 1.75 | 15 ||\n|| Learned (ours) | 39.2 | 17.4 | 37.5 |  2.20 |  75 ||\n\nQuestion: Is it true that It is clear from Table 5 that using the learned reward does not help the RL-based system generate summaries with significantly higher human ratings?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "531171f1-fe4b-4849-81ec-36b06b6eb36f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.\nTable:\n|| Batch size | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Training | Throughput (instances/s) Training | Throughput (instances/s) Training ||\n|| Batch size | Iter | Recur | Fold | Iter | Recur | Fold ||\n|| 1 | 19.2 | 81.4 | 16.5 | 2.5 | 4.8 | 9.0 ||\n|| 10 | 49.3 | 217.9 | 52.2 | 4.0 | 4.2 | 37.5 ||\n|| 25 | 72.1 | 269.9 | 61.6 | 5.5 | 3.6 | 54.7 ||\n\nQuestion: Is it true that The amount of resources is sufficient for executing forward computations, and therefore our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "a0b9120d-320d-4547-967f-d8b3eb9529f2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations Table 3: Cue and token distribution in the conversational negation corpus.\nTable:\n|| Total negation cues | 2921 ||\n|| True negation cues | 2674 ||\n|| False negation cues | 247 ||\n|| Average scope length | 2.9 ||\n|| Average sentence length | 13.6 ||\n|| Average tweet length | 22.3 ||\n\nQuestion: Is it true that The average number of tokens per tweet is 22.3, per sentence is 13.6 and average scope length is 2.9?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "6592737a-49c3-4723-b433-e554703165cd", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that  G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "499719d6-acd1-40d1-8962-3bd945f7691c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Deriving Machine Attention from Human Rationales Table 4: Accuracy of transferring between domains. Models with † use labeled data from source domains and unlabeled data from the target domain. Models with ‡ use human rationales on the target task.\nTable:\n|| Source | Target | Svm | Ra-Svm‡ | Ra-Cnn‡ | Trans† | Ra-Trans‡† | Ours‡† | Oracle† ||\n|| Beer look + Beer aroma + Beer palate | Hotel location | 78.65 | 79.09 | 79.28 | 80.42 | 82.10 |  84.52 | 85.43 ||\n|| Beer look + Beer aroma + Beer palate | Hotel cleanliness | 86.44 | 86.68 | 89.01 | 86.95 | 87.15 |  90.66 | 92.09 ||\n|| Beer look + Beer aroma + Beer palate | Hotel service | 85.34 | 86.61 | 87.91 | 87.37 | 86.40 |  89.93 | 92.42 ||\n\nQuestion: Is it true that The error reduction over the best baseline is only 5.09% on average?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "56344d9a-7b9f-4606-ac3b-ef1134c5db28", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: What do Deep Networks Like to Read? Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.\nTable:\n|| Orig | <u> turns in a <u> screenplay that <u> at the edges ; it ’s so clever you want to hate it . ||\n|| DAN | <u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate ||\n|| CNN | she turns on a on ( ( in in the the the edges ’s so clever “ want to hate it ” ||\n|| RNN | <u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it . ||\n\nQuestion: Is it true that In contrast, DAN masks out punctuation and determiners using words indicative of the class label (i.e?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "62f2b650-5f5f-46e6-8570-bd6ff0013ea0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664 ||\n|| P | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 |  0.7311 | 0.5676 ||\n||  | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 |  0.6533 | 0.5656 ||\n|| R | EN | Europarl | 0.0396 | 0.3999 | 0.5499 |  0.6045 | 0.5887 | 0.0023 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 |  0.6077 | 0.2666 | 0.0019 ||\n|| R | PT | Europarl | 0.0111 | 0.3554 | 0.5795 |  0.6727 | 0.5184 | 0.0053 | 0.0012 ||\n||  | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 |  0.6877 | 0.5515 | 0.4706 | 0.0011 ||\n|| F | EN | Europarl | 0.0591 | 0.0671 | 0.0922 |  0.1015 | 0.1003 | 0.0044 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 |  0.1121 | 0.0037 ||\n|| F | PT | Europarl | 0.0217 | 0.3438 | 0.5513 |  0.6403 | 0.5555 | 0.0105 | 0.0024 ||\n||  | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 |  0.6475 | 0.5819 | 0.5471 | 0.0022 ||\n\nQuestion: Is it true that TF has the best values of recall and f-measure for all corpora except the English version of TED Talks, where DF has the best value of recall and HClust has the best value of f-measure?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "f3feea83-901a-4779-ab3b-bbd37a7b2ad8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.\nTable:\n||  |  Present |  Not Present ||\n|| Emoji | 4805 (76.6%) | 23952 (68.0%) ||\n|| Hashtags | 2122 (70.5%) | 26635 (69.4%) ||\n\nQuestion: Is it true that  Tweets containing emoji seem to be easier for the model to classify than those without?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8e8f27d4-b0f5-43ef-b7ea-55a8d400fc5e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nTable:\n|| Model | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score ||\n|| 1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 |  18.9 | 85.1 ||\n|| 2. Seq2Seq + Copy | oracle | - | - | 86.2 |  72.0 | 15.7 | 94.8 ||\n|| 3. MD-Sequicity | oracle | - | - |  86.6 | 71.6 | 16.8 |  95.9 ||\n|| 4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7 ||\n|| 5. HDSA  | oracle | generated | graph | 82.9 | 68.9 |  23.6 | 99.5 ||\n|| 6. DAMD | oracle | generated | span |  89.5 | 75.8 | 18.3 | 100.9 ||\n|| 7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 |  77.9 | 18.6 |  102.2 ||\n|| 8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0 ||\n|| 9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 |  30.4 | 113.4 ||\n|| 10. DAMD + multi-action data augmentation | oracle | oracle | span |  95.4 |  87.2 | 27.3 |  118.5 ||\n|| 11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 |  16.9 | 83.0 ||\n|| 12. DAMD + multi-action data augmentation | generated | generated | span |  76.3 |  60.4 | 16.6 |  85.0 ||\n\nQuestion: Is it true that  The relative lower BLEU score of our DAMD model compared to other models with different system action forms suggests that it does not outperform them in terms of inform and success rates,  While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), it is not enough to make up for the lower BLEU score,  Moreover, even if a model has access to ground truth system action, the model does not necessarily improve its task performance?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "8da915c7-59a0-473f-9ae4-dc07094a27f0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improving Generalization by Incorporating Coverage in Natural Language Inference Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\nTable:\n||  | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL ||\n||  | EM | F1 | EM | F1 ||\n|| MQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10 ||\n|| +coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold> ||\n|| BIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98 ||\n|| +coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold> ||\n\nQuestion: Is it true that Table 3 shows the impact of coverage for decreasing generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "754e6967-568c-467b-8192-79e841cef788", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Table 4: Pearson correlation with system-level human judgments on MSCOCO dataset. ’M’ and ’P’ are short names.\nTable:\n|| Setting | Metric | M1 | M2 ||\n|| Baselines | LEIC(*) | <bold>0.939</bold> | <bold>0.949</bold> ||\n|| Baselines | METEOR | 0.606 | 0.594 ||\n|| Baselines | SPICE | 0.759 | 0.750 ||\n|| Baselines | BERTScore-Recall | 0.809 | 0.749 ||\n|| Sent-Mover | SMD + W2V | 0.683 | 0.668 ||\n|| Sent-Mover | SMD + ELMO + P | 0.709 | 0.712 ||\n|| Sent-Mover | SMD + BERT + P | 0.723 | 0.747 ||\n|| Sent-Mover | SMD + BERT + M + P | 0.789 | 0.784 ||\n|| Word-Mover | Wmd-1 + W2V | 0.728 | 0.764 ||\n|| Word-Mover | Wmd-1 + ELMO + P | 0.753 | 0.775 ||\n|| Word-Mover | Wmd-1 + BERT + P | 0.780 | 0.790 ||\n|| Word-Mover | Wmd-1 + BERT + M + P | <bold>0.813</bold> | <bold>0.810</bold> ||\n|| Word-Mover | Wmd-2 + BERT + M + P | 0.812 | 0.808 ||\n\nQuestion: Is it true that Table 4: Word mover metrics outperform all baselines except for the supervised metric LEIC, which uses more information by considering both images and texts?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8b062e9b-8f83-4dd6-b370-1a476c743858", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks [Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (‘T’) and Ubuntuforum (‘U’). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (‘±’). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\nTable:\n||  System |  ROUGE-1   R (%) |  ROUGE-1   P (%) |  ROUGE-1   F (%) |  ROUGE-2   R (%) |  ROUGE-2   P (%) |  ROUGE-2   F (%) |  Sentence-Level   R (%) |  Sentence-Level   P (%) |  Sentence-Level   F (%) ||\n||  ILP | 24.5 | 41.1 | 29.3±0.5 | 7.9 | 15.0 | 9.9±0.5 | 13.6 | 22.6 | 15.6±0.4 ||\n||  Sum-Basic | 28.4 | 44.4 | 33.1±0.5 | 8.5 | 15.6 | 10.4±0.4 | 14.7 | 22.9 | 16.7±0.5 ||\n||  KL-Sum | 39.5 | 34.6 | 35.5±0.5 | 13.0 | 12.7 | 12.3±0.5 | 15.2 | 21.1 | 16.3±0.5 ||\n||  LexRank | 42.1 | 39.5 | 38.7±0.5 | 14.7 | 15.3 | 14.2±0.5 | 14.3 | 21.5 | 16.0±0.5 ||\n||  MEAD | 45.5 | 36.5 | 38.5± 0.5 | 17.9 | 14.9 | 15.4±0.5 | 27.8 | 29.2 | 26.8±0.5 ||\n||  SVM | 19.0 | 48.8 | 24.7±0.8 | 7.5 | 21.1 | 10.0±0.5 | 32.7 | 34.3 | 31.4±0.4 ||\n||  LogReg | 26.9 | 34.5 | 28.7±0.6 | 6.4 | 9.9 | 7.3±0.4 | 12.2 | 14.9 | 12.7±0.5 ||\n||  LogReg  r | 28.0 | 34.8 | 29.4±0.6 | 6.9 | 10.4 | 7.8±0.4 | 12.1 | 14.5 | 12.5±0.5 ||\n||  HAN | 31.0 | 42.8 | 33.7±0.7 | 11.2 | 17.8 | 12.7±0.5 | 26.9 | 34.1 | 32.4±0.5 ||\n||  HAN+pretrainT | 32.2 | 42.4 | 34.4±0.7 | 11.5 | 17.5 | 12.9±0.5 | 29.6 | 35.8 | 32.2±0.5 ||\n||  HAN+pretrainU | 32.1 | 42.1 | 33.8±0.7 | 11.6 | 17.6 | 12.9±0.5 | 30.1 | 35.6 | 32.3±0.5 ||\n||  HAN  r | 38.1 | 40.5 |  37.8±0.5 | 14.0 | 17.1 |  14.7±0.5 | 32.5 | 34.4 |  33.4±0.5 ||\n||  HAN+pretrainT  r | 37.9 | 40.4 |  37.6±0.5 | 13.5 | 16.8 |  14.4±0.5 | 32.5 | 34.4 |  33.4±0.5 ||\n||  HAN+pretrainU  r | 37.9 | 40.4 |  37.6±0.5 | 13.6 | 16.9 |  14.4±0.5 | 33.9 | 33.8 |  33.8±0.5 ||\n\nQuestion: Is it true that The HAN models outperform MEAD in terms of sentence prediction?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3b3489c6-cb2f-4a1a-b462-ec4844bcfbf2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. “Raw” indicates the usage of original prosodic features (Section 3.2), while “innovations” indicate the usage of innovation features (Section 3.3).\nTable:\n||  |  Model |  dev mean |  dev best |  test mean |  test best |  α ||\n|| single | text | 86.54 | 86.80 | 86.47 | 86.96 | – ||\n|| single | raw | 35.00 | 37.33 | 35.78 | 37.70 | – ||\n|| single | innovations | 80.86 | 81.51 | 80.28 | 82.15 | – ||\n|| early | text + raw | 86.46 | 86.65 | 86.24 | 86.53 | – ||\n|| early | text + innovations | 86.53 | 86.77 | 86.54 | 87.00 | – ||\n|| early | text + raw + innovations | 86.35 | 86.69 | 86.55 | 86.44 | – ||\n|| late | text + raw | 86.71 | 87.05 | 86.35 | 86.71 | 0.2 ||\n|| late | text + innovations |  86.98 |  87.48 |  86.68 |  87.02 | 0.5 ||\n|| late | text + raw + innovations | 86.95 | 87.30 | 86.60 | 86.87 | 0.5 ||\n\nQuestion: Is it true that The interpolation weight α for the late fusion experiments is low when innovations are used, which further indicates that innovation features are not useful in overall prediction?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "eb9d6e8f-d389-49e1-a146-61202625fda6", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).\nTable:\n|| Train | Test |  System |  BLEU |  NIST |  METEOR |  ROUGE-L |  CIDEr |  Add |  Miss |  Wrong |  SER ||\n|| Original |  Cleaned | TGen− | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31 ||\n|| Original |  Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05 ||\n|| Original |  Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24 ||\n|| Original |  Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen− | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned |  Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94 ||\n|| Cleaned missing |  Cleaned | TGen− | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52 ||\n|| Cleaned missing |  Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86 ||\n|| Cleaned missing |  Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen− | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29 ||\n|| 1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added |  Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88 ||\n\nQuestion: Is it true that The results in the table suggest that cleaning the missing slots did not provide more complex training examples?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "2eaca02a-6756-46bf-99d2-d597218b717d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\nTable:\n|| <bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 Δ | <bold>Graph Diameter</bold> 7-13 Δ | <bold>Graph Diameter</bold> 14-20 Δ ||\n|| S2S | 33.2 | 29.7 | 28.8 ||\n|| G2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2% ||\n|| G2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51% ||\n|| G2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7% ||\n||  | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> ||\n||  | 0-20 Δ | 20-50 Δ | 50-240 Δ ||\n|| S2S | 34.9 | 29.9 | 25.1 ||\n|| G2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8% ||\n|| G2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1% ||\n|| G2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8% ||\n||  | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> ||\n||  | 0-3 Δ | 4-8 Δ | 9-18 Δ ||\n|| S2S | 31.7 | 30.0 | 23.9 ||\n|| G2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2% ||\n|| G2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0% ||\n|| G2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3% ||\n\nQuestion: Is it true that This indicates that GINs cannot be employed in tasks where the distribution of node degrees has a long tail?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "fb295289-5470-4bd0-99a4-18c93946d800", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| Encoder Modules |  |  ||\n|| -Linear Combination | 23.7 | 53.2 ||\n|| -Global Node | 24.2 | 54.6 ||\n|| -Direction Aggregation | 24.6 | 54.6 ||\n|| -Graph Attention | 24.9 | 54.7 ||\n|| -Global Node&Linear Combination | 22.9 | 52.4 ||\n|| Decoder Modules |  |  ||\n|| -Coverage Mechanism | 23.8 | 53.0 ||\n\nQuestion: Is it true that The coverage mechanism is also effective in our models?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "1818df5d-04f4-44b0-8e69-fd87724f010c", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 7: Sensitivity of BERT-large to superficial cues identified in §2 (unit: 10−2). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.\nTable:\n|| Cue |  SCOPA |  SB_COPA | Diff. | Prod. ||\n|| woman | 7.98 | 4.84 | -3.14 | 0.25 ||\n|| mother | 5.16 | 3.95 | -1.21 | 0.75 ||\n|| went | 6.00 | 5.15 | -0.85 | 0.73 ||\n|| down | 5.52 | 4.93 | -0.58 | 0.71 ||\n|| into | 4.07 | 3.51 | -0.56 | 0.40 ||\n\nQuestion: Is it true that We observe that BERT trained on Balanced COPA is less sensitive to a few highly productive superficial cues than BERT trained on original COPA?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "d588b47d-e4c7-49e6-8f76-0c5678b232ea", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nTable:\n|| Model | Training data | Overall | Easy | Hard ||\n|| BERT-large-FT | B-COPA | 74.5 (± 0.7) | 74.7 (± 0.4) |  74.4 (± 0.9) ||\n|| BERT-large-FT | B-COPA (50%) | 74.3 (± 2.2) | 76.8 (± 1.9) | 72.8 (± 3.1) ||\n|| BERT-large-FT | COPA |  76.5 (± 2.7) |  83.9 (± 4.4) | 71.9 (± 2.5) ||\n|| RoBERTa-large-FT | B-COPA |  89.0 (± 0.3) | 88.9 (± 2.1) |  89.0 (± 0.8) ||\n|| RoBERTa-large-FT | B-COPA (50%) | 86.1 (± 2.2) | 87.4 (± 1.1) | 85.4 (± 2.9) ||\n|| RoBERTa-large-FT | COPA | 87.7 (± 0.9) |  91.6 (± 1.1) | 85.3 (± 2.0) ||\n\nQuestion: Is it true that Moreover, training on B-COPA improves performance on the Hard subset, both when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "8b27ad92-0b0c-4813-94fc-d817b8f9837d", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.\nTable:\n||  Model |  Acc |  F1 |  AUC ||\n|| Most Frequent Class | 64.2 | 39.1 | 0.500 ||\n|| LR-All Features – Original Data | 80.5 | 78.0 | 0.873 ||\n|| Dist. Supervision + Pooling | 77.2 | 75.7 | 0.853 ||\n|| Dist. Supervision + EasyAdapt |  81.2 |  79.0 |  0.885 ||\n\nQuestion: Is it true that  However, simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in F1?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "76767541-0820-44a2-9a66-24bea826ecca", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En→De, En→Fi and En→Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nTable:\n|| Method | En→It best | En→It avg | En→It iters | En→De best | En→De avg | En→De iters | En→Fi best | En→Fi avg | En→Fi iters | En→Es best | En→Es avg | En→Es iters ||\n|| Artetxe et al., 2018b |  48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808 ||\n|| Noise-aware Alignment |  48.53 |  48.20 | 471 |  49.67 |  48.89 | 568 |  33.98 |  33.68 | 502 |  38.40 |  37.79 | 551 ||\n\nQuestion: Is it true that Our model improves the results in the translation tasks?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "41907607-9691-409a-9a0a-517d74061500", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| -{4} dense block | 24.8 | 54.9 ||\n|| -{3, 4} dense blocks | 23.8 | 54.1 ||\n|| -{2, 3, 4} dense blocks | 23.2 | 53.1 ||\n\nQuestion: Is it true that The full model does not give the best performance on the AMR15 dev set?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "050de577-add7-44d3-9d2c-7892c25a8464", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section 2.1 for further details of each evaluation metrics). “NS” indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nTable:\n|| Model | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4 ||\n|| Shortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3 ||\n|| PRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9 ||\n|| PRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4 ||\n\nQuestion: Is it true that  Although the PRKGC model do not receive supervision about human-generated NLDs, paths with the maximum score match human-generated NLDs to some extent?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "de8ef478-5e2b-4df6-90f3-1e34c08c069a", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\nTable:\n||  Model | B | C ||\n|| DCGCN4 | 25.5 | 55.4 ||\n|| -{4} dense block | 24.8 | 54.9 ||\n|| -{3, 4} dense blocks | 23.8 | 54.1 ||\n|| -{2, 3, 4} dense blocks | 23.2 | 53.1 ||\n\nQuestion: Is it true that In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "e500eb41-5d94-4380-b1a6-2df603c661c0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.\nTable:\n|| Corpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| Europarl | TotalTerms: | 980 | 1,000 | 1,000 | 1,000 | 1,000 | 996 | 1,000 ||\n|| Europarl | TotalRoots: | 79 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| Europarl | NumberRels: | 1,527 | 1,031 | 1,049 | 1,185 | 1,093 | 1,644 | 999 ||\n|| Europarl | MaxDepth: | 19 | 902 | 894 | 784 | 849 | 6 | 10 ||\n|| Europarl | MinDepth: | 1 | 902 | 894 | 784 | 849 | 1 | 1 ||\n|| Europarl | AvgDepth: | 9.43 | 902 | 894 | 784 | 849 | 2.73 | 4.29 ||\n|| Europarl | DepthCohesion: | 2.02 | 1 | 1 | 1 | 1 | 2.19 | 2.33 ||\n|| Europarl | MaxWidth: | 27 | 3 | 3 | 4 | 3 | 201 | 58 ||\n|| Europarl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| Europarl | AvgWidth: | 1.98 | 1.03 | 1.05 | 1.19 | 1.09 | 6.25 | 2.55 ||\n|| TED Talks | TotalTerms: | 296 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 ||\n|| TED Talks | TotalRoots: | 101 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| TED Talks | NumberRels: | 291 | 1,045 | 1,229 | 3,637 | 4,284 | 2,875 | 999 ||\n|| TED Talks | MaxDepth: | 10 | 860 | 727 | 388 | 354 | 252 | 17 ||\n|| TED Talks | MinDepth: | 1 | 860 | 727 | 388 | 354 | 249 | 1 ||\n|| TED Talks | AvgDepth: | 3.94 | 860 | 727 | 388 | 354 | 250.43 | 6.16 ||\n|| TED Talks | DepthCohesion: | 2.54 | 1 | 1 | 1 | 1 | 1.01 | 2.76 ||\n|| TED Talks | MaxWidth: | 37 | 3 | 79 | 18 | 13 | 9 | 41 ||\n|| TED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| TED Talks | AvgWidth: | 1.79 | 1.05 | 1.23 | 3.64 | 4.29 | 2.94 | 2.37 ||\n\nQuestion: Is it true that The results for the Portuguese corpora are quite different from the ones generated by the English corpora, with terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating shallow taxonomies, disproving the characteristics of each method?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "6fa0512b-21f4-4bd1-86eb-c649baf8805f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Adversarial Removal of Demographic Attributes from Text Data Table 6: Accuracies of the protected attribute with different encoders.\nTable:\n||  |  | Embedding Leaky | Embedding Guarded ||\n|| RNN | Leaky | 64.5 | 67.8 ||\n|| RNN | Guarded | 59.3 | 54.8 ||\n\nQuestion: Is it true that  Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "905c2475-f7a6-4b0f-aeca-1ba199c9fd50", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluation of Greek Word Embeddings Table 4: Word similarity.\nTable:\n|| Model | Pearson | p-value | Pairs (unknown) ||\n|| gr_def |  0.6042 | 3.1E-35 | 2.3% ||\n|| gr_neg10 | 0.5973 | 2.9E-34 | 2.3% ||\n|| cc.el.300 | 0.5311 | 1.7E-25 | 4.9% ||\n|| wiki.el | 0.5812 | 2.2E-31 | 4.5% ||\n|| gr_cbow_def | 0.5232 | 2.7E-25 | 2.3% ||\n|| gr_d300_nosub | 0.5889 | 3.8E-33 | 2.3% ||\n|| gr_w2v_sg_n5 | 0.5879 | 4.4E-33 | 2.3% ||\n\nQuestion: Is it true that According to Pearson correlation, gr cbow def model had the highest correlation with human ratings of similarity?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "5a1d0c5b-836f-4eef-85b7-ff4f0e532907", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. “Best%”: in how many percentage of documents a system receives the highest human rating.\nTable:\n||  | Ours | Refresh | ExtAbsRL ||\n|| Avg. Human Rating |  2.52 | 2.27 | 1.66 ||\n|| Best% |  70.0 | 33.3 | 6.7 ||\n\nQuestion: Is it true that Also, the average human rating for Refresh is significantly higher (p (cid:28) 0.01) than ExtAbsRL,?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "768f70ec-749a-408f-a097-279e7b07e70f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 2: Precisions on the Wikidata dataset.\nTable:\n|| Recall | 0.1 | 0.2 | 0.3 | AUC ||\n|| Rank+ExATT | 0.584 | 0.535 | 0.487 | 0.392 ||\n|| PCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204 ||\n|| PCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396 ||\n|| Our Model | 0.650 | 0.519 | 0.422 |  0.405 ||\n\nQuestion: Is it true that We observe that PCNN+ATT (1) exhibits the best performances?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "4cc9d2c1-a18e-47de-aa39-4909ec936ba9", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTable:\n|| Topic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI |  OD-w2v ARI |  OD-d2v ARI | TF-IDF   Sil. | WMD   Sil. | Sent2vec   Sil. | Doc2vec   Sil. | BERT   Sil. |  OD-w2v   Sil. |  OD-d2v   Sil. ||\n|| Affirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 |  0.14 |  0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 |  0.06 |  0.01 ||\n|| Atheism | 116 |  0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 |  0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 |  0.05 |  0.07 ||\n|| Austerity Measures | 20 |  0.04 |  0.04 | -0.01 | -0.05 | 0.04 |  0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 |  0.19 | 0.1 ||\n|| Democratization | 76 | 0.02 | -0.01 | 0.00 |  0.09 | -0.01 |  0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 |  0.16 |  0.11 ||\n|| Education Voucher Scheme | 30 |  0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 |  0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 |  0.38 |  0.40 ||\n|| Gambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 |  0.35 |  0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 |  0.30 |  0.22 ||\n|| Housing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 |  0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 |  0.13 |  0.13 ||\n|| Hydroelectric Dams | 110 |  0.47 |  0.45 |  0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 |  0.26 |  0.09 ||\n|| Intellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 |  0.05 |  0.14 | 0.01 |  0.04 | 0.03 | 0.01 | 0.03 |  0.04 |  0.12 ||\n|| Keystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 |  0.07 | -0.01 |  0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 |  0.05 |  0.02 ||\n|| Monarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 |  0.15 |  0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 |  0.11 |  0.09 ||\n|| National Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 |  0.31 |  0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 |  0.25 |  0.25 ||\n|| One-child policy China | 67 | -0.05 | 0.01 |  0.11 | -0.02 | 0.02 |  0.11 | 0.01 | 0.01 | 0.02 |  0.04 | -0.01 | 0.03 |  0.07 | -0.02 ||\n|| Open-source Software | 48 | -0.02 | -0.01 |  0.05 | 0.01 | 0.12 |  0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 |  0.18 | 0.01 ||\n|| Pornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 |  0.41 |  0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 |  0.47 |  0.41 ||\n|| Seanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 |  0.32 |  0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 |  0.15 |  0.31 ||\n|| Trades Unions | 19 |  0.44 |  0.44 |  0.60 | -0.05 | 0.44 |  0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 |  0.48 |  0.32 ||\n|| Video Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 |  0.40 |  0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 |  0.32 |  0.42 ||\n|| Average | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 |  0.22 |  0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 |  0.20 |  0.17 ||\n\nQuestion: Is it true that  In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly bad compared to TF-IDF?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "05d0d281-1e18-4f77-932d-b89d635f6ca2", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Improved Semantics for the End-to-End Generation Challenge Corpus Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section 3).\nTable:\n||  Dataset |  Part |  MRs |  Refs |  SER(%) ||\n|| Original | Train | 4,862 | 42,061 | 17.69 ||\n|| Original | Dev | 547 | 4,672 | 11.42 ||\n|| Original | Test | 630 | 4,693 | 11.49 ||\n|| [0.5pt/2pt] Cleaned | Train | 8,362 | 33,525 | (0.00) ||\n|| [0.5pt/2pt] Cleaned | Dev | 1,132 | 4,299 | (0.00) ||\n|| [0.5pt/2pt] Cleaned | Test | 1,358 | 4,693 | (0.00) ||\n\nQuestion: Is it true that This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "4a080275-4fe7-4c3b-b13a-c21e16ec6d34", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Predicting Discourse Structure using Distant Supervision from Sentiment Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined\nTable:\n|| Approach | RST-DTtest | Instr-DTtest ||\n|| Right Branching | 54.64 | 58.47 ||\n|| Left Branching | 53.73 | 48.15 ||\n|| Hier. Right Branch. |  70.82 |  67.86 ||\n|| Hier. Left Branch. | 70.58 | 63.49 ||\n||  Intra-Domain Evaluation |  Intra-Domain Evaluation |  Intra-Domain Evaluation ||\n|| HILDAHernault et al. ( 2010 ) | 83.00 | — ||\n|| DPLPJi and Eisenstein ( 2014 ) | 82.08 | — ||\n|| CODRAJoty et al. ( 2015 ) | 83.84 |  82.88 ||\n|| Two-StageWang et al. ( 2017 ) |  86.00 | 77.28 ||\n||  Inter-Domain Evaluation |  Inter-Domain Evaluation |  Inter-Domain Evaluation ||\n|| Two-StageRST-DT | × | 73.65 ||\n|| Two-StageInstr-DT | 74.48 | × ||\n|| Two-StageOurs(avg) | 76.42 |  74.22 ||\n|| Two-StageOurs(max) |  77.24 | 73.12 ||\n|| Human Morey et al. ( 2017 ) | 88.30 | — ||\n\nQuestion: Is it true that The first set of results in Table 3 shows that the hierarchical right/left branching baselines dominate the completely right/left branching ones?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "c1b4b874-ea14-4243-a9d8-1f7ce0e5c941", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Building a Production Model for Retrieval-Based Chatbots Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.\nTable:\n||  Model |  Parameters |  Validation AUC@0.05 |  Test AUC@0.05 ||\n|| Base | 8.0M |  0.871 | 0.816 ||\n|| 4L SRU → 2L LSTM | 7.3M | 0.864 |  0.829 ||\n|| 4L SRU → 2L SRU | 7.8M | 0.856 |  0.829 ||\n|| Flat → hierarchical | 12.4M | 0.825 | 0.559 ||\n|| Cross entropy → hinge loss | 8.0M | 0.765 | 0.693 ||\n|| 6.6M → 1M examples | 8.0M | 0.835 | 0.694 ||\n|| 6.6M → 100K examples | 8.0M | 0.565 | 0.417 ||\n|| 200 → 100 negatives | 8.0M | 0.864 | 0.647 ||\n|| 200 → 10 negatives | 8.0M | 0.720 | 0.412 ||\n\nQuestion: Is it true that The model performs significantly better when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "c256c279-dab2-4c45-b0e9-b49660868f5f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\nTable:\n||  | Prec. | Rec. | F1 ||\n|| (A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531 ||\n|| (A2) Standard attention | 0.466 | 0.638 | 0.539 ||\n|| (A3) Window size (  ws)=5 | 0.507 | 0.652 |  0.571 ||\n|| (A4) Window size (  ws)=10 | 0.510 | 0.640 | 0.568 ||\n|| (A5) Softmax | 0.490 | 0.658 | 0.562 ||\n|| (A6) Max-pool | 0.492 | 0.600 | 0.541 ||\n\nQuestion: Is it true that Increasing the window size to 10 reduces the F1 score marginally (A3−A4)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7f929dc9-7327-40e8-9352-1767a83b1a2f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nTable:\n|| Model | #Params | BLEU | Train | Decode ||\n|| GNMT | - | 24.61 | - | - ||\n|| GRU | 206M | 26.28 | 2.67 | 45.35 ||\n|| ATR | 122M | 25.70 | 1.33 |  34.40 ||\n|| SRU | 170M | 25.91 | 1.34 | 42.84 ||\n|| LRN | 143M | 26.26 |  0.99 | 36.50 ||\n|| oLRN | 164M |  26.73 | 1.15 | 40.19 ||\n\nQuestion: Is it true that  however, oLRN yields the best BLEU score of 26.73, outperforming GRU (+0.45 BLEU)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "b4829db1-041f-4a7e-9371-9042d2584441", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\nTable:\n||  ResNet-34 |  Eval set % |  #param ||\n|| Baseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M ||\n|| SA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M ||\n|| SA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M ||\n||  SA (S: 1,2,3 - B: 3) |  55.27 | } 0.107M ||\n\nQuestion: Is it true that  We empirically found that self-attention was the most efficient in the 3rd stage?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "7be9b83d-f973-4655-9a2c-39eb8160b687", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Building a Production Model for Retrieval-Based Chatbots Table 5: Recall@k for random, frequency, and clustering whitelists of different sizes. The “+” indicates that the true response is added to the whitelist.\nTable:\n||  Whitelist |  R@1 |  R@3 |  R@5 |  R@10 |  BLEU ||\n|| Random 10K+ | 0.252 | 0.400 | 0.472 | 0.560 | 37.71 ||\n|| Frequency 10K+ | 0.257 | 0.389 | 0.455 | 0.544 | 41.34 ||\n|| Clustering 10K+ | 0.230 | 0.376 | 0.447 | 0.541 | 37.59 ||\n|| Random 1K+ | 0.496 | 0.663 | 0.728 | 0.805 | 59.28 ||\n|| Frequency 1K+ | 0.513 | 0.666 | 0.726 | 0.794 | 67.05 ||\n|| Clustering 1K+ | 0.481 | 0.667 | 0.745 | 0.835 | 61.88 ||\n|| Frequency 10K | 0.136 | 0.261 | 0.327 | 0.420 | 30.46 ||\n|| Clustering 10K | 0.164 | 0.292 | 0.360 | 0.457 | 31.47 ||\n|| Frequency 1K | 0.273 | 0.465 | 0.550 | 0.658 | 47.13 ||\n|| Clustering 1K | 0.331 | 0.542 | 0.650 | 0.782 | 49.26 ||\n\nQuestion: Is it true that The results in Table 5 show that the frequency whitelists perform better than the random and clustering whitelists when the true response is added?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "68d6065c-868c-40ac-b3a9-14218014c2c1", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Solving Hard Coreference Problems Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.\nTable:\n|| Schema | AntePre(Test) | AntePre(Train) ||\n|| Type 1 | 76.67 | 86.79 ||\n|| Type 2 | 79.55 | 88.86 ||\n|| Type 1 (Cat1) | 90.26 | 93.64 ||\n|| Type 2 (Cat2) | 83.38 | 92.49 ||\n\nQuestion: Is it true that These results use the best performing KnowComb system?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "dee030ab-36f2-4c8f-a9f2-297aac021d8f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Localization of Fake News Detection via Multitask Transfer Learning Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.\nTable:\n|| Model | Val. Accuracy | Loss | Val. Loss | Pretraining Time | Finetuning Time ||\n|| Siamese Networks | 77.42% | 0.5601 | 0.5329 |  | 4m per epoch ||\n|| BERT | 87.47% | 0.4655 | 0.4419 | 66 hours | 2m per epoch ||\n|| GPT-2 | 90.99% | 0.2172 | 0.1826 | 78 hours | 4m per epoch ||\n|| ULMFiT | 91.59% | 0.3750 | 0.1972 | 11 hours | 2m per epoch ||\n|| ULMFiT (no LM Finetuning) | 78.11% | 0.5512 | 0.5409 | 11 hours | 2m per epoch ||\n|| BERT + Multitasking | 91.20% | 0.3155 | 0.3023 | 66 hours | 4m per epoch ||\n|| GPT-2 + Multitasking | 96.28% | 0.2609 | 0.2197 | 78 hours | 5m per epoch ||\n\nQuestion: Is it true that BERT achieved a final accuracy of 91.20%, now marginally comparable to ULMFiT's full performance?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "a50811fb-3024-4844-b141-56da2fa21184", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Effective Attention Modeling for Neural Relation Extraction Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. † denotes the previous best state-of-the-art model.\nTable:\n|| Model | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1 ||\n|| CNN zeng2014relation | 0.413 | 0.591 | 0.486 | 0.444 | 0.625 | 0.519 ||\n|| PCNN zeng2015distant | 0.380 |  0.642 | 0.477 | 0.446 | 0.679 | 0.538† ||\n|| EA huang2016attention | 0.443 | 0.638 | 0.523† | 0.419 | 0.677 | 0.517 ||\n|| BGWA jat2018attention | 0.364 | 0.632 | 0.462 | 0.417 |  0.692 | 0.521 ||\n|| BiLSTM-CNN | 0.490 | 0.507 | 0.498 | 0.473 | 0.606 | 0.531 ||\n|| Our model |  0.541 | 0.595 |  0.566* |  0.507 | 0.652 |  0.571* ||\n\nQuestion: Is it true that Our model does not outperform the previous state-of-the-art models on both datasets in terms of F1 score?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "0ed421ff-4061-441d-bbdd-6f1cdc44ca0b", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.\nTable:\n||  GCN +RC (2) | B 16.8 | C 48.1 |  GCN +RC+LA (2) | B 18.3 | C 47.9 ||\n|| +RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1 ||\n|| +RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8 ||\n|| +RC (9) |  21.1 | 50.5 | +RC+LA (9) |  22.0 | 52.6 ||\n|| +RC (10) | 20.7 |  50.7 | +RC+LA (10) | 21.2 |  52.9 ||\n|| DCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7 ||\n|| DCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) |  25.5 |  55.4 ||\n\nQuestion: Is it true that When we increase the DCGCN blocks from 1 to 4, the model performance does not necessarily increase on AMR15 development set?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ffc551ba-ac96-4d34-ab5f-fec1d0e20a57", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nTable:\n|| Type | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num ||\n|| Full | 8.413 | 903 | 10.59 | 450 | 11.18 | 865 ||\n|| Other | -99.95 | 76 | -48.15 | 99 | -71.62 | 135 ||\n\nQuestion: Is it true that It can be observed that the learned reward function does not have good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "335c8128-859a-4bb2-808d-c48b428dd5d0", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.\nTable:\n|| ID LSTM-800 | 5-fold CV 70.56 | Δ 0.66 | Single model 67.54 | Δ 0.78 | Ensemble 67.65 | Δ 0.30 ||\n|| LSTM-400 | 70.50 | 0.60 |  67.59 | 0.83 |  68.00 | 0.65 ||\n|| IN-TITLE | 70.11 | 0.21 |  |  | 67.52 | 0.17 ||\n||  SUBMISSION | 69.90 | – | 66.76 | – | 67.35 | – ||\n|| NO-HIGHWAY | 69.72 | −0.18 | 66.42 | −0.34 | 66.64 | −0.71 ||\n|| NO-OVERLAPS | 69.46 | −0.44 | 65.07 | −1.69 | 66.47 | −0.88 ||\n|| LSTM-400-DROPOUT | 69.45 | −0.45 | 65.53 | −1.23 | 67.28 | −0.07 ||\n|| NO-TRANSLATIONS | 69.42 | −0.48 | 65.92 | −0.84 | 67.23 | −0.12 ||\n|| NO-ELMO-FINETUNING | 67.71 | −2.19 | 65.16 | −1.60 | 65.42 | −1.93 ||\n\nQuestion: Is it true that  The 'alternating' LSTM layout we chose for our submission actually outperformed the 'traditional' one in terms of both single model and ensemble performance?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "1d4b85e2-79a4-463d-873b-f6bdcef4fa2f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.\nTable:\n|| Feature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1 ||\n|| +BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 |  0.93 | 0.94 | 0.92 |  0.93 | 0.91 | 0.91 |  0.91 ||\n|| +BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89 ||\n|| +Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88 ||\n|| +BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n\nQuestion: Is it true that  Lin-SVM outperforms other classifiers in extracting most relations?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "3df48964-f174-4875-97f2-dee5dfb515c5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except “dialog turns” are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nTable:\n|| Method | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success ||\n|| GP-MBCM | 2.99 | 19.04 | 44.29 | 28.9 ||\n|| ACER | 10.49 | 77.98 | 62.83 | 50.8 ||\n|| PPO | 9.83 | 83.34 | 69.09 | 59.1 ||\n|| ALDM | 12.47 | 81.20 | 62.60 | 61.2 ||\n|| GDPL-sess |  7.49 | 88.39 | 77.56 | 76.4 ||\n|| GDPL-discr | 7.86 | 93.21 | 80.43 | 80.5 ||\n|| GDPL | 7.64 |  94.97 |  83.90 |  86.5 ||\n||  Human |  7.37 |  66.89 |  95.29 |  75.0 ||\n\nQuestion: Is it true that  Though ALDM obtains a higher inform F1 and match rate than PPO, it does not get a significant improvement  on task success  Ablation test is investigated in Table 3?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "07dfd438-80f3-41d8-ba09-1f769f983131", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A Lightweight Recurrent Network for Sequence Modeling Table 2: Test error (ERR) on document classification task. “#Params”: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.\nTable:\n|| Model | Model | #Params | AmaPolar ERR | AmaPolar Time | Yahoo ERR | Yahoo Time | AmaFull ERR | AmaFull Time | YelpPolar ERR | YelpPolar Time ||\n|| Zhang et al. ( 2015 ) | Zhang et al. ( 2015 ) | - | 6.10 | - | 29.16 | - | 40.57 | - | 5.26 | - ||\n|| This | LSTM | 227K |  4.37 | 0.947 |  24.62 | 1.332 | 37.22 | 1.003 | 3.58 | 1.362 ||\n|| This | GRU | 176K | 4.39 | 0.948 | 24.68 | 1.242 |  37.20 | 0.982 |  3.47 | 1.230 ||\n|| This | ATR | 74K | 4.78 | 0.867 | 25.33 | 1.117 | 38.54 | 0.836 | 4.00 | 1.124 ||\n|| Work | SRU | 194K | 4.95 | 0.919 | 24.78 | 1.394 | 38.23 | 0.907 | 3.99 | 1.310 ||\n||  | LRN | 151K | 4.98 |  0.731 | 25.07 |  1.038 | 38.42 |  0.788 | 3.98 |  1.022 ||\n\nQuestion: Is it true that LRN does not accelerate the training over LSTM and SRU by about 20%?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "1d89289c-47a5-4052-8b51-a516aead51a8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\nTable:\n||  Complaints   Feature |  Complaints   r |  Not Complaints   Feature |  Not Complaints   r ||\n||  Unigrams |  Unigrams |  Unigrams |  Unigrams ||\n|| not | .154 | [URL] | .150 ||\n|| my | .131 | ! | .082 ||\n|| working | .124 | he | .069 ||\n|| still | .123 | thank | .067 ||\n|| on | .119 | , | .064 ||\n|| can’t | .113 | love | .064 ||\n|| service | .112 | lol | .061 ||\n|| customer | .109 | you | .060 ||\n|| why | .108 | great | .058 ||\n|| website | .107 | win | .058 ||\n|| no | .104 | ’ | .058 ||\n|| ? | .098 | she | .054 ||\n|| fix | .093 | : | .053 ||\n|| won’t | .092 | that | .053 ||\n|| been | .090 | more | .052 ||\n|| issue | .089 | it | .052 ||\n|| days | .088 | would | .051 ||\n|| error | .087 | him | .047 ||\n|| is | .084 | life | .046 ||\n|| charged | .083 | good | .046 ||\n||  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) |  POS (Unigrams and Bigrams) ||\n|| VBN | .141 | UH | .104 ||\n|| $ | .118 | NNP | .098 ||\n|| VBZ | .114 | PRP | .076 ||\n|| NN_VBZ | .114 | HT | .076 ||\n|| PRP$ | .107 | PRP_. | .076 ||\n|| PRP$_NN | .105 | PRP_RB | .067 ||\n|| VBG | .093 | NNP_NNP | .062 ||\n|| CD | .092 | VBP_PRP | .054 ||\n|| WRB_VBZ | .084 | JJ | .053 ||\n|| VBZ_VBN | .084 | DT_JJ | .051 ||\n\nQuestion: Is it true that  Across unigrams, part-of-speech patterns and word clusters, we see a distinctive pattern emerging around pronoun usage?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "4366020a-5cdf-4758-aa6a-8185b337656e", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= “b*tch”\nTable:\n|| Dataset | Class | ˆ  piblack | ˆ  piwhite |  t |  p | ˆ  piblackˆ  piwhite ||\n||  Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 |  | 0.978 ||\n||  | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020 ||\n||  Waseem | Racism | 0.011 | 0.011 | -1.254 |  | 0.955 ||\n||  | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203 ||\n||  | Racism and sexism | 0.012 | 0.012 | -0.162 |  | 0.995 ||\n||  Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152 ||\n||  | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997 ||\n||  Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091 ||\n||  Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728 ||\n||  | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956 ||\n||  | Spam | 0.010 | 0.010 | 0.000 |  | 1.000 ||\n\nQuestion: Is it true that The Waseem and Hovy (2016) classifier is not particularly sensitive to the word \"b*tch\" with only 1% of black-aligned and 1% of white-aligned tweets predicted to belong to this class?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "5d749e06-1775-42bd-b28f-d106eab9163f", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Enhancing AMR-to-Text Generation with Dual Graph Representations Table 4: Results of the ablation study on the LDC2017T10 development set.\nTable:\n|| <bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold> | <bold>Size</bold> ||\n|| biLSTM | 22.50 | 30.42 | 57.6M ||\n|| <italic>GEt</italic> + biLSTM | 26.33 | 32.62 | 59.6M ||\n|| <italic>GEb</italic> + biLSTM | 26.12 | 32.49 | 59.6M ||\n|| <italic>GEt</italic> + <italic>GEb</italic> + biLSTM | 27.37 | 33.30 | 61.7M ||\n\nQuestion: Is it true that The complete model has slightly more parameters than the model without graph encoders (57.6M vs 61.7M)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "c6b5330f-3246-4e62-a3c8-78aeae05a8f5", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\nTable:\n||  Representation |  Hyper parameters Filter size |  Hyper parameters Num. Feature maps |  Hyper parameters Activation func. |  Hyper parameters L2 Reg. |  Hyper parameters Learning rate |  Hyper parameters Dropout Prob. |  F1.(avg. in 5-fold) with default values |  F1.(avg. in 5-fold) with optimal values ||\n|| CoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49 ||\n|| SB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 |  75.05 ||\n|| UD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57 ||\n\nQuestion: Is it true that The results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance, followed by the CoNLL08 representation?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "90528389-92f2-4f21-8903-6700b00bcec4", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\nTable:\n||  | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| P | EN | Europarl |  0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761 ||\n|| P | EN | Ted Talks |  0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664 ||\n|| P | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 |  0.7311 | 0.5676 ||\n||  | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 |  0.6533 | 0.5656 ||\n|| R | EN | Europarl | 0.0396 | 0.3999 | 0.5499 |  0.6045 | 0.5887 | 0.0023 | 0.0017 ||\n|| R | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 |  0.6077 | 0.2666 | 0.0019 ||\n|| R | PT | Europarl | 0.0111 | 0.3554 | 0.5795 |  0.6727 | 0.5184 | 0.0053 | 0.0012 ||\n||  | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 |  0.6877 | 0.5515 | 0.4706 | 0.0011 ||\n|| F | EN | Europarl | 0.0591 | 0.0671 | 0.0922 |  0.1015 | 0.1003 | 0.0044 | 0.0033 ||\n|| F | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 |  0.1121 | 0.0037 ||\n|| F | PT | Europarl | 0.0217 | 0.3438 | 0.5513 |  0.6403 | 0.5555 | 0.0105 | 0.0024 ||\n||  | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 |  0.6475 | 0.5819 | 0.5471 | 0.0022 ||\n\nQuestion: Is it true that In some cases it seems to make no difference in results, e.g., Europarl in Portuguese which did not increase the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in no increase of f-measure from F=0.5555 in DF to F=0.6403 in TF?\n### RESPONSE:\nAnswer: no\n\n### END"}
{"id": "ea497540-0fb6-42d3-971a-c539b056ba98", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.\nTable:\n|| Feature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1 ||\n|| +BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 |  0.93 | 0.94 | 0.92 |  0.93 | 0.91 | 0.91 |  0.91 ||\n|| +BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n|| +ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89 ||\n|| +Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88 ||\n|| +BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91 ||\n\nQuestion: Is it true that Word embeddings derived from Wiki-PubMed-PMC outperform GloVe-based embeddings (Table 1)?\n### RESPONSE:\nAnswer: yes\n\n### END"}
{"id": "2373a5b5-05cc-45ca-9e6c-5323513811b8", "text": "### INSTRUCTION:\nGiven the following table and question, generate the answer to solve it.\n\nCaption: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.\nTable:\n|| Corpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust ||\n|| Europarl | TotalTerms: | 957 | 1,000 | 1,000 | 1,000 | 1,000 | 836 | 1,000 ||\n|| Europarl | TotalRoots: | 44 | 1 | 1 | 1 | 1 | 43 | 1 ||\n|| Europarl | NumberRels: | 1,588 | 1,025 | 1,028 | 1,185 | 1,103 | 1,184 | 999 ||\n|| Europarl | MaxDepth: | 21 | 921 | 901 | 788 | 835 | 8 | 15 ||\n|| Europarl | MinDepth: | 1 | 921 | 901 | 788 | 835 | 1 | 1 ||\n|| Europarl | AvgDepth: | 11.82 | 921 | 901 | 788 | 835 | 3.05 | 8.46 ||\n|| Europarl | DepthCohesion: | 1.78 | 1 | 1 | 1 | 1 | 2.62 | 1.77 ||\n|| Europarl | MaxWidth: | 20 | 2 | 3 | 4 | 3 | 88 | 41 ||\n|| Europarl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| Europarl | AvgWidth: | 1.99 | 1.03 | 1.03 | 1.19 | 1.10 | 4.20 | 2.38 ||\n|| TED Talks | TotalTerms: | 476 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 ||\n|| TED Talks | TotalRoots: | 164 | 2 | 1 | 1 | 1 | 1 | 1 ||\n|| TED Talks | NumberRels: | 521 | 1,029 | 1,331 | 3,025 | 3,438 | 3,802 | 1,009 ||\n|| TED Talks | MaxDepth: | 16 | 915 | 658 | 454 | 395 | 118 | 12 ||\n|| TED Talks | MinDepth: | 1 | 913 | 658 | 454 | 395 | 110 | 1 ||\n|| TED Talks | AvgDepth: | 5.82 | 914 | 658 | 454 | 395 | 112.24 | 5.95 ||\n|| TED Talks | DepthCohesion: | 2.75 | 1 | 1 | 1 | 1 | 1.05 | 2.02 ||\n|| TED Talks | MaxWidth: | 25 | 2 | 77 | 13 | 12 | 66 | 98 ||\n|| TED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1 ||\n|| TED Talks | AvgWidth: | 1.83 | 1.03 | 1.36 | 3.03 | 3.44 | 6.64 | 2.35 ||\n\nQuestion: Is it true that  As we can observe in Table 6, limiting the number of terms to 1,000, Patt and DocSub do not to generate relations for all terms?\n### RESPONSE:\nAnswer: yes\n\n### END"}
